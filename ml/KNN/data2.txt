发现 spark 2.0 的特点，也可以用第七篇里总结的 dataframe 的特点来说明，那就是：<ul> <li><em>write less : 写更少的代码</em></li> <li><em>do more : 做更多的事情</em></li> <li><em>faster : 以更快的速度</em></li></ul>	spark
<title></title>spark 2.0 的特点，也可以用第七篇里总结的 dataframe 的特点来说明，那就是：write less : 写更少的代码do more : 做更多的事情faster : 以更快的速度	spark
<title></title>Mesos与YARN比较Mesos与YARN主要在以下几方面有明显不同：（1）框架担任的角色在Mesos中，各种计算框架是完全融入Mesos中的，也就是说，如果你想在Mesos中添加一个新的计算框架，首先需要在Mesos中部署一套该框架；而在YARN中，各种框架作为client端的library使用，仅仅是你编写的程序的一个库，不需要事先部署一套该框架。从这点上说，YARN运行和使用起来更加方便。（2）调度机制Mesos采用了双层调度策略，第一层是Mesos master将空闲资源分配给某个框架，而第二层是计算框架自带的调度器对分配到的空闲资源进行分配，也就是说，Mesos将大部分调度任务授权给了计算框架；而YARN是一个单层调度架构，各种框架的任务一视同仁，全由Resource Manager进行统一调度。总结来说，Mesos master首先完成粗粒度的资源分配，即：将资源分配给框架，然后由框架进行细粒度的资源分配；而Resource manager直接进行细粒度的分配，即:直接将资源分配给某个任务（Task）。其他各个特性对比如下表：5. Mesos与YARN发展情况个人认为Mesos和YARN均不成熟，很多承诺的功能还未实现或者实现得不全，但总体看，它们发展很快，尤其是YARN，在去年年末推出Hadoop-0.23.0后，近期又推出Hadoop-0.23.1。随着各种计算框架（如Spark，S4，Storm等）的日趋成熟，一个统一的资源管理和调度平台将不可或缺。另一个与Mesos和YARN类似的系统是Facebook开源的Hadoop Coroca，具体可参考：“Hadoop Corona介绍”。	spark
<title></title>Taste简介Taste 是 Apache Mahout提供的一个协同过滤算法的高效实现，它是一个基于 Java 实现的可扩展的，高效的推荐引擎。Taste 既实现了最基本的基于用户的和基于内容的推荐算法，同时也提供了扩展接口，使用户可以方便的定义和实现自己的推荐算法。同时，Taste 不仅仅只适用于 Java 应用程序，它可以作为内部服务器的一个组件以 HTTP 和 Web Service 的形式向外界提供推荐的逻辑。Taste 的设计使它能满足企业对推荐引擎在性能、灵活性和可扩展性等方面的要求。Taste原理系统架构image0接口设计DataModelDataModel是用户喜好信息的抽象接口，它的具体实现可能来自任意类型的数据源以抽取用户喜好信息。Taste提供了MySQLDataModel，方便用户通过JDBC和MySQL访问数据, 此外还通过FileDataModel提供了对文件数据源的支持。UserSimilarity 和 ItemSimilarityUserSimilarity用于定义两个用户间的相似度，它是基于协同过滤的推荐引擎的核心部分，可以用来计算用户的“邻居”，这里我们将与当前用户口味相似的用户称为他的邻居。ItemSimilarity 类似的，定义内容之间的相似度。UserNeighborhood用于基于用户相似度的推荐方法中，推荐的内容是基于找到与当前用户喜好相似的“邻居用户”的方式产生的。UserNeighborhood 定义了确定邻居用户的方法，具体实现一般是基于 UserSimilarity 计算得到的。RecommenderRecommender 是推荐引擎的抽象接口，Taste中的核心组件。程序中，为它提供一个DataModel，它可以计算出对不同用户的推荐内容。实际应用中，主要使用它的实现类 GenericUserBasedRecommender 或者 GenericItemBasedRecommender，分别实现基于用户相似度的推荐引擎或者基于内容的推荐引擎。	spark
<code>package spark.exampleimport org.apache.spark._import SparkContext._object WordCount {  def main(args: Array[String]) {    //命令行参数个数检查    if (args.length == 0) {      System.err.println("Usage: spark.example.WordCount <input> <output>")      System.exit(1)    }    //使用hdfs文件系统    val hdfsPathRoot = "hdfshost:9000"    //实例化spark的上下文环境    val spark = new SparkContext(args(0), "WordCount",      System.getenv("SPARK_HOME"),SparkContext.jarOfClass(this.getClass))    //读取输入文件    val inputFile = spark.textFile(hdfsPathRoot + args(1))    //执行WordCount计数    //读取inputFile执行方法flatMap，将每行通过空格分词    //然后将该词输出该词和计数的一个元组，并初始化计数    //为 1，然后执行reduceByKey方法，对相同的词计数累    //加    val countResult = inputFile.flatMap(line => line.split(" "))                      .map(word => (word, 1))                      .reduceByKey(_ + _)    //输出WordCount结果到指定目录    countResult.saveAsTextFile(hdfsPathRoot + args(2))  }}</code>	spark
到spark_wordcount目录，执行编译：cd ~/spark_wordcount/./sbt compile./sbt package编译过程，sbt需要上网下载依赖工具包，jna，scala等。编译完成后可以在target/scala-2.10/目录找到打包好的jar[root@bd001 scala-2.10]# pwd/usr/local/hadoop/spark_wordcount/target/scala-2.10[root@bd001 scala-2.10]# lscache  classes  wordcount_2.10-1.0.0.jarWordCount执行可以参考Spark分布式运行于hadoop的yarn上的方法，写一个执行脚本#!/usr/bin/env bashSPARK_JAR=./assembly/target/scala-2.10/spark-assembly_2.10-0.9.0-incubating-hadoop2.2.0.jar \    ./bin/spark-class org.apache.spark.deploy.yarn.Client \      --jar ~/spark_wordcount/target/scala-2.10/wordcount_2.10-1.0.0.jar \      --class  spark.example.WordCount \      --args yarn-standalone \      --args /testWordCount.txt \      --args /resultWordCount \      --num-workers 3 \      --master-memory 4g \      --worker-memory 2g \      --worker-cores 2	spark
<div><strong> <wbr /> <wbr /> <wbr /> <wbr /> <wbr /> 1.市场的铁律，就是让绝大多数人亏钱，极少数人赚钱。在这极少数人中，每年还在冲刷淘汰着.....更何况其中多数仅是吻饱而已。问题在于，这样一个残酷的市场，你还玩吗?</strong></div><div><div><strong><wbr /> <wbr /> <wbr /> <wbr /> <wbr /> 2.介入市场并不需要了解所有参与者，只要了解价格的轨迹，充其量加上一点量的关注即可。价格说明一切。价格的运动语言是期货市场最重要的市场语言。记住，价格永远是第一位的。<wbr /> <wbr /> <wbr /> <wbr /> <wbr /> 3.市场的职能就是驱使一批批的羊群进入屠场，以获得一块羊肉的报偿。然而，这是市场的需求，也是市场的职能。牺牲者的全部意义是发现产生市场价格，对会的经济活动提供指标。就这个意义来说，他们的牺牲，功不可没。<wbr /> <wbr /> <wbr /> <wbr /> <wbr /> 4.“先为不可胜，待敌之可胜。不可胜在己，可胜在敌。”在交易中，风险控制永远是第一位的，并贯穿交易的全过程。利润是风险控制的产品，而不是欲望的产品。从这个意义上说，风险控制怎么强调，也不过分。<wbr /> <wbr /> <wbr /> <wbr /> <wbr /> 5.预测是个陷阱，一个美丽的陷阱，一个分析师永远津津乐道的陷阱。从本质上讲，预测从属于主观。一切必须由市场来决定，市场永远是客观的，不以交易者的主观来决定的。跟着市场行动，抛弃任何主观的东西，是成功交易者的前提。建立起你的交易系统，放弃预测，放弃恐惧，也放弃贪婪和欢喜，一切由你的交易系统决定出入市。<wbr /> <wbr /> <wbr /> <wbr /> <wbr /> 6.止损不怕，止损错了也不怕。怕的是不止损。记住，一次意外，足以致命。止损正是杜绝这种意外出现。没有止损的习惯，早晚要被淘汰出局，这只是时间问题。如果你有足够聪明的话，止损过多，是可以避免的。<wbr /> <wbr /> <wbr /> <wbr /> <wbr /> 7.只管价格变动，不听传言。不问、不听、不传、不辨，任尔蜚短流长，奈我何如?聪明者如斯也。<wbr /> <wbr /> <wbr /> <wbr /> <wbr /> 8、交易的最高境界是无我，无欲、无喜、无忧、无恐惧。成功的交易者总是张着两只眼，一只望着市场，一只永远望着自己。任何时候，最大的敌人，就是你自己。校正自己，永远比观察市场重要。<wbr /> <wbr /> <wbr /> <wbr /> <wbr /> 9.成功的交易者是技巧、心态和德行的统一，三者不可分离。交易者应该是开心的、愉悦的，无论赚钱还是赔钱。如果交易使你痛苦，使你的生活痛苦，那么，最好的选择就是退出交易。<wbr /> <wbr /> <wbr /> <wbr /> <wbr /> 10.交易永远是修炼场，人性的一切弱点在这里暴露无遗。无论什么伟大人物，相信在这里，大多数是丧魂落魄的。无我，无欲、无喜、无忧、无恐惧......从这个市场走出来的人，冷眼看世界，一觅众山小。</strong></div></div>	股票
1.只买上升轨道的股票，不买下降轨道的股票，如果股票一直在上升轨道，就应该把握住机会，坚持持有，千万不要脑袋一热就卖掉！2.在上升轨道的下沿买进股票，然后持有，等到上升轨道发生了明显的变化时，就要果断卖出，不要犹豫。3.对于那些局面复杂、自己看不清的股票，千万不要贸然进去，柿子捡软的捏，炒股也是一样。4.不要把所有的钱一次性买进同一只股票，即便你非常看好它，而且事后证明你是对的，也不要一次性买进。因为市场瞬息万变，谁也不知道明天发生什么，总有可能买得更低，或者有更好的机会买进。5.如果你误买了下降轨道中的股票，一定要赶紧卖出，避免损失扩大。6.如果买的股票目前还没有损失，但已经进入下降轨道，也要赶紧退出观望。7.不是上升轨道的股票，根本不要看。管它将来怎么样，不要陪主力去建仓。散户可没时间陪他们耗着。8.赢钱时加仓，输钱时减码，如果你不想死得快而想赚得快，这是唯一的方法。9.不要相信业绩，那只代表过去，不代表将来。10.炒股是在炒将来，而不是过去。11.不要幻想自己能赚钱而老是去做短线，每天进进出出。频繁进出，可能会给你带来快感，但是会让你损失很多的钱，唯一受益的就是证券公司，而且你不会有那么高的水平，你也不是庄家。不要买太多的股票，最好不要超过五只。你没有那么多精力看着它们。这就像如果你想娶五个老婆，即便你身体够好，你也满足不了你的老婆们。韦小宝的故事只发生在小说里。12.股票很便宜了，跌了很多了，不是你买入的理由，永远不是！它还可能更加便宜！13.股票很贵了，已经涨了很多了，也不是你拒绝买入或者卖出的理由。它还可能涨得更高！	股票
<b>看量追庄</b>众所周知，量是资本市场运行的主要动力源，亦为股市中支撑股价涨跌的血液。在计算量能关系中，也要如股价一样设定量能的平均轨迹。只有将量能的平均线与价的平均线综合加以权衡，我们才能较为准确地把握后市的运行格局。主力吸筹有以下三种方式：1.丘峰式量能：具体特征为每一次一低峰量能大于前一个循环中的低峰量能，此为主力介入征兆。一般而言，主力介入一只个股，都有一段默默吸筹的过程，一俟其吸筹完毕，即控盘达到３０％左右才会进入拉升。根据其丘峰量能的每波段运行所消耗的总成交量加以研判，如果超过流通盘，则表明主力吸筹完成。此时，如果Ｋ线图上也呈现出完善的波浪起伏，则可以低谷处介入。2.高举高打式进货量能：其表现为该股经过长达半年以上的横盘整理，突然量能急剧放大，一举突破前期高点。具体量化则为５日均量迅速放大，但１０日均量线缓步攀升。从股价上看，为一根长阳将前期往复震荡整理的小阴线小阳线尽踩足下。一旦出现此类个股，投资者应密切关注其成交量的变化，具体而言市场上存在着以下两种情况。Ａ、主力继续高举高打，一意轧空，量能持续沿着中短期的通道上行，直到换手率达到８０％以上，方才进行缩量调整。反映在股价运行图上，则为长驱直上的快马走势，径自沿着５日均线上行；盘中即使有所回档，亦于５日均线处获得支撑。Ｂ、均线形态显示出冲高回落进行平台整理，但股价到放量处均线上方即告调整完毕。后期即使某一日量能低于５日均量，回落亦将在１０日或２０日均量附近受支撑，表明中短期内多空双方力量较为均衡。如出现此类状况，投资者可以在５日均量下叉１０日均量或即将下穿１０日均量时，于下影线部分介入。该类股票一般背后有较大的题材支撑，因此主力迫不急待地大举吸货，纵使成本再高亦在所不惜，其势在必得之心可见一斑。在现行的涨跌停板制度下，庄家一般把握投资者心理。放量直逼涨停处，到午后开盘或收盘前后又数次将涨停板打开，令部分投资者产生获利了结心理，从而骗取足够筹码。３、散兵坑式吸筹量能：具体特征为集中在某一时间段内（时间不超过５日，一般１－２日）放量，随后量能突然萎缩，但每一次量能低谷均高于前一波底位量能。换而言之量能已步入上升通道。从Ｋ线组合看，在量能的第一个循环中，均突然拉出一根长阳线，随后回落。中长期看呈现箱体运行，后期若连续放巨量，超越前期最高量能处时即该股将进入如上所言的高举高打阶段，意味着该股拉升在即。入驻这类股票的主力都相当有耐心，一般而言，未在底位吸足筹码，绝不草率行事。而第一次偷袭，仅做试探市场反应，在数次从容洗盘后，震出不坚定分子，为今后发动总动员奠定基础。对于该类股票，投资者往往会在其突然拉升之时抛出，而狡猾的庄家正是利用这一心理在最后拉升期前夕，将一部分持股者扫地出门；只有真正有信心者，才能与庄共舞笑到最后。<b>炒"消息"与跟庄的学问</b>股市是信息高效率流动的市场。金融政策、宏观经济、上市公司、机构或庄家动向等等各个方面和各个层次的信息，都会引发股市及其个股价格的波动。"信息就是金钱，时间就是效率"在股市中体现得淋漓尽致。在建仓时，若能够提前知道利好消息，就能够在别人之前以较低的价格买入；同样，若能提前得到利空信息，则能够安全出局，获利或减轻损失。信息在股市中的重要地位，使得每一位进入股市的投资者都非常重视股市信息，机构投资者设立了专门的信息分析队伍，研究信息的有效性和力度，在一定的情况下，利用信息进行炒作，达到诱空或诱多的目的。另外，市场上还有一些策划机构，将上市公司的题材和炒作思路制定成方案，联系庄家入市，通过炒作获利进行利润分配。作为中小投资者，与这些机构相比，在信息来源和获取信息的时间等方面都处于不利条件。因此，许多中小投资者将获取信息的希望寄托在股评人士身上，但这种方法也非十分有效。相反，盲目地追逐市场消息或市场谣传往往成为庄家的抬轿人。其实，要作证券市场的主人，就必须学会根据自己了解和得到的信息进行仔细分析，通过分析去伪存真，把握消息带来的投资机会。按照中小投资者获得信息的渠道，消息源主要有以下几类：1.新闻媒介公布的信息。包括金融和证券市场管理政策、宏观经济报告、上市公司年报、上市公司信息公告等。其中，前两类直接影响股市大盘的走势，后两类影响个股的涨跌。2.各类证券市场分析报告。包括各抒已见的股评，研究人士作的上市公司分析，行业分析报告等。3.市场传闻。一般地，中小投资者比较关注第二类信息和第三类消息。由于第三类消息具有许多不确定性，极富神秘感，易于被庄家利用，引发市场的跟风气氛，因而激起中小投资者的浓厚兴趣。其实，在证券市场规模日益扩大，机构资金实力在市场竞争中不断增强的背景下，所谓庄家要对某只股票进行大资金的炒作，都必须在深入研究第一类信息的基础上进行。这类信息最真实，是研判大势、精选个股的基础，同样应该引起散户投资者的高度重视。因为无论对机构大户和散户，在这些信息面前是平等的。考虑到机构或大户在获取信息的渠道和时间等方面较散户具备更优越的条件，散户投资者在对待上述三类信息的判断上，应结合市场特点把握以下原则：（1）在市场极度投机阶段，股市对第一类信息反应最迟钝，第二类和第三类消息满天飞，成为市场追棒的对象。此时，投资者应保持清醒的头脑，不为市场现象所迷惑，相信自己对第一类信息的判断。1996年10月和1997年4月均属这类情况。当时对个股的炒作已造成股票价值和价格的严重背离，管理层每发布一个调控市场的利空政策，均被市场迅速消化视为利多对待，投资者沉浸在狂热和兴奋之中。而机构投资者已开始出货，因为第一类信息是最最真实可靠的，当风险聚集后，一场暴跌出现，使众多散户成为高位套牢者。相反，在极度低迷的市场中，1996年4月，许多投资者将取消保值贴补率作反弹信息处理，丢掉了十分宝贵的筹码。<b>坐庄的原理</b>在大街上，有时可以看到一种玩38的临时赌局。一个人拿着一个小酒杯，酒杯里有一个白色的小片，小片的一面写着3，另一面写着8，他一面快速地在地上转着酒杯，一面口中念念有词，不外是解说规则招 呼围观者下注。随着酒杯旋转，杯中的小片也跟着翻滚，转上一阵后 ，他突然用一块硬纸板把杯口盖上，周围的人开始压钱。由于小片一直在晃动，看清上面的字多少是有些困难的，所以有人压3有人压8。压完之后一开盖，压对的一赔一，压错的压多少全输掉。这个游戏看似对压钱的人有利，因为只要眼力好，仔细看，就算不是每一次都能看准，至少能对多错少，只要看的仔细些，应该是只赢不 输。但一玩就知道了，有时候看的清清楚楚是3，可一开盖却是8。玩 的人只以为是自己没看清，于是更仔细的看，看清楚了再压，结果还是有时对有时错，等玩一段时间再看，肯定是输钱多赢钱少。道理何在呢？原来在小片中夹有一小片磁铁，聚赌者手中拿着另一块 磁铁。他悄悄的把磁铁N极S极往上一凑，要它是8就是8，要它是3就是 3，和他玩的人有输没赢。从这个例子包括了坐庄的基本原理。坐庄有两个要点，第一，庄家要 下场直接参与竞局，也就是这样才能赢；第二，庄家还得有办法控制 局面的发展，让自己稳操胜券。股市做庄也是如此，股市做庄庄家要把仓位分成两部分，一部分用于 建仓，部分资金的作用是直接参与竞局；另一部分用于控制股价。与 38游戏不同，38游戏中靠作弊控制局面，而在股市中庄家必须通过实 际买卖来控制股价；38游戏中用一块磁铁就可以控制局面了，完全没 有成本，而股市中必须用一部分资金控盘，而且控盘这部分资金风险 较大，一圈庄做下来，这部分资金获利很低甚至可能会赔，庄家赚钱主要还是要靠建仓资金。股市控盘是有成本的，所以，要做庄必须进行成本核算，看控盘所投 入的成本和建仓资金的获利相比如何，如果控盘成本超出了获利，则 这个庄就不能再做下去了。一般来说，坐庄是必赢的，控盘成本肯定比获利少。因为做庄控盘虽然没有超越于市场之外的手段无成本的控 制局面，但股市存在一些规律可以为庄家所利用，可以保证控盘成本比建仓获利要低。控盘的依据是股价的运行具有非线形，快速集中大量的买卖可以使股 价迅速涨跌，而缓慢的买卖即使量已经很大，对股价的影响仍然很小 。只要市场的这种性质继续存在下去，庄家就可以利用这一点来获利。股价之所以会有这种运动规律，是因为市场上存在量对行情缺乏 分析判断能力的盲目操作的股民，他们是坐庄成功的基础。随着股民 总体素质的提高，坐庄的难度会越来越大，但做庄仍然是必赢的，原因在于做庄掌握着主动权，市场大众在信息上永远处于劣势，所以在 对行情的分析判断上总是处于被动地位，这是导致其群体表现被动的 客观原因。这个因素永远存在，所以，市场永远会有这种动性可以被庄家利用。<b>庄家的坐庄路线</b>坐庄的基本原理是利用市场运动的某些规律性，人为控制股价使自己获利。怎样控制股价达到获利的目的呢？不同的庄家有不同的路线。 最简单最原始也最容易理解的一种路线是低吸高抛，具体的说就是在低位吸到货然后拉到高位出掉。坐庄过程分为建仓、拉抬、出货三个阶段，庄家发现一只有上涨潜力的股票，就设法在低价位开始吸货，待吸到足够多的货后，开始拉抬，拉抬到一定位置把货出掉，中间的一段空间就是庄家的获利。这种坐庄路线的主要缺点是做多不做空，只在行情的上升段控盘，在行情的下跌程不控盘，没有把行情的全过程控制在手里，所以随着出货完成做庄即告结束，每次坐庄都只是一次性操作。这一次做完了下一次要做什么还得去重新发现市场机会，找到机会还要和其他庄家竞争，避免被别人抢先做上去。这么大的坐庄资金，总是处于这种状态，有一种不稳定感。究其原因在于只管被动的等待市场提供机会，而没有主动的创造机会。所以，更积极的坐庄思路是不仅要做多，而且要做空，主动的创造市场机会。按照这种思路，一轮完整的坐庄过程实际上是从打压开始的。第一阶段，庄家利用大盘下跌和股利空打压股价，为未来的上涨制造空间；第二阶段是吸货，吸的都是别人的割肉盘，又叫扎空；然后是拉抬和出货。出货以后寻找时机开始打压，进行新一轮做庄；如此循环往复，不断的从股市上榨取利润。这是做庄的思路，如果把前面一种庄比做打猎，这类长庄则好比养鸡，每一轮炒做都可以赚到一笔钱，就象养着鸡下蛋一样。打猎运气好时可以打到一只大狗熊，足吃上一冬，但运气不好时也可能跑了很多路费了很多力气但什么也没打到；养鸡每次只捡一个蛋，但相对来说比较稳定。<b>坐庄的思路</b>以上的坐庄路线设计得很理想，但还太主观，不一定能成功。因为做多时有出 不去的风险，做空时有打下价格但接不回来的风险，要想坐庄成功还要考虑一些更本质的问题。两种坐庄路线有没有什么共同的本质性的东西呢？比较两种思路，做长庄既做多又做空，是一种更老道的手法。分析一下长庄的思路，发现做长庄的特点是不怕涨也不怕跌，就怕看不明白，因为涨的时候庄家可以做多赚钱，跌的时候庄家可以借机打压，为以后做多创造条件，只要看明白了，不管是利多还是利空都可以利用。所以，这种坐庄思路成功的关键是看准市场方向。庄家看准市场方向不同于散户的预测行情走势，庄家可以主动地推动股价，他要考虑的是怎样推对方向，推对了方向就可以引起市场大众的追随，做为启动者就可以获利。所以，这种庄家的基本思路是把握市场上的做多和做空潜力，做市场的先导，推动股价运动，释放市场能量。再会过头来看低吸高抛路线，其实也是释放市场能量，寻找做庄股票的过程就是在寻找谁有上涨潜力，拉抬和成功出货是释放这一市场能量。不过这种思路只想着怎样释放市场的多头能量，而不考虑怎样释放空头能量和怎样蓄积新的多头能量，所以不够完整。可见，不管是长庄还是一次性庄，做庄成功的关键都是正确的释放市场能量，前面的两种不同操作路线可以看成是利用不同类型的市场能量的方法。低吸高抛思路适用于定价过低的绩优股，做价值发现。这种股有一个长期利好作为背景，市场情绪波动和其它各种小的利空因素可以忽略，庄家可以大量收集，锁定大量筹码，使得股价不太受市场因素干扰，庄家可以按自己的主观意图拉抬和出货。要想这样做庄需要有资金实力，因为要想做大幅度的拉抬必须锁定相当多的筹码，没有钱是做不了的，而且这样的股票别的庄家也在打主意，没有一定的实力是抢不过人家的。做长庄的思路适合于股价在合理价值区域内的股票，这种股票可涨可跌，庄家不能持仓太重，筹码锁定少，所以股价受大盘影响比较大，庄家必须顺势而为，利用人气震荡股价获利。所以，长庄股震荡行情，最主要利用的市场能量是人气，也就是市场大众情绪的起伏波动。市场情绪的起伏是有一定节奏的，而行情的一般规律是，人气旺时、市场情绪高涨时买盘强、股价高、市场承接力大；人气弱、市场情绪低落时买盘弱，股价低。庄家利用这种规律反复的调和打击市场情绪，在市场情绪的起伏波动中实现高抛低。这是做长庄的最基本的手法。长庄庄家的市场作用是通过摸顶和探底探索股价的合理值区间，这也是一种价值发现。前面讨论的是一般原则，具体到每一只股上，其业绩和人气特点都不一样，相应的在炒做上各个不同。如同样的业绩，但行业不同、经营者的经营作风不同、与庄家配合的程度不同，都会使其炒做有不同特点。整个市场人气循环一般来说是差不多的，但不同股票也会有微小的差别，这是由于很多投资人都习惯做自己熟悉的几只股票，如西藏人偏爱西藏明珠，内蒙人对伊利股份会多看一眼；还有的人在一只股上赚过钱，以后总喜欢多看一眼这只股，对其走势越来越熟悉，一有机会就做一把；还有的大户和一些庄家有联系，庄家在一只股上长驻，则他们也在这些股上长驻。所以，每只股票上的投资群体都不等同于市场散户总体，每只股上的参与者在行为上都有一些与散户总体不同之处。这种差异是比较细微的，但操盘的庄家可以感觉到，这会对走势有细微的影响。低价绩优和人气循环是市场上两种最常见最一般的市场能量，除此之外，还有其它形式的市场能量，如题材消息等，题材和消息也可以看成是一种类似绩优的市场能量，但不象真正的低价绩优那么稳定，故从炒做手法上虽然类似于低吸高抛的一次性炒做，但只能短炒。<b>怎么炒垃圾股</b>股票的炒做特点决定于股票的性质。垃圾股和绩优股相比，垃圾股进货容易出货难，炒做垃圾股的主要矛盾是出货；而绩优股出货较易进货难，炒做的主要矛盾是进货。第二，绩优股在高位能站稳，而垃圾股在低位才是其合理位置，在高位不容易站稳，只能凭一时的冲力暂时冲上高位。这些性质决定了垃圾股的炒做必须是快进、快炒、急拉猛抬、快速派发。垃圾股平时没有长庄驻守，因为垃圾股没有长期投资价值，随时可以跌一轮，庄家不宜老拿着垃圾股不放；而且庄家在炒做垃圾股时一般是要尽量少收集，只要能炒的起来，能少收集一些尽量少收集一些，所以也不需要预先建仓；另外，垃圾股有容易收集的特点，随时开始收集都不晚，庄家也没有必要早早的抢着收集。行情一旦展开，庄家觉得有机可乘，随时可以调入资金开始快速收集。而且必须快速收集，因为，炒垃圾股必须借势，大势一旦变坏，就要马上撤出来，给他的炒做时间是较短的，必须快速收集。由于垃圾股在高位站不稳，炒高全凭一鼓作气冲上去；也由于庄家不能收集太多的筹码，控盘能力不够，所以，要靠急拉猛抬，造成上升的冲力，带动起跟风者，借这种冲力，把股价炒上去。炒上去后不能向绩优股那样在高位站稳从容出货，所以，一定要快速出货。垃圾股出货就是一个比到底庄家的手快还是散户的反应快的比赛，庄家要抢在散户看出自己出货动作之前把大部分货出掉。第一种办法是少建仓，快速拉起来一点就立刻跑掉，仓位少，出的快，涨幅不大。如果投入资金较多，建仓较多，出货变得复杂，就要做复杂的计划。在上升途中，就要开始出货，在顶部能出多少就出多少，出不了就向下砸盘出货，这样把追涨盘和抄底盘全部杀掉。由于要做好砸盘出货的准备，所以，在拉升时一定要拉出较高的空间，但庄家自己从一开始就没有打算要这么多获利，在拉抬过程中要增加持股成本，在顶部逃不掉时就要杀跌出货，杀跌要损失空间，里外里一算，庄家在中间能拼出总幅度的三分之一的获利来就满意了。垃圾股炒做大量消耗空间资源，但庄家自己的获利则不太高。为什么有的庄家喜欢炒垃圾股呢？炒垃圾股的好处就是快，由于减少了收集和收集后等待机会的时间，炒做过程紧凑，收集、拉抬、派发一气喝成。如果以收集10天，拉抬10天，派发10天计算，庄家可以在六周内完成完整的一轮炒做，如果庄家手快，时间还可以更短。所以，如果以单位时间资金产生的利润率来计算，炒垃圾股是较高的。所以，炒垃圾股是一种短炒行为，就算投入资金较多，开的空间较大也还是要短炒。短炒的特点是愿意牺牲利润换取时间，以追求最大利润率。垃圾股的炒做最适合于利用短期贷款炒做的人。贷款利息按时间计，所以，要速战速决，不能拖时间；一轮炒做的利润率一定要高于贷款的利率，坐庄才有钱可挣，所以，要追求高利润率；由于贷款炒做是借鸡生蛋，只要能获利出来，还上贷款利息剩多少钱都是赚的，所以，并不在乎利润低。<b>怎样炒绩优股</b>典型的绩优股炒做与典型的垃圾股炒做正好相反。绩优股的特点是庄家建仓难，但拉抬容易，拉高后只要不主动砸盘，大势也没有出现暴跌，则可以在高位长时间站稳，可以从容出货。所以，绩优股炒做的主要矛盾是建仓。绩优股炒做是做价值发现，市场给出一个不合理的低价位是犯了错误，庄家只要抓住这个错误，把它兑现成自己的获利就可以了。建仓过程就是抓住市场错误，这是炒做成功的关键，所以是炒做的主要矛盾。与此对比的是垃圾股的炒，要想炒做成功要指望炒上去以后有人会犯错误，肯在高位接自己的棒，所以其炒做的主要矛盾在出货。从博弈的较度看，股市竞局要想取胜依赖于对手的错误，对手犯不犯错误和能不能抓住其错误是取胜的关键，是主要矛盾所在。炒绩优股是抓住市场已犯的错误，炒垃圾股是期待市场在未来会犯错误，炒绩优股比较稳健。垃圾股炒做的基本思路是诱使市场犯错误，绩优股炒做的基本思路是抓住市场的错误充分利用。所以，在炒做思路上绩优股炒做要充分发掘股票的潜力，充分利用上涨空间转化成获利，而垃圾股则要浪费空间，换取散户犯错误。这些特点决定了绩优股的炒做特点，大资金、做长庄、大量收集、稳步拉抬、平稳出货。对一只有巨大上升潜力的股票来说，如果可能，庄家不妨把股票全都收过来，然后拉到合理价位再慢慢出货。实际上虽然不可能把全部股票收到自己手中，但也应尽量多收一些，所以要有大资金。在收集过程中，庄家为了充分利用空间，要低价位收集，所以庄家最怕惊动市场，在自己完成收集前把价格炒上去，牺牲了庄家宝贵的空间资源。一般的做法是隐蔽、长时间吸筹，而比较积极的做法是利用市场下跌逆势吸筹，或主动打压制造恐慌性抛盘，借机吸筹等等。反正庄家要利用一切可能的办法，在低位完成吸筹，完成了这一步，则绩优股的炒做已经完成了一半了。只要在低位吸到了足够的筹码，则绩优股以后的炒做是很简单的。因为庄家锁定筹码后卖压减少，盘子变轻，较容易拉抬。庄家在拉抬初期买入的股票也成了仓位的一部分被锁定，所以越涨越轻。在达到通吃的股上，庄家不必刻意拉抬，锁定仓位不动，股价会自己向上走，盘面上会出现很短的小阴小阳，行情没有震荡的向上飘，一直到庄家的出货位，庄家出货才会引发震荡。绩优股的出货动作只要不太快，是不会引起暴跌的，庄家一般没必要牺牲空间砸盘出货，所以，典型的出货手法是缓慢平稳出货。大部分股票处于典型的绩优股和典型的垃圾股之间，而或者偏向于绩优股或者更接近垃圾股。理解了两极，可以帮助理解一般股票的炒做。<b>怎么炒小盘股和大盘股</b>度量一门股票盘子的大小有两个标准，第一是股票流通盘的大小，第二是股票流通市值的多少。从炒做规模上考虑，一门股票盘子大小不仅决定于流通盘的多少，也决定于股价，比较合理的度量一门股票盘子大小的指标应该是该股的流通市值，也就是股价乘以流通盘。比如，一只流通盘2亿的股票，股价3元，令一只流通盘4千万的股票，股价20元，哪一个盘子更大？哪一个炒起来占用的资金多？当然是后者，前者流通市值为6亿，而后者流通市值为8亿。如果以庄家炒做需要备下总流通市值70%的资金计算，则做前一只股的庄需要4.2亿元，后一只股票需要5.6亿元。所以做盘子大的股票要求庄家更有实力。以总流通市值计算盘子大小，则股票盘子大小是变化的。比如，前面那只股票，如果价格有3元涨到了6元，则6亿元的盘子变成了12亿，盘子变大。如果有一个庄家有4.2亿元，他先以3亿元在3元附近收集了1亿股，占总盘子的50%，还剩下1.2亿元，按当时股价相当于流通盘的20%，占外面未锁定流通盘的40%。庄家把这部分资金用于控盘，也就是用1.2亿元控3亿元的未锁定盘子。如果以控盘资金和未锁定盘子的比例作为度量控盘能力的指标，则此时的控盘能力为40%。以后庄家一路把股价拉上去，假设在拉抬过程中他进进出出，一路不赔不赚，到股价6元时他还有1.2亿元的控盘资金。此时外面未锁定的流通市值已上升到6亿元，庄家的控盘资金占未锁定市值比例下降到20%，庄家控盘能力下降。可见，在拉抬过程中庄家的控盘能力是逐渐下降的，这也是限制了庄家炒做目标位的一个因素。如果有钱，哪个庄家不知道炒的越高越好呢？就算炒上去呆不住，出货时再回吐很大一段空间也还是拉高一些开出更大的空间更划算。但拉多高是要受庄家实力限制的，如果庄家实力不足，控盘资金少，则随着上涨，当控盘能力下降到一定程度后就不敢再往上炒了。所以，要炒多高的上限决定于庄家有多少控盘资金，控盘资金充裕炒的高，控盘资金不足炒的低。随着拉抬庄家的控盘能力下降，这时庄家可以利用临时贷款补充控盘能力，或者卖掉一部分建仓股票补充控盘能力。此时，留下的未锁定筹码越多则为了补充控盘能力需要借的钱或卖的股票就越多。极端情况是绝对绩优股的通吃炒做和垃圾股的短炒。前者锁定筹码极多，外面留下的市值很少，即使股价翻上几倍，也不用补充太多资金就可以维持控盘能力，稍微卖掉几张股票就可以保持控盘能力了，这就成了边上涨边出货，边出货边增加控盘能力打开上涨空间。所以，边上涨边出货不仅是一种出货手法，有时还是炒做所必须的。垃圾股短炒把大量的市值留在外面，随着上涨，庄家无法通过卖股票维持控盘能力，只有通过贷款，但大量短期代款风险很大，所以，这种庄家炒做时上涨空间的上限是一开始就决定了的，缺乏进一步向上打开上涨空间的能力。所以，通吃手法炒做往往可以把股价炒的很高，经常以翻几倍计；而垃圾股的上涨空间则有限，涨个百分之几十就差不多了。下面讨论流通盘的作用，股票流通盘的大小对股票走势的特性有影响。比如，在股票同样市值下，高价小盘股和低价大盘股的股性还是有区别的。在业绩相同的情况下，流通盘小的股票会比盘子大的股票定价高。这是因为，股价由市场供求关系决定，股票的供应量就是股票的数量，股票的需求量就是市场上想买这只股的资金量。针对一只股票，每个投资人都有一个自己愿意接受的价位，也就是说愿意在这个位置上持有该股票。设想把每个价位上愿意持有的人数和资金量进行统计，则可以形成一个柱状图，愿意在高价持有的一般只是少数人，大部分人对一个适中的价位可以接受，还有人只愿意接受极低的价格。市场价格的决定机制是这样的，从出价最高的人开始向下数，一直到需要等于供应量，这个价格就是市场价。市场的交易过程就是在找到这个价格。所以，在需求量分布相同的情况下，小盘股的价格会高于大盘股的价格。在同一个市场上，同样一群投资人，对于业绩相同的股票，需求量分布应该是相似的，但大盘股的供应量比小盘股大，可以有更多的人持有，所以价格自然比小盘股偏低。股票的价格是不断变化的，反映出需求量分布也在不断变化，人们心中对一只股的价格定位也在变化。但供应量的差异是稳定持久起作用的因素，所以，从长期看大盘股的总体价格定位要比小盘股低。统计学中的大数定理告诉我们，大群体的统计特性经常会呈正态分布，人们的一般经验也是如此。如果假设人们对一只股票的心理价位呈正态分布，那么可以有一个推论：大中型盘子的的股票价格定位上的差异不太明显，盘子对价格定位影响最显著的是盘子很小的股票。比如，5千万流通盘和1亿流通盘的股票，在价格定位上的差异不太大，而1千万流通股和5千万流通股的股票，则会有显著差距，并且盘子越小随着流通盘变化价格落差越大。这种现象的原因在于，正态分布条件下，大部分人的心理定价集中在中间区域，中型盘子的股票，定价已经接近这个中间区域了，此时每下降一个价位就会增加很多潜在买盘，所以，以后盘子就算再大上很多，也不会也价格也不会下降太多；相反，小盘股只有少部分人能持有，股价定位在正态分布的尖端，这个区域，愿意持有的人分布稀疏，价格下降好大一段才能引出足够的持有者，所以，小盘股到中盘股一段，盘子增长的绝对值虽然不大，但股价定位的落差则相当大。换一个说法，从心理定位最高的人数起，按正太分布假定，数到前一万人可能价格已经下来了十几元钱，再数到5万人，价格可能只下来2、3元，原因在于向上分布稀疏，下面分布集中。<b>正态分布假定的另一个推论是，小盘股容易震荡，而大盘股不容易震荡。价格的变化是买卖力量变化推动的结果，小盘股在高位定价，参与者分布稀疏，价格上下波动的阻力小，涨跌速度快；买卖力量的一点点变化在价格上都必须有较大的调整才能重新达到平衡，造成波动幅度大。总的说就是容易形成剧烈震荡。</b>盘子越小，上述现象越明显，如邮票盘子比股票小，所以涨跌更剧烈，价格脱离普通人心理定位更远。极端情况是艺术品拍卖市场，名家作品只有一件，无法复制，盘子小到了极限，价格决定于出价最高的一个愿意出多少钱，其价格自然也远远脱离一般人的心理定位，而且容易剧烈涨跌。正态分布假定的又一个推论是，超级大盘股的定价会明显的偏低。这也是由于正态分布尖端人数较少所致。如果盘子大到供应量超过市场心理定价人数最多的峰位，则进一步增加需求量必须在价格上出让较大的空间，与小盘股的道理一样。不过，随着价格降低，会有大量原来对这只股不关心的人参与进来，使参与者的本底变大，所以定价偏离的情况不会象小盘股那么明显。什么叫小盘股什么叫大盘股呢？可以这样估算。中国有2千万股民，近1千只股票，如果平均每人关心10只股票，那么，平均每只股票有20万人关心。统计学中，如果统计样本成正态分布，那么大约70%的样本处于分布峰值加减方差的区间内，高于方差的样本数量约16%，近似的可取15%。可以以此为界，股票少到只有顶端不到15%的人参与的为小盘股。则20万的15%为3万，以中国股市大多数股民一般平均每人每只股票持股不到1000股计算，为3千万股。所以，标准的中型盘子为3千万股左右，可以粗略的估计为2-5千万，2千万以下为小盘股，5千万以上为大盘股。类似的，超级大盘股的标准应为可以供应85%以上的人，20万的85%为17万，估算为1.7亿股，放宽一些为2亿股以上，即中国股市上2亿以上的盘子为超级大盘股，定价会明显偏低。当然，以后随着中国股市的发展，以上估算用到的参数会发生变化，小盘股大盘股和超级大盘股的概念也会发生变化。同样业绩下，流通盘小限制了参与交易者的范围。对两只业绩相同的股票，大盘股定价低，使需求分布中心理定价偏低的人也能买到，小盘股定价高，只有愿意接受较高价位的人才能买到。这使得参与者的群体会略有不同，影响股票走势特性。同样市值下，盘小价高的股票，参与者范围受限制。股市交易是以"手"为基本单位的，股价越高，每手的价钱也越贵，这会限制参与者的范围。极端情况，如美国股市上巴菲特控制的柏克夏公司，股价高到上万美圆，只有富人才买得起。中国股市上，也有很多散户不喜欢买20元以上较贵的股票，而喜欢10元以下的股票，因为资金少，买贵的股票买的太少。盘子大小还影响股票的流动性，同样市值的股票，盘子大，"手"多，每单位的价钱低，相当于在流通时颗粒细，流动性就更好；单位价钱高而盘子小，则分成的颗粒大而少，流动性下降。一般而言，流动性大，参与者多，会增加坐庄控盘的难度，减少走势中的人为痕迹，走势越自然越稳定；流动性小则人为影响大，越容易出现忽涨忽跌和跳跃性走势。所以，大盘股容易走成弧形反转，而小盘股容易走成V形反转；大盘股在分析其走势时应该多考虑群体心理动力学的影响，而小盘股应该多考虑人为因素的影响。<b>小盘股控盘容易，大盘股控盘难。</b>所谓控盘是说庄家持有一笔资金以短炒的方式进进出出，其买卖保持平衡，只通过把握买卖节奏影响和控制股价。所以，控盘难度大小决定于留在外面的未锁定筹码的绝对规模，未锁定筹码多，则参与者人数多，人多则想法多，控盘困难，这和任何活动都是人越多越不好组织是一个道理。通过锁定筹码减少外面筹码的规模可以使盘子变得容易控制一些，但大盘股上即使这样做了之后留在外面的股票仍会很多，不好控制。比如，对一只1亿股的盘子，庄家已经锁定了70%的筹码，但外面仍有3千万股未锁定。另一只天生只有1千万的盘子，即使不锁定筹码，外面的筹码规模也比前者少，控盘难度比前者锁定70%后还低。如果这只股也锁定70%，则外面只剩下3百万股，如果按平均每个人10手计算，则只有3000人参与，如果每天有10%的人参与交易，则只有300人，相当好控制了。而前一只股票要达到同样的程度需要锁定97%的筹码，这显然是不可能的。<b>大盘股由于盘子不好控制，所以，只能炒；小盘股可以实现绝对控盘，适合做。炒就是短炒，不锁定太多筹码，也不炒到太高，随着人气，拉起来就放手，一波一波有节奏的炒，类似于垃圾股的炒法。做就是做价值发现，低位收集大量筹码锁定，实现绝对控盘，拉到较高位置出货。所以，小盘绩优股可以用通吃的手法炒做，而大盘绩优股，即使业绩再好，即使最终可以走成长期上升通道，也只能是一波一波炒上去的，分析其走势可以明显看出庄家进出的节奏，感觉到盘子震荡的力量，而不可能没有力度的往上飘。所以，盘小和绩优是连在一起的，在同样业绩的情况下，盘小的因素会使走势偏向于更典型的绩优股，而盘大因素会使走势偏向于垃圾股。</b>庄家锁定大量筹码，市场上流通筹码变少，中大盘股可以暂时变成小盘股。如果有一个资金较少的机构或资金较多的大户，这时可以短期的做一把庄。不过这种"小盘股"只是市场瞬间形成的，情况稍微一变就不成了。而且这种股背后如果有一个大庄家锁定了大量筹码，则好比"螳螂捕蝉黄雀在后"，大庄家随时可能出手管你。所以，只有在庄家无法出手的时候才能这样做，只有在超跌的时候才有这种机会。当超跌到CYS低于-20，即短线客已经亏损20%的时候，此时筹码都被锁定在上面，不怕庄家继续打压，他打压也不会有人跟着杀跌，他打出去的筹码反倒可能收不回来。此时，临时小庄突然出手帮他控盘，从他口里抢几只小虾，他也没有办法。	股票
前面我们讲到的主要是在股票二级市场坐庄的知识，但是随着证券法的出台和管理层监管措施越来越成熟，中国股市日趋规范。因此，在二级市场坐庄的机构也面临越来越大的政策风险。    在二级市场上坐庄，主力必须遵从低买高卖的原则，如果买进股票后，不去采取措施和手段引导和控制股价向自己预定的方向发展的话，股价无疑会成为一潭死水，甚至走出缓慢下跌的趋势，最终导致被套！相反，如果采取积极措施引导股价变化，即使达到自己低买高卖的目的，势必要以身试法，承受一定的法律风险。这一点，也是大多数主力，经常在管理层脸色下委曲求全的主要原因，也是我国股市被投资者称之为政策市的根本所在！再加上政治面上的风云变幻（比如：中东局势、台海局势等），宏观上的调控手段（比如金融危机），行业内的潮起潮落，甚至包括上市公司经营面上的千折百回等等，都使二级市场潜藏着巨大的市场风险。    但是如果脱离了二级市场上的炒做，从纯粹投资的角度上来讲，目前国内的股市基本上是无法运作的，推动经济发展也成为不切合实际的幻想！不要说大部分上市公司成年累月的没有给投资者分过红，即使分了一点少得可怜的红利，也不够市场上股价一个极小的震荡！从这一点来看，实物资产短时间内，甚至在较长时间内不可能有很高的收益，有的甚至就没有收益！这也是投资者把买卖股票称之为“炒股票”的原因所在，也是很多专家在严格的监管措施下呼吁“水至清则无鱼”的一个重要缘故！所以，炒做是必然的，但同时炒做肯定又是非理性的。投资者投资股票，主要是想通过二级市场上的股票价格的波动赚取低买高卖的利润差价！需要将股权定价以不同方式脱离合理区域，炒作才变得有利可图，才能够将风险转移给别人。但是炒做的过于厉害，要承受法律风险！所以机构投资者们一直在这种矛盾的夹缝里寻求生存。    有没有既能规避法律风险，又能够逃避市场风险的坐庄方案呢？有！    这就是一级市场上的坐庄方案！    和在二级市场坐庄不同的是，在一级市场坐庄，必须首先要成为上市公司的大股东！这是最基本的先决条件，离开这个先决条件，将无从做起！在一级市场坐庄必须逐步地从一个信息的接受方过渡和转换到信息的制造和掌握方！    一级市场上的运做同样离不开资金的支持。选定运做目标后，首要问题就是融资，然后进行资本运做，收购目标的股权，直到掌握和控制上市公司。    譬如：某上市公司总股本是3.6亿，流通股是10000万股，每股净资产2.5元，市价17元，第一大股东持股1.8亿，占总股本的50%。第一大股东进行股权转让，根据我国的证券法规，应以不低于净资产的价格进行出售。主力以每股高出净资产0.3元的价格（即2.8元的价格）高价进行收购，共计投入成本2.8元乘以1.8亿股，合计5.04亿元资金。成为上市公司第一大股东，掌握和控制了上市公司。从表面上看，主力是吃亏了，因为每股多掏了0.3元，1.8亿股就多掏了5400万元，原第一大股东也因为卖了个较好的价格，额外收益了5400万元！但是，主力可以增加一些攻关成本和费用，通过各种关系和途径，拿到管理层的增发批文，在二级市场进行融资。    比如按增发6000万A股，每股以15元比市场价格便宜2元的价格进行增发，共募集资金15元乘以6000万股计9亿元，扣除各种发行费用后，全面摊薄，每股净资产上升至4.5元左右。这样，主力再进行股权转让时，还是参照国家有关资产转让的政策法规，以不低于每股净资产值的价格转让，4.5元乘以1.8亿股合计转让费用为7.9亿元。减去投资成本5.04亿元，净收入2.86亿元，利润率高达57%。这样高的收益，又不担任何风险，何乐而不为呢？    还有一种可以在一级市场坐庄的方法。仍然是采用资本运做的模式，成为上市公司控制性的第一大股东，不同的是这一次在二级市场上融资的方案是配股。但这种以配股为融资方式的坐庄方案，关键的问题是作为主力的大股东必须放弃配股。    譬如：某上市公司总股本是3亿，流通股是20000万股，每股净资产2元，市价12元，第一大股东持股6000万，占总股本的20%。第一大股东进行股权转让，根据我国的证券法规，以不低于净资产的价格进行出售，主力以每股高出净资产0.2元的价格（即2.2元的价格）高价进行收购。共计投入成本2.2元乘以6000万股，合计1.32亿元资金。成为上市公司第一大股东，掌握和控制了上市公司。从表面上看，主力是吃亏了，因为每股多掏了0.2元，6000万股就多掏了1200万元。原第一大股东也因为卖了个较好的价格，额外收益了1200万元！但是，主力仍然花费一些攻关成本和费用，通过各种关系和途径，拿到管理层的配股批文，在二级市场进行融资。    比如：按10：3的比例进行配股，一共实际配售股份数额为总股本3亿减去6000万第一大股东的股数再乘以0.3合计7200万股，配股价格以每股10元比市场价格便宜2元的价格向股东进行配股，一共募集资金10元乘以7200万股合计7.2亿元，此时总股本已扩张至3.72亿股。扣除各种费用后，全面摊薄，每股净资产值上升为3.6元左右。这样，主力再进行股权转让时，还是参照国家有关资产转让的政策法规，以不低于每股净资产值的价格转让，3.6元乘以6000万股合计转让费用为2.16亿元，减去投资成本1.32亿元，净收入8400万元，利润率高达63%。    通过以上案例的剖析，我们发现在一级市场坐庄，既规避了法律风险，又逃避了市场风险，还产生了较高的收益。难怪上市公司在二级市场上圈钱成风！在一级市场坐庄的机理是由于我国上市公司股本结构的特殊性，由此导致了股权转让时计价方式的不同，正是因为这种不同导致了公司再筹资时财富由流通股股东向非流通股股东自然的转移，而控制上市公司决策权的正是非流通股股东，从而起到把一级市场上的风险转移到二级市场上的流通股股东身上！    可是，有时在资本运做结束后，股权并不一定能够快速转让出去，投资上的收益也只是账面形式上的。这样融来的资金不是自己的，又有一个归还期限，怎么办？    这时，可以采取股票向银行抵押的方式，获得资金。先行归还融资。但是仍然享受上市公司的大股东应有的权利！而并不失去上市公司的控制权，其他的利益还可以慢慢的得到，等于白赚了一个上市公司。    另外还有一种办法，就是可以采用一级市场和二级市场相结合的办法共同运做股价，比如把股权进行抵押贷款，用获得的资金借助资产重组的题材，在二级市场对股价进行炒做。这样从一级市场和二级市场同时下手，获得双重丰收，把在二级市场上炒做股价的盈利部分，用来归还当初坐庄的融资成本。这样的无本买卖，简直和空手套白狼没有什么不同。不但白捡了一个上市公司，运做得当的话，还可以赚到大把的现金，真正的一石四鸟。既赚得了实物资产（上市公司）又获取了较大现金的利润，创造实质性炒做题材的同时，还解决了归还融资资金的问题，并且还通过募集资金的陆续投入，改变了上市公司的质量，扩大了上市公司的规模，提升了上市公司的业绩。    以上讲的几种方案，都是上市公司符合再融资政策下的方案，那么，如果上市公司不符合再融资的条件怎么办呢？    除了成为控制性股东后，向上市公司注入好的项目或利润外，还有一种最简单的办法，就是调节财务杠杆，促使上市公司符合再融资条件。    比如A公司上市以来一直业绩平平，由于行业一般，效益一直处于中下游阶段。虽然不至于亏损，但是也很难有大的起色。股性也极度呆滞，这个公司每股收益每年就是0.15元附近，劳心费神的苦心经营，一年到头，并没有很大的收获。那么我们就可以采用最简单的调节财务杠杆的办法，隐瞒净利润，让它亏损。隐瞒的利润越多，报表显示亏损的越厉害，该上市公司到时间的增长速度也就越快！比如报表显示其每年亏损0.5元，连续两年，其实际相当于每股隐瞒利润1.3元，到第三年可以对A公司进行重组，每股收益可以扭亏为赢，提高到0.20元，第四年可以进入成长期每股盈利0.3元，第五年进入高潮，每股收益高达0.8元，这样，又一家中国第一蓝酬股就横空出世了。股价成为大黑马是绝对的，实际上仅仅是财务杠杆的调节的作用，根本就不用费事。　　这样在塑造高成长神话的同时，只需要简单的调节一下财务报表，不但有了理直气壮的股价炒做的题材，而且也符合了上市公司增发和配股的准则。这样A公司就有了上市公司再融资的资格了……	股票
低位如何骗取筹码 我有个朋友的同学，从未涉足过股市，但她对炒股赔钱的人却是嗤之以鼻，经常嘲讽他们太傻。在她眼里，这些人赔钱太应该了。她时常对我说：“低价买，高价卖，跌了就买进，涨了就卖出，这么简单的道理他们都搞不懂？真傻！”我看着她有点迷茫却又充满自负的双眼，心里明白到底谁最傻...... 我朋友的同学在股市圈里可归类为最原始的类型。但现在大部分涉足股市的大中小散户，都3、有一套自以为是的理论——或自己发明创造，或者沿用抄袭，但都无外乎两种：基本分析和技术分析。近几年江湖门派上又崛起另一后起之秀----跟庄分析。这些林林总总、五花八门的方法促使如今的散户越来越难对付。 但是道高一尺，魔高一丈。毕竟很多散户看问题只是注重问题的表象，技术派基本派都无足轻重。但也有一些散户高手经常认真仔细地研究主力的动向，这无疑给机构主力操盘手操盘增加了的不少难度。因为股市就象容器理论一样。比如：一杯满满的水，如果杯子不变的情况下，即使掉进杯子一根细小的绣花针，杯子外面也会溢出些许水来......同样道理，如果在股市流通筹码不变的情况下，大资金的进出，必然会在平静的盘面或K线走势图上掀起些许浪花，或者改变原来的运行轨迹。那么我们有没有办法在偷吃鱼的同时却不粘上腥味呢？下面我们就大资金进入股市的办法做一点探讨： 一.上升通道吸筹方法： 选择在黎明前黑暗或者大盘人气悲观的时候，借着夜幕做掩护，悄悄地走出不为投资者所注意的缓慢上升通道。这个时候，一般抛单多为割肉之举，机构操盘手可以隐蔽吸纳廉价筹码。当盘面上档有大卖单挂出的时候，可以分批或照单全收，但这样势必会引起股价上涨，导致市场人士警觉。那么我们完全可以以实质性买单买进上方大单筹码的同时，稍晚几秒用另一台电脑确认向下以实质性卖单卖出极小部分的股票，这样利用人气不旺或市场人士都不注意时，既不引起价格上涨，又能吸到廉价筹码。采取多进少出的形式，吸纳廉价筹码。当上档无大卖单出现的情况下，无论大势涨得再好，千万要按兵不动，不要徒劳去做高股价。如果有小卖单主动出现，也不要去维护盘面（如果筹码还不足的话），让股价自由地回落，直到再次有大卖单出现......也有的时间，由于大势不好的原因，上档不断有大卖单出现，主力只好抓紧吸筹力度......这样虽然多进少出，一天下来，不知不觉中到收盘时，在日K线上也做成了个中阳线。但由于还可以在相对低位再次吸到廉价筹码，增加相对低位的成交密度，降低持仓成本。这时可以趁收盘时，非职业投资者注意力比较涣散，下档承接盘稀薄时，再次用少量筹码将股价打下。第二天可以在此价位反复吸纳筹码，争取在交易的相对低位增加筹码的密集程度，增加低位的筹码吸收量，降低持仓成本。但随着时间的推移和某个价位上的流通筹码日趋减少，总是固定在某个价位区域很难再吸到廉价筹码了。可以小幅度地以实质性的小买单稍稍推升股价，然后进行新一轮的吸筹。虽然此时的成交量绝对是真刀真枪的买进，但因为量能不大，大多也不会引起市场投资者的注意。但随着时间的推移，日积月累，仍然会在日K线或走势图上留下小规模的上升通道。并且由于股价长期的稳定，会给一些散户投资者造成一种安全的感觉，而不愿低价卖出筹码，造成主力吸货被动。当这种吸货方法进行一段的时候，渐渐会引起专业人士的注意。那么没关系，下面我们将采用下跌趋势或者打压的方法建仓。 二.下降趋势或打压方式建仓： 要做出一个下降趋势或采用打压方法建仓，必须要抛出一部分筹码。如果抛出的筹码太多，反而适得其反，得不偿失。我们先谈一下下降通道的运作方法和吸筹机理吧！ 1.下降通道.选择大盘下跌或者人气低迷、悲观的日子开始建仓。首先利用开盘时，下档承接盘稀薄时，抛出小量筹码，就可将股价造低。由于大势不好，人气低迷，市场上空头气氛较浓，再加上股价下跌，技术形态难看，一般散户投资者都会纷纷抛售筹码。这时主力可以在下档的买单上分批挂进小量买单，守株待兔，待成交完毕后再行挂出买单，周而复始；也可以根据需要，使买单的重心逐步下移或者横盘......在尾市的时候再次利用人气涣散和承接买单稀少时打低股价，连同自己当日买进的筹码造成主动被套......几次三番股价重心逐步下探.......由于股价逐步下跌，一些散户投资者会逐渐产生惜售心态，或者摆出一幅死猪不怕开水烫的模样，这样势必会影响到主力吸筹的效果。这时可以用实质性买单分批吃进上档卖单，制造小幅反弹，而后再度步入下跌通道，给上方套牢者或散户投资者施加以更大的心理和思想压力，迫使他们交出筹码...... 2.打压吸筹：也就是通常说的炸单。 此种方法通常适用于大盘或板块人气极度悲观或者个股有利空袭来的时候。在恐慌气氛正浓，下档又无人承接时，在下档首先埋下大单子，然后以小单子向下卖出，促使关注或者持有该股的人在股价不断下跌的心理压力下，眼看着下档买单一点点被卖单吞噬掉，忍不住产生一种想卖出去的冲动，可下决心把股票卖出去后，股价也就见底回升了......其实主力还是把大多数筹码自己卖给自己，只是向下诱导股价，大多数投资者看到股价下跌而且带着成交量，都纷纷卖出，其实正中主力下怀！但这种方法只适用于市场形象差和主力仓位较轻的股票。如果市场形象较好的个股采用此方法，往往会在炸单的同时被其他主力机构捷足先登，投鸡不成蚀把米，反而会弄巧成拙。主力仓位较重的也应回避此方法，否则会造成一些跟风或者炒底盘介入者成本比自己还低，对自己以后的拉抬和出货形成不利局面。 三.横盘吸货：该方法多用于绩优大盘股上。有时候由于个股基本面非常优良，股价一有异动，就会引来大批的跟风盘，造成主力没吸到货股价已经涨了开去的局面。但如果采取向下打压的方法，恐怕手中打压的筹码就象肉包子打狗，有去无回，会被其他的散户和机构一一接走。这时可以采用横盘吸货的方法吸货，在大盘上涨的时候在上档的阻力位处放上虚张声势的大卖单，适时阻止住股价的上涨，吓走多头；在股价下跌的时候，在下档分批埋上小买单，吸纳筹码。在跌到关键位置时，在支撑位上放上大买单，吓跑空头。这样股价在上有盖板，下有托盘的区域内运行，主力可尽情吸筹；也可以利用主动性的买卖单量控制股价，走出平台走势。由于平台横盘时间较长，有时连个差价也难以打出来，看着其它的股票潮起潮落，频频有差价可赚，绝大多数投资者都会耐不住寂寞，抛出廉价筹码，去追求短线收益......笔者的一个朋友，在1999年2月份的时候以8元的价格跟随主力一道买入0539粤电力1万多股。后来的日子里，0539股价始终控制在8.3元到8.9元之间，很难做出一个象样的差价。就这样一晃三个月过去了，我的朋友情绪开始逐步变坏，看着别的股价上上下下，嘴里骂主力：“你不涨，你就是跌也行啊，你个鳖孙！”在上涨的前一个星期，他终于挺不住了，几乎平手出局。在挂单卖出时，嘴里还不停地嘟哝：“你个鳖孙，以后涨到100元，老子也不稀罕！”后来该股3周时间，涨到17元......可是他从该股上撤出来后，99年一年的收益还不足30%，现在回想起来，还是后悔莫及！相同的例子，不胜枚举...... 四.以上三种吸筹方法是最基本的吸筹手段，可单独使用，也可根据市场人气灵活组合运用。通过组合使用，可以在股价的历史底部区域不断地塑造一些经典的头部状态，诱使跟风盘和上档的套牢盘交出筹码。比如1999年3---4月份笔者以实战资金买入的0024招商局（原名深招港），该股从7---8元附近小幅推升到10元左右时主力已经吸纳了一定数量的筹码，然后主力利用手里的筹码在10元区域划图。在股价的历史底部区域，做出一个标准的头肩顶形态。笔者在挂单买进的时候，看见持有0024的人都在抛售，他们对我的行为感到奇怪，指着图形对我说：“老兄，很经典的头肩顶啊，你钱没地方花是不是？”我说：“头肩顶形态是真的，但成交量反过来证明头肩顶的意义是假的。至少这个股目前是如此。”后来，我就在该股的头肩顶形态宣布正式成立时以8.7元买进，后来该股又下跌了0.3元左右，一个月后，该股最高摸至16.7元附近，令当时不少的投资者咋舌......再举一例说明，2000年度5---6月份的时候，0653ST九州，主力从4.15元先开始以小的上升通道吸筹，然后在5.9元附近又采取横盘吸筹，接着趁大势低迷，又走出下降通道吸筹。如此反复，到7月份，在历史底部区域反复做了三个经典的双头，但等到股价拉高到7---8元时，在5块多抛出的散户才明白，原来不是双头，而是个三重底结构啊.......对于很多散户来说，他们只注重形态和形态的重要性，很少有人注意形态所处的位置和形态代表的意义。更多的人喜欢研究股价上涨和下跌的表象，极少有人愿意去深入地研究股票上涨和下跌的背后实质和机理，这也就是人们常说的功夫在图形之外。 五：无论在吸筹的基本方法单独运用或者组合运用上，都可以利用技术形态或者技术指标进行有效的配合。由于一些市场行为的研究者们也分为两大门派：技术形态和技术指标，尤其是一些短线客，往往企图在市场上刻意寻求一种战胜市场的捷径，而对技术指标崇拜有加，这将会给我们造成可乘之机。我们在研究技术指标的同时，更注重研究技术指标的计算方法和原理，找出其内在的真实机理，加以控制和运用...... 比如市场上最常用的KDJ指标： 其计算公式和理论上的依据： ct-ln n日RSV=————X100 hn-ln 其中ct（代表当日收盘价），ln代表n日内的最低价，hn代表n日内的最高价。 我们把它们代入公式后就会显得更加简单明了： 当日收盘价 — n日内的最低价 n日RSV=———————————————— n日内的最高价 —n日内的最低价 对RSV进行指数平滑就得到如下K值：. 两日的RSV等于昨日K值 2 1 今日的K值=—昨日K值+—今日RSV 3 3 对K值进行平滑后得出D值： 2 1 今日D值=—昨日D值+—今日RSV 3 3 其中，首日D值取50 由此我们可以看出，通过股价异动---拉高或者打低收盘价和当日内的最高价和最低价或者低开、高开等等，可以改变和操控技术指标，促使这些指标为我们服务。 其它技术指标，与此大同小异，我们在这里仅举一例，限于抛砖引玉...... 六.涨停板吸货方法和跌停板吸货方法： 我们在研究散户的心理时，发现一个非常有趣的现象，大致也可以归纳为三类： 第一类散户：股价上涨时不怕，跌时害怕。这种散户比较普遍。他们在股价上涨时兴高采烈，稍有震荡就会逃之夭夭，往往是追涨杀跌，买高卖低，股价翻了一倍，帐上却没收益。遇见股价震荡幅度稍微大点，往往还会出现亏损，这就是人们常说的赚了指数赔了钱。 第二类散户是套不怕，不怕套，怕不套的类型。他们往往在股价下跌的时候被套住了，就会安心持股，甘愿被套，有的甚至一拿几年。如果有一天股价稍微上涨或保本，或稍有赢利，他们也会束手无策，生怕股价再次下跌，失去逃跑机会，再次挨套的恐惧心理会促使他们不问青红皂白，夺路出局，往往股价在他们出局后会一路高歌，青云直上...... 第三类散户喜欢静态地看待事物。喜欢从技术指标或形态上寻找短线机会。譬如箱体或者平台，进行低买高卖，赚取蝇头不利....... 根据以上散户的心理状态，可以采取涨停板或跌停板方式进行吸筹： 比如0653ST九州，在2000年7月27日以前的时候，主力在4——6月份的三重底结构内吸纳了一定的筹码，后来由于大势热点集中在重组股和ST类个股上，主力为了抓住战机，在7月27日上午，决定进行最后一次吸筹。当天上午10点左右，该主力在突破前期3次都没有冲过的高点5。97元价位后，于10点左右把股价打至涨停价位6.09元处，借助前期跟进散户三次在箱体顶部没卖而坐电梯的后悔莫及心理，一上午之内，五次打开涨停板，并且放出巨大的成交量，促使所有持有0653ST九州的投资者，惊慌失措，生怕不卖出去，辛辛苦苦赚到的蝇头小利会象前几次一样再度失去！于是散户竟相抛售......主力成功接纳了大批的廉价筹码，直到下午开盘，浮码才渐趋稳定，股价牢牢封在涨停......第二天股价再度涨停，第三天摸高到涨停后开始回调后，然后又接连四个涨停...... 跌停板的吸货方法；1999年12月份，0682东方电子经过长期的横盘后跌破平台，走出跳水行情，12月27日该股以跌停板开盘，使跳水行情进入白炽化状态，散户恐慌抛盘如云，最后这些廉价筹码都被主力在绝对低位一网打尽，从而展开一轮翻番的拉升行情...... 涨停板吸货方法的运用：涨停板的位置最好选择在前期的投资者都能预测到的阻力位附近。这样经过反复的打开涨停，更能促进散户对后市不确定性的遐想，增加心理压力，引发抛售欲望，然后再吸取筹码。 跌停板吸货方法运用；在强势市场里，选择有利空袭击的个股，破坏技术和走势形态，引起止损盘抛售；或采取炸单的形式，促使日K线上阴线越跌越快，越跌越大，促使持股者的心理压力大增，引起投资者不抛售将会下跌空间无限的恐惧感。白炽化状态时以大单封住跌停，这时散户的抛盘竟相而至，待散户抛盘达到一定程度时，主力可釜底抽薪，采取撤单形式撤走自己的卖单，然后以实质性的买单大肆吸收上档的抛盘，达到快速建仓的目的。 关于吸筹期间媒体舆论的应用： 主力在吸筹期间，总会有意无意地留下痕迹来，引起一些舆论媒体的关注。如在吸筹末期，可趁势拉抬股价，突破上档阻力位；如果在吸筹初期，在跟风盘不太旺盛时可反向操作，向下打压股价，促使股价下跌，吓退欲跟进者，如果吸筹不多，而散户又跟风旺盛，主力可暂放弃操作，让股价短期内自生自灭。待媒体舆论的影响力逐步消退后，可利用手中筹码再行打压股价，造成主力出货迹象，诱使散户割肉出局。 九.总述： 总的来说，在主力吸筹期间，主力无论是从技术指标或技术形态、K线组合或股价走势，一般多采用反向操作，坚决拒绝与投资者和技术人士对话，促使他们进也错，出也错，不断追高踏低，左一耳光，右一巴掌，打击其跟进的信心。也可通过技术指标、技术形态、股价走势，利用媒体舆论等工具向投资者撒谎，影响投资者正确的判断能力，最终导致投资者错误的决策，从而达到低位建仓的目的。 六.拉高建仓： 这是一种利用散户惯性思维逆向操作的一种典范。由于市场人士大多会认为主力为了降低成本会采取打压股价进行建仓或主力建仓往往在低位的习惯性思维而采取的逆反手段，把股价推升至相对高位建仓的一种方法。 但这种方法的使用必须依照几条先决条件：1.股票绝对价位不高；2.必须确定大盘处于牛市初期或中期；3.公司后市有重大利好或大题材做后盾；4.有大比例分配方案；5.有足够的资金控盘，具备中长线运作思路。以上条件缺一不可。 这种建仓方法的好处是能够加快吸取筹码的速度，缩短建仓时间，争取拉升战机。这种方法往往容易误导投资者，促使其发生方向性的错误判断。市场人士往往把主力推高建仓误解为主力拔高，推高后的建仓行为理解为主力派发，这样主力就很容易买进大量筹码。虽然主力成本相对较高，但由于大盘处于大牛市的初中级阶段，个股背后有重大题材和利好，在推高建仓的背后，个股就会存在更大的涨幅。比如0425徐工科技（原名徐工股份），从19995月份起，主力开始推高建仓，埋头苦吃筹码，把股价从9块多钱一路推高到13元附近，然后又向下缓慢盘跌到12元附近，很多市场人士认为主力在出货，竟相逃走，主力顺利接走大批筹码。笔者认识的一位投资者，在该股没有主力的时候持有该股长达两年时间，苦苦等待拉升机会。可惜在主力推高建仓的过程中，看错方向，误以为主力出货，在12元的时候逃走了。后来该股经过一年时间运作，股价从主力建仓成本的12元附近一直上涨到最高49元（复权后），涨幅达到300%！ 这种方法采用逆向思维，但必须遵照一定的先决条件。比如在熊市时，或股价绝对价位较高或个股基本面无重大变化时忌用。如果有很宽裕的建仓时间的话，一般也不提倡用此方法，毕竟会在一定程度上增加主力运作成本。	股票
拉升前的准备工作 一般说来，标准主力一般将资金划分为两大用途。用资金比重的70%左右做为战略建仓部分，剩余30%左右低吸高抛做为滚动拉抬股价之用。 经过我们前期种种努力的运做，基本上已经完成战略建仓的过程。下一步就是要选择适当的时机，进行拉抬股价，促使股价脱离成本区域。 在两军作战之前，为了做到知己知彼，百战不殆，军队一般都要派出一股先遣的小组，进行交战前的侦察工作，以探明敌我双方的力量悬殊，现在部队上称做“侦察兵”，古时候所谓“探子”。 在股市上也一样。作为主力，在准备大幅拉抬股价之前，也应该了解和掌握多空力量的悬殊，以及市场对该股股价的认同程度和其他投资者的持筹意愿。这时候，就要选择在合适的时机，用小部分资金拉抬和打压股价，以探明市场对该股的反应和其他投资者对该股的持筹意愿。根据市场和其他投资者不同的表现，再采取相对应的策略，进一步运做股价。这个过程，就叫作“试盘”。我们看“试盘”和用兵打仗上的“侦察兵”的功能是不是很相近呢？ 当然，不是运作每只股票都要试盘的。这就象在用兵打仗上一样，在敌我力量悬殊极大的情况下，比如敌方只有一个班，而我军有一个团或者一个师，那就不用大费周折，对方肯定不堪一击。在股市上，如果主力掌握流通盘绝大多数筹码，市场上只存在一定意义上的流通筹码的话，则试盘就失去了其真正的意义。试盘的主要目的也就是为了更好地掌握主动，利用技巧造市，尽可能有效地降低主力运作成本，从而起到火借风威的良好效果。 试盘一般采取以下几种形式： 1.稚莺初啼：主力经过长期的历史低位横盘吸筹后，筹码达到资金预先部署的部分时，主力便开始了试盘的动作。用小部分资金向上买进股价......但由于股价长期处于历史低位，对散户投资者形成了习惯性的思维定势。股价稍一上涨，浮码立即涌动。由于主力还没有培养出跟风盘，所以在日K线上便落下长长的上影线。 比如0931中关村，在2000年度1月21日和1月24日以及25日、28日，短短的6个交易日内留下了5个长上影线。但这些并不能阻碍主力做多的信心。2000年1月21日，该股拉升未果，在日K线上留下了长长的上影线。无奈主力只好在1月24日进行洗盘，1月25日高开高走，再次遭遇空头大军，又以长上影线报收，无奈26日再次洗盘，以长阴线报收。经过1月26日的长阴洗盘后，该股在27日上攻时，浮码明显有所减少，但仍然制约主力上攻步伐。元月29日，主力高开后再度洗盘，又以中阴线报收。2月14日该股主力高开后，发动攻势，空头大军全线崩溃......该股仅以2%的换手率直封涨停......短短的12个交易日，该股从20元起步最高上冲到44元，股价上涨100%多......从1月21日到1月28日，短短的6个交易日内留下了5个长上影线。从股价不间断的拉高打低和高开低走，低开低走，反映出主力焦躁不安，急于拉升的意图，也同时从侧面反映了主力做多的信心和决心。 再如，1999年4月28日，0733振华科技，主力吃饱喝足后在历史低位拉出带长上影线的中阳线，证明上档仍然存在不安定的浮码，主力为了彻底清除持有该股的不坚定分子，在该价位附近采用横盘整理的办法清洗浮筹。在横盘了12个交易日后，该股在一个月内走出一波轰轰烈烈的翻番行情。 相同的例子还有：0578数码网络，原名青百股份，主力吃饱喝足后于1999年12月27日一举高开到5日、10日、30日、60日均线上方，当日走出冲高回落走势，股价带出长长的上影线，成交量也有效放出。后来股价在均线上方缩量整理数日，该股便飞云直上，连连翻几番，涨幅高达300%以上。 在历史低位，主力经过长期横盘吸筹后，在日K线上留下放量的长上影线，往往露出了主力试盘的痕迹，表明了主力蠢蠢欲动的心理，随后股价稍做整理，清洗浮筹后，一波行情就会呼之欲出...... 该方法往往预示着拉升之前，上档仍然存在着一定的浮动筹码。作为主力，应该在随后的日子里彻底清洗这些浮动筹码，以免在日后的拉升过程中成为绊脚石，增加运作成本...... 2.金针探底：此试盘方法和稚莺初啼则完全相反。稚莺初啼是向上拉升，测试上档压力和抛盘，而金针探底则恰恰相反，是利用手中的筹码向下打压股价，测试下档的承接力和原投资者持有筹码的稳定性。由于采用稚莺初啼的方法试盘很容易引起投资者的注意和跟风，当日后展开拉升行情的时候，容易造成低位跟风盘过多，获利客竟相出逃的局面，造成主力拉升被动。而金针探底则回避了上述的风险，采取向下打压的措施，反而在上升之前捡拾到一批恐慌性廉价筹码。这样既测试了筹码的稳定性，又捡到好处，是不是得了便宜又卖乖呀？ 比如0679大连友谊，该股主力在1999年12月27日以前的平台上吸足筹码后，于12月27日以10.90元开盘，最低控盘打压到10.38元主力反手开始收集不坚定分子抛售的廉价筹码，当日以长下影线的“十”字线报收，成交放出天量。试盘以后，该股又经历4天的洗盘调整，主力开始发动攻势。1个半月时间，最高涨幅达到130%左右...... 再比如0815美利纸业，该股在历时一年的横盘之后于2001年3月5日开始小幅放量向下试盘，早盘开盘于13.2元（高开0.02元），主力开始控盘打压，于上午10点左右最低下探至12.24元，下跌0.96元，然后主力开始一路买进，于10.15分该股最高上摸至13.45元，终盘以13.33元报收，在日K线上形成刺穿4条均线（5日、10日、30日、60日）的带长下影线的放量小阳线......第二日该股又故技重施，又收出带长下影线的小阴线，只是幅度稍微减小，第三日试盘完毕，开始新一轮的攻势。一个多月后，该股涨幅高达40%左右...... 3.双针探底：该方法与金针探底大同小异。顾名思义，就是在间隔的一段时间内出现两次金针探底，并且量能都有效放出，并且双针探底的位置能够保持平行，更能证明原投资者持有筹码的稳定性和下档的踊跃的承接力。双针探底比起金针探底给人的感觉更为牢固、稳健和踏实...... 比如近期的600186莲花味精：主力在完成最后的吸筹后从2001年4月16日开始回调。于4月23日控盘打压股价，向下试盘，最低探至10.67元30均线处，开始遭遇踊跃的承接盘，股价以11.33元收盘，当日收出带长下影线的中阳线。随后几日，股价开始小幅盘升。于5月11日，再次临近前期高点时，主力再次控盘打压股价，该股当日从开盘的11.6元附近再次探低到30日均线11.14元，遭遇多头积极买盘，收盘11.74元。当日日K线再次收出带长下影的中阳线，量能温和放大。这时日K线图上4月23日金针探底和5月11日金针探底遥相呼应，形成标准的双针探底，在走势图上形成一道靓丽的风景线，给人以踏实牢稳的感觉。新的一波升浪呼之欲出。果然5月14日，该股走出凌厉的上攻行情......短短的十几个交易日，股价升幅40%左右，最高上攻到14.86元，该股才进入中线洗盘阶段。 4.多针探底：主力吸足筹码后，在股价历史低位或相对低位，为了测试筹码的稳定性和下档的承接力而采取多次控盘打压，在阶段时间内的日K线上形成了N次金针探底的形态。此方法比起双针探底更显稳固可靠。 比如前期的0975乌江电力，主力历时一年的箱体吸筹后于2001年3月30日，该股主力开始在相对低位控盘打压， 在日K线上留下放量带有长下影线的中阳线。4月6日该股主力再次控盘打压，再度收出带有长下影线的小阳线。4月9日，主力高开后第三次打压股价，日K线上三度收出带长下影线的高开阴线。4月11日，主力第四次控盘打压股价，虽然日K线上收出极长下影线的小阳“十”字星，但量能日趋减少，进一步证明浮码逐渐减少，承接盘暗暗增加......但该股主力为了确保万无一失，仍然再次对敲向下进一步清洗浮筹。半个月以后，该股逆市上扬，成为阶段时间内市场上鲜有的闪亮登场的黑马...... 5.探底阳线：稚莺初啼的试盘方法容易吸引短线跟风盘，造成短线获利客众，容易制约主力拉升，造成主力运作成本提高。而采取金针探底和双针探底，又极易遭遇其他机构和猎庄者突然袭击，造成打压过程中主力筹码流失。因此新的试盘方法就应运而生——探底阳线。 探底阳线既回避了稚莺初啼和金针探底及双针探底的短处，又吸收了它们的优点，可谓扬长而避短——即：顺手牵羊捡拾恐慌性廉价筹码，又不至于流失筹码和吸引跟风盘，还能顺利测试到筹码的稳定性和承接力，可谓一石三鸟。 比如0682东方电子，主力经过长期横盘吸筹后，在1999年12月27日，该股主力早盘集合竟价时，以跌停盘14.5元附近开盘。主力当天顺手牵羊收集恐慌性抛盘，最高上摸16.3元，尾市以16.02元收盘，单日放出379488手巨量，也从侧面证实了筹码还是比较稳定的。随后3个交易日，股价略做整理，便一飞冲天，从最低14.5元，仅一个月左右，便上冲至45元，涨幅高达300%。 再如：600103青山纸业：该股主力于2000年度在9--10元的平台区域，经历漫长的吸货后，借内部职工股上市的契机于2000年6月20日早盘将股价瞬间低开到8.56元，当日低开6%，随后股价一路走高，收于8.89元。当日放出325823手巨量。第二天，该股主力再次试盘，将股价再次低开0.17元。然后开始收集浮动筹码。最终收盘于9.27元。由于两次试盘成交量都比较大，说明筹码的稳定性有待加强，因此该主力随后又连续强势整理7个交易日，使市场上的浮动充分换手后，才展开为期两个月的拉抬行情........ 6.二阳开泰：所谓二阳开泰，也就是探底阳线的翻版，和双针探底有异曲同工之妙。即：在相对低位出现两次或多次历史低位平行阳线。 比如0973佛塑股份：在2001年2月28日出现大幅低开阳线，同年4月19日再度出现大幅低开阳线。二阳线平行出现于长期低位的缓慢上升通道中，都表明主力试盘的意图。虽然该股后来短期内涨幅只有20%多一点，但同样蕴含着套利机会。 再比如：近期的0858五粮液的走势。该股在2001年2月21日走出金针探底的走势。最低探到33.16元开始见底回升。2001年4月25日该股以34.11元开盘，低开2.69元。形成探底低开阳线，与2001年2月21日的金针探底形成呼应，2001年6月22日，该股再次低开0.76元，以37.56元开盘，与昨日阳线形成低位并列双阳，和2001年2月21日金针探底；2001年4月25日的探底阳线；形成首尾呼应。一个金针探底，外加一个二阳开泰。主力试盘的意图是不是很明显啊？如果买到这样的股票，不想赚钱真的很难。 2、7.高开阴线：由于有的散户抱定亏本不卖的理念，采取金针、双针、多针探底或低开阳线反而对之无效，所以迫使主力必须往上做。而稚鹰初啼的方法反而会增加其观望的态度，所以只好采取高开阴线的方法进行试盘，以测试这种处于犹豫和彷徨边缘的散户之态度。这种试盘的方法包含有换手洗盘的味道。 比如600614胶带股份：主力在长期吸筹后，在相对低位附近，于2000年9月13日、9月28日、2001年1月24日分别进行了三次毫无道理的大幅高开低走，主力试盘痕迹明显。尤其是2000年1月24日，第三次高开，从放量高开低走上可见散户投资者竟相出逃之一斑。后经历一周的小幅调整、清洗浮筹，主力开始发动拉升行情，一个半月时间，股价上涨到了35.86元，涨幅高达100%。 8.实力主力的最后一踹：该方法和低开阳线的区别在于时间和空间上。从时间跨度上来说最后一踹的方法要比低开阳线的时间跨度长得多，也可能是几天甚至十几天。从空间上来说也要大于低开阳线，可能是百分之十几，甚至有的达到30%。从主力的魄力上来看也明显大于其它试盘方法。这种试盘方法资金实力较小和无魄力者禁用，否则会弄巧成拙。必须具备有统吃筹码魄力的主力方可采用......此方法包含最后一次诱骗筹码的含义。 这种试盘方法，在大黑马股中使用比较普遍，并且也很能迷惑局外人。大凡经历过这种试盘方法测试过关的投资者，绝大多数成为主力的铁杆追随者。此方法是技术派人士的天敌。因为绝大多数技术人士往往会在最后一踹中抱着宁舍三池而不愿亡国的态度止损出局......这种方法可谓阴险毒辣。以下试举两例略做说明： 比如0008亿安科技（原名深锦兴）：主力收集不少筹码后，在1999年12月份，利用个股利空，居然连跌三个跌停，跌破长达几个月的收集平台，散户蜂拥而逃，主力一一笑纳。随后主力一路吃进，又将股价拉回8——9元的平台位置，整理十几日后，主力开始发动长达两年的行情，股价涨幅高达10几倍，创造了股市上的百元神话！ 再比如0429粤高速：该股是上市后即有主力先行收集筹码，历时一年之久。在拉升的前几日，借导弹事件，于1999年5月17日炸出历史新低5.33元。但是1亿多的流通盘当日成交只有2000多手，足以表明绝大多数筹码被主力拿走，剩下的全是铁杆股民，筹码之安定可见一斑......经过试盘后，主力开始拉抬股价，历时一年，涨幅达到4倍之巨...... 小结：作为机构主力操盘手，应当仔细辨别各种试盘方法的优点和缺点以及内在的真实机理，而不是表象。要根据当时的市场状况采取正确、科学、合理的试盘方法对盘面进行测试，也可以根据不同阶段的市场采取不同的试盘办法进行组合，灵活掌握运用。根据测试的不同结果采取相应的策略，清理绊脚石，为日后拉升股价做出准备。	股票
拉升方法的运用准备工作充分后，接下来要考虑的就是如何拉升股价：    主力操盘手应根据具体情况采取相对应的拉升方法。比如运作项目流通盘的大小，自身资金实力强弱以及同期的市场背景（如政策面对股市的调控等），选择最佳的拉升方法。    从拉升的形态上大致可分为以下几类：1.震荡拉升 2.台阶式拉升 3.单边上扬式拉升 4.火箭式拉升；    从拉升的方法上又可以分为：1.压单对敲拉升 2.直接对敲拉升 3.梳式对敲拉升 4.哄鸭子过河 5.推土机式拉升 6.钓鱼杆式拉升。    那么究竟该选择哪种适合自己的拉升方法呢？这还要从我们熟知的基本面和技术面去考虑。依据上述两大因素，详细阐述选择各种拉升方法所处的时机。    一、震荡拉升式：    对于运作项目基本面无重大题材或者市场监管部门监管力度加大以及主力资金不够充裕或实力较差时多采用该种拉升方法。    该拉升方法主要采取低吸高抛的方法，以波段操作博取利润差价为目的，以时间换取空间为手段进行运做。在市场上经常表现为低点和高点逐步上移，走出比较规律的宽幅上升通道。主力可在上升通道的下轨积极吸纳筹码，在上升通道上轨附近进行高抛。股价呈现出较规律的震荡上扬走势或者说是向上倾斜的平行箱体。主力正是通过这种反反复复低吸高抛的措施从而在二级市场上博取丰厚的利润。    这种拉升方法颇有将各类风险化整为零的味道。可谓好处多多。既回避了来自管理层的监管压力（因为升幅总是不大，在市场中算不上树大招风。），又节约了资金成本，还能回避由于项目基本面过于一般没有重大题材而招致猜疑等不利因素，可谓一石三鸟。    比如近期的0811烟台冰轮：该股从2000年1月14日的低点10元附近，历时1年零5个月，共经历了三波拉升，最高于2001年6月中旬震荡拉抬至18元左右，总涨幅高达80% 。该股第一波行情是从2000年1月14日的10.5元附近起步，拉升到5月30号的15.3元附近，上涨50%左右。从5.30号以后震荡下滑，于2000年7月13日下探到13.3元附近；第二波行情是从7月13日的13.3元开始缓步上扬于2001年1月4日的17.8元，升幅高达40%－－50%。之后一路探低至2001年2月21日的14元附近；第三波行情历时3个多月拉高到18元一带，涨幅高达30%左右。目前运行的正是第四波行情……    二、台阶式拉升：    台阶式拉升与震荡式拉升的不同之处，从形态上看，台阶式拉升在股价上涨了一定幅度后采取平台或强势整理的方法，经过清洗或赢利盘换手后再度拉升，股价呈现出台阶样步步高升，而震荡式拉升则采用低吸高抛逐步上行的方法；相对于走势来说，震荡拉升远比台阶式拉升要复杂的多。台阶式拉升比震荡式拉升的走势上显得更为简单明快。    台阶式拉升适用于主力实力较强、运做项目基本面优良、后市存在重大题材的大盘绩优个股。这种主力操作风格通常较为稳健。    由于运做项目基本面比较优秀，后市存在重大题材，而且主力实力也非同一般，所以主力运做起来得心应手，信心十足。但由于运做项目流通盘较大，主力性格又沉稳老练，在股价拉到一定涨幅的时候往往采取横盘的方法，经过长期换手后，清洗下档跟进的获利筹码。在大盘或者人气较旺的时候，主力适时抛出一部分筹码压制盘面；在大势或者人气较差的时候，主力又适当地买进一部分筹码进行护盘，由于长时间股价处于横盘状态，保持不涨不跌的态势，从而促使下档早期跟进的获利盘出现焦躁不安的情绪，信心不坚定者草草出局，信心坚定者继续持仓，而看好后市的新多头此时兴高采烈地入场买进。这样经过充分换手，采取不断提高他人投资成本的方法，从而为下一波拉升行情打下坚实的基础。反复运用这种手法在日K线形态上就形成了股价像楼梯一样逐级上升趋势。这样的拉升方法还有一大优点，由于前期股价在低位徘徊时间较久，无形中给大部分投资者造成了一种心理定势。当股价从低位启动上升到一个新的境界时，一般投资者站在相对高位开始有点犯恐高症，觉得赚钱了或者有风险了从而不认可新的价位。而主力采用这种横盘方法的目的就是，经过长期横盘从而形成新的价值中枢，进一步获得大众投资者对新价位的认可和赞同，这一切都是发生在潜移默化中，也是由量变到质变的结果。    比如，0429粤高速。该股上市以来在6元附近长期横盘，时间长达一年之久。主力吃饱喝足之后将股价从6.8元附近打压到5.3元，创出历史新低后，一鼓作气将股价拉升到9元区域。由于股价从5元附近上涨到9元，涨幅达到80%，况且该股当时流通盘较大（一个亿左右），新的价位由于短时间内得不到广大投资者的认可，于1999年6月25日该股产生一波小幅度的下挫行情，一周后即7月5日，主力把股价重新推回到9元，并构筑了一个长达5个多月的平台，使下档的获利盘充分换手，在新多头的投资成本进一步提高后于2000年1月13日开始将股价推升到一个新的境界15元附近。而5个月的平台整理使得大部分投资者如同在沸腾的开水里游泳的青蛙，逐渐适应了新的环境，无缘无故地认可了新的价位。    在15元附近再次整理，历时两个多月的平台后再次向上突破，创下19.5元的新高.....又稍经整理后最高创下21.75元的历史高点......    回顾该股，共经历了三个台阶，升幅达300%多，时间跨度长达1年零一个月。类似的走势还有很多，比如1999年至2000年度的600674川投控股，600679凤凰股份等等不胜枚举。    三、单边上扬式拉升：    单边上扬式拉升往往受时间的制约性比较强。比如大行情进入中后期，或离运做项目利好公布时间比较近，等等。    在拉升过程中，主力往往一气呵成，中间没有比较明显的大幅度洗盘动作。绝大多数采用依托均线边拉边洗式。主力拉升思路明确，股价走势轨迹明显。常常走出单边上扬的独立上升态势。主力开始大幅度洗盘之际，也标志着拉升行情结束之时。此拉升方法有三个显著的标志：    第一，在行情初期，股价常常走出极小幅度的阴阳交错，慢牛爬坡的缓慢走势，此阶段是主力建仓或增仓阶段；    第二，拉升阶段比建仓阶段有所加速，经常依托均线系统边拉边洗，拉升前期和中期，主力在早盘将股价推高之后，就任其自由换手，不过多去关照股价，使股价常常出现自由落体之势，在回落至下轨线处或者必要的塑造图形时，主力再度护盘，重新吸引多头买盘。走出极为规律的走势，以吸引多头资金积极买进股票，起到助庄的作用。可谓四两拨千斤！  第三，单边上扬式拉升，后期往往以快速拉升、疯狂刺激多头买盘人气而达到高潮，一般会采用各种转换角色的“示形”方式，诱导、促使场外的小散户投资者失去正常的投资心理、控制能力，产生一种过量、过度的放大和虚化的投资激情，这也是行情见顶的信号。这种拉升方法在大多情况下都是受客观条件制约的，主力常以中线操作为主。绝大多数主力控盘要占到流通盘的30%－40%，涨幅多以50%－100%不等，很少有超过100%以上的涨幅。    比如近期的0157中联重科。主力在该股上市之初为期三个月的平台上吸纳了大约占流通盘20%左右的筹码，然后借大盘跳水之际，将股价迅速从25元附近打低到20元区域，然后主力开始逐步增仓。这时主力持有流通盘约40%的筹码，开始运用单边上扬式的拉升方法，将拉升和洗盘的艺术完美地融为一体，于8月份最高涨到39.05元，涨幅达到85%。    回顾该股整个运作过程，从2001年3月份开始共运行5个月。这期间，大市环境比较恶劣，可谓险象环生。该上市公司中期推出10送10的高比例送配方案，对于利好公布的时机来说，时间较为紧迫，大势已岌岌可危。这一切都迫使主力时不我待，必须抓紧战机。我想这是主力采取单边上扬拉升方式的最主要原因吧。雷同的走势还有600313中农资源和600232金鹰股份.....等等。    四、火箭式拉升；    顾名思义，此类拉升方式犹如火箭发射，升势一旦启动，行情锐不可挡。此类飚升行情可谓惊天动地。但纵观此类个股，在行情飚升之前，往往要经过一年甚至更漫长的吸货过程。在这整个过程中，股价犹如一潭死水，波澜不惊，臭不可闻，然而，主力已经悄悄掌握了流通盘50%－60%的流通筹码。行情一旦启动，往往升势如虹，气冲霄汉。这种玩家，做起活络来，驾轻就熟，清清爽爽，就好比一个直爽的汉子的生活习性，吃饭就是吃饭（吸筹），干活就是干活（拉升），绝不会一边吃饭（吸筹）一边干活（拉升），可谓思路清晰，恩怨分明。    这种主力实力绝非一般，运做项目一般后市有强大的题材配合，准备工作异常充分。选择的运作项目多为中等偏下的流通盘子较多。除此之外，该类主力都具备他们认为强大的政治背景，或者说是手持尚方宝剑，所以才会有恃无恐，正所谓艺高人胆大。殊不知枪打出头鸟，这类主力在成为市场的大明星后，政策面稍有变化，监管措施一紧，管理层就会迫于舆论、媒体等压力，先拿他开刀，以起到杀鸡儆猴的作用。该类玩家玩起来常常得意忘形，直到把股价玩到令人难以置信的地步，真真正正的空中楼阁，最终导致玩火自焚。做为此类主力，须要把握一个“度”字，在股票的玩法上已经不成问题，剩余的时间该好好学学如何做人。    比如0008亿安科技、0047中科创业，0503海虹控股，包括最近的0557银广厦，600756齐鲁软件（原名泰山旅游）等。	股票
以上部分我们主要探讨了拉升的方式，下面我们着重分析一下拉升手段。 一、压单对敲： 主力经过试盘后，觉得盘面较轻，浮动筹码较少，可在卖一、卖二或者卖三上压上大笔卖单，然后，再分批快速买进。诱导多头主动性买盘出现，刺激市场人气，促进股价上涨。当股价轻松上扬到另一个台阶时，再次压单对敲买进。所不同的是，第二次压单数量应高于第一次压单数。当再次分批买进时，在分时线上就会呈现出价涨量增的良好局面。第三次也是如此，压单的数量应一次比一次更大，从而促进成交量一波比一波放大，股价也一浪比一浪升高。当日股价完成操作计划之时，主力可适时抛出一批筹码，打压住升势。当股价向下回落时可分批小单买进，控制股价走势，引导和掌握股价。当然在股价拉到当日操作既定目标时，也可在卖三或更上方挂出较大的卖单，压制股价，在买三或更低一点价位挂上较大的买单，利用夹板的方式控制股价，促进股价在该区域内充分换手。需要注意的是，在第一波压单对敲的过程中，如果发现没有跟风盘的话，主力应适时停止无谓的对敲，以免形成自拉自唱的局面。二、直接对敲： 直接对敲拉升和压单对敲拉升比较起来，更为隐蔽和安全。因为在压单对敲的过程中，跟风盘蜂拥而至，很容易造成筹码分流，丧失廉价筹码。而直接对敲拉升则回避了此类风险。主力在卖一、卖二或者卖三上挂单后，一般挂单量适中，主力迅速地一笔买走，股价也随之走高，成交量的放大和股价的涨升同步进行，这样就避免筹码被其他机构或投资者买走的可能性。如此反复，股价迅速推高。 压单对敲拉升往往是先放量后涨价，而且股价分时线向上的涨势较为缓慢。而直接对敲拉升是成交量的放大和股价的涨升同步进行，而在分时线上的涨势速度明显要比压单对敲的拉升速度快的多，这是两者之间的显著区别。 三、梳式对敲拉升： 这种拉升方式，往往是主力实力较弱时采用的一种。比如，手头资金紧张，或者可用来对敲的筹码较少，而大盘人气较好，个股浮码较小，在如此成熟的时机下，主力为完成操作计划而采取的拉升方法。 由于主力前期手中握有一定筹码，或者说筹码的锁定性比较好，主力也是采取直接对敲拉高式。可是由于主力手头资金紧张，如果频繁采用直接对敲拉高的方法，一旦股价做高，资金用完，弹尽粮绝时，就再也没有资金护盘了，可是拉升时机又很好，那该怎么办呢？主力只好利用有限的资金，每隔一分钟或几分钟进行一次直接对敲拉高，成交量也逐步放大，股价稳步盘升，所不同的是成交量看起来每笔间隔都留有空隙，好像日常生活中所用的梳子。经过这样运作，既拉高了股价，又不耽误良好的战机，还能护盘，第二天又可以运用前一天卖出的股票的资金进行周转，也算识时务的明智之举。 四、哄鸭子过河： 这种拉升，不包含任何对敲成分，只是采用挂单上的微妙关系，给关注该股的投资者施加思想上的压力，改变或进一步刺激其固有的思维定势，从而赢得主动，起到不战而屈的良好效果。 比如某股浮码稳定，大盘又人气高涨，可是该股缺乏主动性买盘，股价纹丝不动，关注该股的多头都在等待逢低买进的机会，而欲卖出的空头都在等待寻找高点抛出，这时主力只要在买一、买二或者买三上挂上虚张声势的大买单，并且频繁向上移动该买单，促使一直关注该股的多头按捺不住，恐怕等不到逢低买进的机会，只好主动向上买进上方的卖单，这样一来，股价不断上涨，散户多头奋勇买进，多头实力大增，而原来处于观察的空头逐渐心虚，渐渐产生惜售心理......主力只要在下档不断变换买单的价位和数量就可起到煽风点火的效果...... 五.推土机式拉升： 此拉升和哄鸭子过河有一定的相同之处，都是在下档挂上大买单。唯一的区别是哄鸭子过河主力仅是在下档挂上一个大买单，通过不断变幻买单的数量和位置，促进多头人气，促使股价上涨。而推土机式拉升则是在下档连挂三档大买单，而在上档则挂上均匀而较小的卖单，通过这种挂单上的虚张声势的做法，诱导散户和其他不明真相的投资者买进上方主力挂出的小批量卖单，股价在分时线保持一定角度的缓步上涨。这种拉法虽然也有一定的上涨，但是从严格的意义上来说，并不是拉升。而是主力出货的一种手段，主要出现在行情上涨的末期。 六.钓鱼竿式拉升方法： 此拉升方法和推土机式拉升方法在挂单上恰恰相反。钓鱼竿式拉升方法则是在卖一、卖二、和卖三上连挂三张较大的卖单，下档则不挂或者少挂买单。自己又通过不断的对敲买进上档之卖单的方法，促进股价上涨，引诱多头买进，由于下档买单极小，所以想卖出的空头又无从下手。从成交来看，外盘和内盘成交悬殊极大，外盘有时甚至大出内盘几倍甚至十几倍！但股价上涨的幅度却与此不成比例。这也是一种边拉边出的方式。虽然从一定意义上来说，钓鱼竿式拉升和推土机式拉升有着主力相同的出货意图，但比较起来，钓鱼竿式拉升比推土机式拉升主力显得更为心虚一点。但是两者又同时出现在行情的末期。钓鱼竿式拉升，可能出现的更晚一些……两者的区别，除了挂单方法不一样外，推土机式拉升包含的对敲成份要比钓鱼竿式拉升包含的对敲成份要少得多，或者是说也有可能不包含对敲成份。另外推土机式拉升，如果是出现在低位，也有可能是实力较弱的主力在虚张声势的造高股价，并不完全是出货，而出现在高位则另当别论……	股票
洗盘的艺术　　作为控盘主力，将股价拉升到一定的高度，逐步脱离自己的建仓成本区域后，下面就要进入第一个环节——洗盘。    为什么要洗盘呢？因为每一个控盘主力无论手段多么高明，其只能控制流通盘部分或大部分流通筹码，而市场上仍然保留着一定意义上的流通股份。而这一定意义上的流通股份的持有者，随着股价的逐步上涨，已经渐次获利。而这些获利的筹码就犹如没有被排除引信的炸弹，揣在主力怀中，时刻威胁着主力资金的安全，很大程度上制约和牵制着主力再次造高股价。这些小资金投资者由于资金较小，持有流通筹码的份额较少，和控盘主力的大资金持有者持有流通筹码的份额比较起来，有着船小好掉头的巨大优势。这样势必会造成主力在做高股价后在高位派发获利筹码的难度。    正是基于此，所以作为控盘主力，往往在股价有一定涨幅或是取得阶段性胜利后，或获利筹码涌动时，恰当地利用大势或者个股利空、传闻，强制股价，破坏原来的走势，进入箱体震荡或平台整理、或向下打压股价，通过股价走势上的不确定性，破坏小资金持有者对市场正确的感知能力。极力渲染和极度虚幻地放大在资金持有者的恐慌情绪，进一步虚妄地深化在资金持有者对后市错误的感知能力。利用这些散户投资者对后市股价走势的不确定性，促使获利的小资金持有者和散户投资者中的不坚定分子，交出筹码和看好后市的新多或新的增量资金入驻，充分换手，从而进一步提高和垫高除了主力以外投资者的投资成本，为日后再次做高股价，打下牢固的基础。依次类推，周而复始，经过几轮涨升与洗盘后，其他投资者的投资成本也越提越高，最终形成中小投资者和小资金持有者在高位自然而然地毫无意识地帮助主力锁仓，从而沦为主力出货时的掩护部队。  控盘主力就是在这种潜移默化的过程中，渐渐瓦解了小资金持有者和散户投资者船小好掉调头的优势。也通过层层递进，诱敌深入的方式，化解了自有资金过于庞大，形成的船大难调头的劣根性。主力就是采用这种循序渐进的方式，一步一步，小心翼翼地将小资金持有者和散户投资者引向了空头大军的埋伏圈......　　从表面上来看，洗盘是处于拉升和再拉升的过程阶段。但从实质上来说，洗盘是主力利用心理战来逐步提高除了主力以外的在二级市场上保持一定意义的流通份额持有者的投资成本。　　洗盘的手法从大的方面来说一般可分为四种：一. 震荡洗盘。二.打压洗盘。三.横向整理。四.边拉边洗。　　这四种洗盘应根据市场背景的不同和运做项目基本面的差异，以及各种客观条件的变化，加以选择和运用。下面我们详细介绍一下这四种不同的洗盘方法。  一.打压洗盘：这种洗盘方法，适用于流通盘较小的绩差类个股。由于购买小盘绩差类个股的散户投资者和小资金持有者，绝大多数是抱着投机的心理入市，所以这类个股的安定性就要差一些。这些散户投资者和小资金持有者常常一脚门里，一脚门外，时刻准备逃跑。而看好该股的新多头由于此类个股基本面较差，大多都不愿意追高买入，常常等待逢低吸纳的良机。鉴于持筹者不稳定的心态和新多头的意愿，做为控盘主力，往往利用散户对个股运做方向的不确定性，控盘打压股价，促进和激化股价快速下跌，充分营造市场环境背景转换所形成的空头氛围，强化散户投资者和小资金持有者的悲观情绪，促进其持有筹码的不稳定性，同时也激发持筹者在实际操作过程中的卖出冲动，无法抑制自己正常的投资心理，使这种悲观的情绪达到了白热化状态。主力通过控盘快速打压，采用心理诱导的战术，促进市场筹码快速转化，达到洗盘的目的。  打压洗盘方法的好处在于“快”和“狠”，采用时间较短，而洗盘的效果较好。  比如0616大连渤海在2000年2月22日和23日股价的走势（该股当时总股本7000多万，流通股本只有3000万左右，1999年度中报每股收益0.03元，可谓典型的绩差股），该股从11元起步，于2000年2月22日以15.9元开盘，最高涨到17元整，后由于大盘跳水（深圳当日最低下探170点），主力遂采用打压的方法洗盘，从17元当日最低下探至15.18元，单日震幅1.82元。2000年2月23日早盘开盘后，主力再次疯狂打压，从22日的收盘价15.61元一直打到跌停板价格14.05元。但当日该股虽然以跌停板价格收盘，可全天并没有跌停。股价一直象气垫船一样浮在跌停板的价位成交，无论抛单再大，股价始终不能跌停。经历2天打压洗盘后，从？？月24日开始股价开始不放量上涨，盘面浮码明显打算幅度减少，股价上涨越发显得更加轻灵......直到创下27元的新高。    二.横盘整理：此类洗盘方法适用于大盘绩优白马类个股。正是由于这种具备投资类个股大家都虎视耽耽地盯着的缘故，所以做为主力，绝对不能采用打压的形式洗盘。因为这类个股业绩优良，发展前景看好，散户投资者和小资金持有者的心态稳定。如果采用打压洗盘，散户投资者和小资金持有者不但不会抛售原有的筹码，反而还会采用逢低买进的方法摊平和降低持仓成本。而其他虎视耽耽的场外投资机构也会抢走打压筹码。这样很容易造成主力的打压筹码流失严重，形成肉包子打狗，有去无回的局面......    采用横向整理洗盘的主力实力较弱的，往往保持一定幅度的震荡，在震荡中不断以低吸高抛赚取差价以摊低成本和维持日常的开支。实力较强的主力，往往将股价震幅控制在很窄的范围内，使其走势极其沉闷。这种横向整理洗盘的方法，主要侧重于通过长期的牛皮沉闷走势来打击和消磨散户投资者和小资金持有者的投资热情和考验他们的信心毅力。  这种洗盘方法是所有洗盘方法里耗时最长的一种。一般的大盘绩优股的中级洗盘，往往要耗时3——6个月，有时甚至一年不等。在这漫长的等待中，面对大盘的跌荡起伏和其它个股的纷纷上窜，绝大多数的投资者都会按耐不住寂寞与孤单，纷纷换股操作，选择追涨杀跌的操作方法。等股价突破平台快速上扬时，他们往往会快速杀回，追涨买进，从而起到促进他们买高卖低，提高投资成本的目的。也有极小部分的散户投资者和小资金持有者经历了长期的煎熬享受到胜利的喜悦后更加坚定了持股的信心。从而为主力在经过多次拉升、横盘、洗盘如此反复的心理诱导下越发坚定持股信心，最终导致常坐电梯，为主力出货贡献微薄之力。    横盘整理的形态在K线上的表现常常是一条横线或者长期的平台，从成交量上来看，在平台整理的过程中成交量呈递减的状态。也就是说，在平台上没有或很少有成交量放出。成交清淡，成交价格也极度不活跃。为什么会出现这种情况呢？其内在的机理就是：当股价上升到敏感价位或浮码涌动亦或市场背景有所转换的时候，主力应适时抛出一部分筹码，打压住股价的升势，用一部分资金顶住获利抛盘，强制股价形成平台整理的格局，在这个阶段内，成交量稍显活跃，一旦平台整理格局形成，成交量应迅速地萎缩下来。主力一般应让散户投资者和小资金持有者所持筹码在平台内充分自由    换手，只是在大势不好股价下滑的情况下，适时控制股价上涨的冲动。此阶段时间内的成交量由于主力活动极少，成交量应该是清淡的。     成交量的迅速减少，也进一步说明了场内的浮动筹码经过充分换手后日趋稳定。随着新增资金的陆续入场，成交量也逐步呈放大状态，股价也开始缓缓上扬。此阶段的成交量和第一阶段强制股价进入平台时的成交量遥相呼应，形成漂亮的圆弧底形态，预示着股价即将突破平台，形成新一轮的升势。比如0429粤高速，该股主力从1999年5月18日启动，股价从6元附近拉升到9元区域后，主力强制股价进入平台整理区域，成交量也从96911手快速委琐到5471石油，随后股价也形成漫长的6个月平台整理走势。在平台区域内，成交量逐步萎缩，直到2000年1月份，成交量才逐步放大，于1月21日重新放出95973手成交量，与前期该股强制性进入平台区域时的成交量遥相呼应，形成漂亮的圆弧底形态，伴随着股价突破平台，展开新一轮的拉升行情。  三.震荡洗盘：由于利用打压洗盘容易丧失手中的廉价筹码，而采取横盘洗盘的方法要花费很长的时间做代价，而这种震荡洗盘则是把拉升、横盘、打压糅合贯穿到一起，象打太极拳一样把它们组合起来，取长补短。由于散户投资者和小资金持有者往往看到股价上涨的时候，追涨买入，这时候由于他们的心理准备不够充分，在他们的心目中买入的理由就是股价涨了，买进就能赚钱。他们买进后股价也许稍微上涨一点或者立即进入横盘或遭主力控盘打压，这时他们买入的理由随即消失，由于买在相对高点或者相对次高点，心理很容易失去平衡，股价稍有风吹草动，就会引起心理恐慌，尤其当主力控盘打压的时候，极容易产生割肉卖出的冲动。很多散户投资者和小资金持有者都是在这种心理压力下，经过主力的心理诱导战术，克制不住自己的恐慌情绪，在低位割肉出局。这时主力已经初步达到洗盘和预期目的，进而向上展开拉抬震荡。这时割肉出局的散户看到股票刚一卖出，股价就上涨了，心里懊悔不已，又产生新一轮的买入冲动......当股价再次行到前期高点或次高点附近时，上次在相对高点买入的套牢盘好不容易熬到了解套的机会，也极容易产生卖出解套的的冲动。如有意志坚强的多头不肯卖出，当再次遭遇主力控盘打压股价时他们往往会痛惜自己痛失解套的大好机会，而后来又反手买进的投资者更是迷惑自己，怎么卖出是错了，买进也不对，左一耳光，右一巴掌，两面都不是人......　　主力采用这种反复震荡洗盘的方法不断诱导散户投资者和小资金持有人追涨杀跌，踏高撵低，进一步促进和提高他们的投资成本。  震荡洗盘的好处在于和横盘整理洗盘比较起来节约了时间，和打压洗盘比较起来又回避了丧失廉价筹码的风险，可谓中庸之道。  震荡洗盘从表现的形态来看往往可分为以下四大类：  1.三角形形态：当股价上升到某一位置区域时，股价在主力的打压下或者获利回吐的压力下，开始震荡回调。在股价下调到一定幅度后，卖放的抛压逐步被买盘所消化，股价止跌回升，但是在股价回升到前期高点或者未到前期高点时，再次遇到主力的抛压或者获利回吐的压力，股价二次回探，但在第二次股价回调的时候，由于主力的护盘行为或者在新增资金的介入下，股价在达到或未曾达到前期低点的时候，股价第三次回升。这样股价高低点之间的波动幅度逐渐收敛，震荡区域也越来越小，促使买进和卖出的价位越来越近，使上档的卖压和下档的买力逐步逼近，在形态内进行低吸高抛的短线客，也逐渐无利可图。该形态至少有两个高点和两个低点组成，我们把该形态的两个高点互相连接后形成一条直线；把该形态的两个地点也互相连接，也形成一条直线；而这两条边线最终交会与一处，形成一个三角形的形态。    而根据这些三角形中的高点与高点，低点与低点所出现的价位不同，又可细分为：A.对称三角形，B.上升三角形。  A、对称三角形： 当股价上升到一定幅度或到达敏感价位区域亦或市场背景有所转换的时候，主力抛出一部分筹码或者市场上的获利盘兑现了一批筹码，造成股价下跌，当股价跌到一定位置或者卖方力量逐渐被买方力量消化或新增资金入驻后，股价止跌转升，但新增资金好象有点对前景有点犹豫，或者说主力拉升后感觉浮动筹码清洗的不够，需要彻底清洗获利筹码，在股价还没有上升到前期高点附近的时候，股价在主力的打压下，或者获利盘的抛压下，再次掉头向下；在股价还没有达到前期低点的时候，股价在主力的维护下或者逢低介入资金的推动下，再次勾头向上运行......这样股价的后来几次上涨和几次下跌均未达到前期的高点和低点。并且股价运行的高点一次比一次低，股价下跌而达到的低点却一次比一次高，高低点之间的波动幅度逐渐收敛，震荡区域越来越小，促使买进和卖出的价位越来越近......我们把股价的几个高点连接起来延伸为一条直线，同样地把股价的几个低点连接起来也延伸为一条直线，这两条直线最终会交会到一起，形成了对称三角形。  对称三角形所代表的意义是买卖双方在某一价值区域内力量暂时达到平衡状态的结果，即：获利盘和不看好后市的空头急于抛售手中的筹码，在股价尚未运行到前期高点附近时就抢先抛售；而新的逢低介入资金和看好后市的多头急于买进筹码，在股价还没有回落到前期低点的时候就先行买进。这样平衡的结果使股价高点逐步下移，而股价的低点也渐渐抬高。常常反映出买卖双方势均力敌。  要点提示：由于此形态为主力震荡洗盘形态，所以在该形态内的量价关系应该符合震荡洗盘的量价要点，包括价涨量增，价跌量缩；从整个形态来看，成交量应该随着股价震荡幅度的收敛，而逐步萎缩！从而预示着浮动筹码越来越少，场内筹码日趋安定！  B、上升三角形：股价上升到一定幅度或到达敏感价位区域亦或市场背景有所转换的时候，主力在某一水平区域打压住股价的升势，抛出一部分筹码或者市场上的获利盘兑现了一批筹码，造成股价下跌。当股价下跌告一段落时，多头开始买进或者主力开始护盘，股价逐渐回升，当回升到前期高点附近时，主力再次打压股价，股价再次回落，但由于多头买进士气正盛，股价尚未回到前期低点，即告弹升.....这种情形一直持续，令股价随着一条水平阻力线波动。而波动幅度逐渐收窄。我们把每次波动的高点连接，形成一条水平阻力线；而每次震荡的低点连接，形成一条向上倾斜的直线，两条直线最终交汇一处，就形成了一个上升三角形。  上升三角形代表的意义：是买卖双方在特定的价值区域内较量的结果，买方略占上风！而看淡后市的空头也并不急于出货，只在某一特定区域内减磅操作。也可能是主力故意在某一价值区域内刻意压制股价，促进筹码换手。随着股价震荡幅度的收窄使市场筹码持有者的投资成本逐渐升高，渐趋一致。  要点提示：在上升三角形形态内伴随着股价的震荡，和筹码的逐步换手，成交量也逐步递减，表示经历洗盘和换手后，盘面浮码日趋安定。在股价形成突破时往往伴随着较大的成交量，预示着新一轮的升势即将展开。  主力操盘手在实际运作的过程中，可根据市场背景的不同和人气状况选择相应的三角形形态。  需要注意的是，在主力运作的三角形整理形态即将结束的时候，而市场的背景依然没有转换，或运作的项目利好公布因故需要延迟的，亦或发现浮动筹码清洗并不充分的时候，主力应当重新修正三角形的整体形态。比如在第一次高点和第一次低点的波动范围之内，重新创造高点和低点，从而扩大和延长三角形的整理形态......在维护多头信心的同时，又延长了作战的时机。  2.箱型整理：亦叫长方形或矩形整理形态。就是股价上行到某个区域内出现多空完全平衡的状态。也就是说，当股价上行到某个价位附近时，即遭到主力的打压，强制股价回调；当下行到不远的另一个价位时，即遭到主力护盘或新多头吸纳。这样反反复复震荡把上档形成的高点互相连接形成一条水平阻力线，而把下档的低点也相互连接后形成一条水平支撑线，两条直线形成平行的通道，不上倾，也不下移，而是水平发展，形成长方形走势或箱体走势。市场筹码在箱体或长方形价值区域内震荡换手。这种洗盘方法适合于牛皮市、盘整市里的洗盘方式。  箱体整理代表的意义是：股价在股票箱内上下错落，由于散户投资者和小资金持有人在主力的心理战术诱导下失去了对市场正确的感知能力，见到股价上涨即追涨买入，买入后股价反而下跌，看到股价下跌即割肉出局，但买出后，股价却又拐头向上。这样不断追涨杀跌，垫高其他投资者的持仓成本，也从而促进信心不坚定分子出局观望，使筹码在股票箱内充分换手，同时也逐步培养铁杆追随分子......  3.旗型整理；顾名思义，旗型整理的图形就象一面挂在旗杆顶上的旗帜，由于其倾斜的方向不同，又可分为上升旗型和下降旗型。也被人们经常称作平行四边形。这种情况大多数是股价在上升到相当的幅度后，主力开始控盘打压股价，但股价下滑不多后主力开始护盘或者新多入驻，股价也开始上行。由于股价已经有一定的涨幅，往往出现跟风盘不太踊跃的现象，当上行高度高于或低于前期高点时，股价再度回落，如此反复，把股价的高点和高点连接以后形成向上或向下的一条直线，把低点和低点连接后也形成向上或向下的一条直线，两条直线保持平行，形成向上或向下倾斜的箱体。这种整理洗盘形态，如果出现在上升途中一般预示着涨升行情进入了中后期。如果出现在下跌途中，经常暗示下跌行情才刚刚开始。  4.空中加油形态：即在股价运行一定涨幅后，在中位主力利用手中的筹码控盘作图，一方面清洗获利筹码，使筹码充分换手；另一方面用以重新吸引多头买气和跟风盘！主力往往在震荡中划出漂亮的双底形态、头肩底形态、和三重底形态。  洗盘和出货最显著的标志是在洗盘的各种形态内，伴随着股价走低或横盘成交量逐步递减，表示经历洗盘后，盘面浮码日趋安定。而真实的量价关系，往往是检验主力洗盘和出货的试金石。而从成交量上的真假和伴随着股价的涨跌，往往更能真实地反映出主力的意图。  四.边拉边洗：这种洗盘方式最显著的标志是在日K线上没有标志，这也是区别与其它洗盘方法的一个显著特征。这种洗盘方法往往受客观条件制约，常常出现在单边上扬的行情中，主力把拉升和洗盘的艺术融为一体。这种洗盘方法就是主力每次都推高股价，然后就撒手不管，任凭散户自由换手，不管股价涨跌，次日或者隔天再次推高股价......主力只管寻找机会推升股价，散户只管自由换手......这是边拉边洗的一大景观。虽然在日K线上找不到主力明显洗盘的痕迹，但是主力采取的是化整为零，少吃多餐的策略，常常使散户在盘中换手、洗盘......这种主力洗盘时一般在股价拉升一定价位后，会在相对高位抛出一小部分筹码，在相对低位则无大抛单。如有大抛单，则在大抛单出来后股价立即转跌为升，或放量止跌。主力洗盘后的股价上升更加轻灵，只须少量买盘即可将股价推高。	股票
出货的方法　在股市上，影响股价涨跌波动的因素太多。比如：宏观上的、微观上的、行业内的、政治、经济、政策、甚至包括自然灾害以及企业产品的供需变化逐步都会反映到股价上来。具体到个股方面，诸如：技术面上的，消息面、基本面以及市场环境面等等存在很多的不确定的影响股价涨跌的因素。而市场参与者众多，成份极为复杂，也从另一个方面促进了股价涨跌的不确定因素，股价波动方向呈现出无序的状态。坐庄的含义就是：主流资金的持有者，通过种种手段，防范和化解种种不利因素，有效地利用和发展市场环境，从而改变和稳定股价涨跌的不稳定性。甚至积极地引导股价、控制股价、掌握股价，化被动为主动，改变股价涨跌的不确定性，使之能够按照自己的预期发展方向前进，引导和控制股价走势。从而极大地降低了主力投资者的投资风险。使之风险趋于最小化，利润最大化。    散户投资者和庄家最大的区别在于：庄家具备对股价未来走势不确定性因素的驾驭能力。或者说能够化解、引导、改变股价的不确定性。而散户投资者则不具备这些能力。但由于目前或者说是现阶段国内股市的实际情况使然，使众多的市场参与者，试图通过投资上市公司而取得分红收益的投资方式几乎成为不可能。绝大多数的市场参与者都是通过低买高卖的形式，在二级市场上博取差价从而获得利润、赚取利润。从这客观存在的事实来说，庄家和散户投资者又存在这一共同点。正是由于这一共同点的存在，才在有限的利益搏奕基础上，使庄家和散户投资者在争相巧取豪夺的同时，双方成为势不两立的对手。即使有双方友好协商的过程，那也是为了某一目的而使用的手段。    一个完整的利益搏奕手段应该是低买十高卖二收益。从这一客观规律来说，无论哪个主力，不管手段如何高明，股价涨幅有多大，如果不尊重这一客观规律，必将遭到坐庄失败。须知股价涨幅再大，如果不将手中的筹码兑现出局，也只是徒有帐面利润而已。而事实上，低买高卖的道理众所周知，每个市场的参与者又都不是傻子，谁会在高位奋勇买进呢？因此，怎样在高位调动多头买气，主力出货的时候，在供大于求的同时仍然维持较高的人气？这就好比一面要保持鼎内的开水沸腾不止，又要慢慢抽掉赖以加热的燃烧的柴禾一样，成为每一个主力面临的难题。    下面我们就探讨一下几种出货的方式。    一.震荡出货：这种出货方式从表面上来看和震荡洗盘别无二致。但从实际意义上来看，二者却有本质性的区别。首先我们对二者加以细致的区分：    震荡出货和震荡洗盘虽然都是采取以震荡为手段，但他们二者的目的却有天壤之别。震荡出货的目的是采用震荡的手段来掩饰主力派发的痕迹。一方面以派发手中的筹码为主要目的，另一方面在派发的同时还要维持较高的人气。这正是由于主力具备这种派发的目的，所以从盘面上来看，股价在采取向下震荡的时候，向下抛出的卖单具备连续性，并且卖单比较均衡，成交量比较真实，基本上都是真刀真枪的卖单。并且在向下震荡至箱体底部或较低价位时仍有较大的抛单抛售。这些较低位的抛单出来后，股价仍然盘软，股价借大盘走好或利好公布而采取向上震荡时，买单往往不具备连续性。或者持续性的买单很假，绝大部分为主力诱多时的对敲盘。当股价向上震荡到一定价位时，只要上档持筹投资者稍有抛售意愿，主力根本不愿正面交锋，股价遂掉头向下......总体表现来看，主力基本上扮演的是空头角色。多头多为对后市仍抱有幻想的中小投资者的行为。从这时的成交量来看，由于股价下跌时，成交卖单均衡而持续，显得比较有组织有计划，所以在盘面上和分时K线上就形成跌时放量的态势。而股价向上震荡时的多头力量基本上来自对后市仍抱有幻象的散户投资者。所以股价上涨时的买单就显得零碎和杂乱，缺乏集中性和计划性。在股价上涨时的成交量上则表现出涨时缩量的特征。这种跌时放量涨时缩量的不健康的量价关系，表明了主力急于出局的做空心理。    震荡洗盘虽然同样是采取了震荡的手段，但由于主力的主要目的是促进获利盘换手，同时由于主力对后市股价的走势很有信心，所以主力在向上震荡的时候（或者与主力有各种各样关系的其它资金）买单往往具备持续性、集中性和均衡性。反映在盘面上往往形成价涨量增的健康走势。而向上震荡到无压力区域的时候，一旦有持筹者有抛售意愿，主力也敢于和空头搏斗，从而显示出主力信心百倍。相反在股价采取向下震荡的时候，由于做空的能量大多是来自对股价后市走势具备不确定性疑虑的散户投资者，所以在股价采取向下震荡的时候，卖单显得零碎和杂乱，缺乏计划性、持续性和集中性，成交量表现在盘面上也是价跌量缩，表明投资者不愿在低位沽售。而股价向下震荡到箱底或者较低位时，一般就没有大的抛单抛售了，就是偶尔有较大的抛单沽售，股价也应该止跌回升。其机理就是，把不看好该股的大户清洗出局后把股价做高，让其没有逢低吸纳的机会，从而追高买入，踏错节拍，垫高其投资成本，真正起到洗盘的作用。还有一种情况就是放量止跌。这种情况一般是主力采取放量向下对敲卖出抛单很多很大，就是股价并不下跌，主要目的是恐吓意志不坚定分子，引诱信心不坚定者出局。此类情况根据盘面不同，也可能是其他小资金持有者获利出局另有机构进场换手。这两点看似细微却也 非常主要，是区别主力出货与洗盘的重要标志。正是由于以上几重情况的存在，所以震荡洗盘的成交量表现在K线和分时线上往往形成价涨量增、价跌量缩或者放量止跌的健康态势，这是主力震荡洗盘和震荡出货的重要标志。    再从整个形态的成交量来进行比较和分析。由于二者存在本质上的区别，震荡洗盘在整个形态演变的过程中成交量迅速萎缩，标志着经换手后盘面浮码迅速减少最终以向上突破而使震荡洗盘形态成为涨升过程中的中继形态。震荡出货则恰恰相反，由于主力派发的行为，导致整个形态演变的过程中浮码越来越重，最终选择向下突破而使这种震荡演变为头部形态。    二.横盘出货：这类出货方法仍然是最适合绩优大盘股。因为绩优大盘类个股往往给投资者形成一种安全、稳定的错觉。殊不知经过股价日积月累的上涨，高屹的股价早已包容和消化了个股基本面上的一切优势。再者，由于此类个股一直采用横盘洗盘的方法，曾经给予了坚定持股者不少甜头，这就更增加了他们坚定持股的信心。而在横盘洗盘的过程中，有的投资者由于缺乏耐心，而低卖高买的投资者则吸取了上次的教训，所谓吃一堑长一智，这次也信心十足地加入多头队伍。更有的在主力高位横盘出货的过程中进一步加仓操作，从而在潜移默化中，固守一种思维，无形之中帮助主力在高位进行锁仓，为主力暗渡陈仓立下汗马功劳。这种横盘，多以股价选择向下突破而演变为历史性的巨大头部而结束自己的历史使命。    相同的横盘却带来异样的结果，仔细辨别，横盘洗盘和横盘出货，虽然同样是以横盘为手段而达到不同的目的，但由于两者之间的本质区别，，导致主力在整个横盘形态过程中的不同行为。横盘洗盘主力主要是以换手洗盘为主要目的，只有在关键时刻才在高位或低位以主动性买卖单子控制股价，横向整理。有时间则以被动性买卖挂单强制控制股价横向洗盘，促使中小散户投资者自由换手。整个过程，主力活跃不多，走势沉闷，但比较坚挺。而成交量也伴随着股价换手，迅速萎缩，标志着筹码日趋集中，浮码逐步减少。横向洗盘最终以放量向上突破标志着中继形态的结束。    而横盘出货则恰恰相反，由于主力以派发为主要目的，导致在整个横盘形态演变的过程中主力身影频频活跃，常常做出各种突破姿态，引诱跟风盘，但随着主力不断派发，盘面浮码日趋沉重，股价走势也日趋疲软，每次股价跌至低点，主力维持股价时，显得特别沉重。这些都是前期主力筹码分散到散户手中后，出现安定性较差的局面。横盘出货表现在成交量上就是，在整个形态演变的过程中成交量能较活跃，始终不能萎缩。须知，横盘洗盘的平台并不需要太大的成交量来维持股价横盘的走势。在这么高的价位，也不会存在换庄的可能性，再加上盘面浮码日趋沉重，不是主力出货又是什么呢？    三.打压出货：打压出货通常比较适合于小盘绩差类个股。由于此类个股在炒作过程中的参与者，绝大多数都是抱着投机的心态。但是在股价快速上涨的过程中，由于人类与生俱来贪婪的通病，都奢望卖个更高的价钱，所以在涨升的过程中，极少有人出手。但由于这类个股基本面较差，如果采用高位横盘或震荡的手段出货的话，鉴于散户资金较小，比较灵活，往往由于主力资金较大，存在船大难调头的弊端，极易造成主力自拉自唱的局面。因此主力采用快人一步，趁散户投资者好梦未醒时，抢先抛售的策略，首先套住上档后进的跟风盘，再一路抛售，将敢于抢反弹者一网打尽。此手法讲究的是心狠手辣，才利用大盘或者个股人气极为火爆的时候，使用回马枪的手法，反手做空，往往令众多投，资者粹不及防......此种手法也叫跳水出货。    打压出货和打压洗盘，同样以使用“打压”为手段，其目的和意义却截然不同。打压出货以主力派发筹码为目的，以打压为手段。打压洗盘则是通过“打压”的手段清洗获利筹码，震出不坚定分子，从而促进筹码快速换手，以提高其他投资者成本为目的。因此，我们有必要对二者加以详细的区分：    打压洗盘：由于主力的目的是清洗获利筹码，促进筹码换手，震出不坚定分子，从而导致主力在整个形态演变过程中的行为。由于主力既想打低股价吓出获利筹码和市场中的不坚定分子，又不想丧失手中的廉价筹码，因而往往采用向下挂单对敲，也叫炸单或空中对敲的形式打低股价。从盘面走势上来看，股价跌势极为凌厉，鲜有反弹。5分钟K线上留下多个向下跳空缺口，成交量暴增。但仔细观察却发现，绝大部分成交量？？来自下对敲的成分。这是主力的诡计使然。打压洗盘从日K线上来看，往往是巨量长阴，形态极为恶劣。主要是吓唬那些不仔细观察盘面的技术派人士，造成一种放量出货的假象。这一招不仅蒙蔽了很多散户投资者，甚至一些号称大师级的股评人士身上也是屡试不爽。比如0425徐工科技，0557银广厦等。    打压出货则与此有所不同。主力利用的是跟风盘正旺盛的时候，趁投资者好梦未醒，而突然反手做空，先套牢后进买盘，接着将敢于抢反弹的人士一网打尽。从盘面上来看，虽然也是快速下跌，但盘中多有反弹，以吸引买盘跟进，同时稳定套牢者之持股信心。但股价总体走势呈逐波下探之势，重心快速下移，在日K线上往往形成长阴线。由于股价下跌的过程中卖出的成交量俱是真刀真枪，常常一张卖单，打低数个价位，而盘中向上做反弹时，却有对敲盘出现，其目的是引诱跟风盘。因此，打压出货未必有巨量成交放出，相反，由于抢反弹的人越来越少，成交量还会逐步缩小......  四.拉高出货：此方法往往不能够单独使用，经常和打压出货组合运用，效果甚佳。    1.被动出货法：运用此方法须选择大盘人气高涨、群情激昂，买气最盛时，主力利用个股利好或者传闻，在上档每相隔数个价位放上大笔卖单，主力趁人气鼎盛时，率先快速小批量买进，刺激多头人气买气，引诱跟风盘蚕食上档卖单，在股价快速上涨的过程中，不知不觉地将筹码转换到中小投资者手中。    2.涨停板出货法：同为拉高出货，但此方法与被动出货的区别是股价以涨停板的方式将拉高出货的行为演绎至高潮阶段。并带有主动抛盘的性质。主力将股价拉高后进入加速上扬阶段，并且上扬速度越来越快，出现飑升行情，使观望的跟风盘忍受不住股价快速上涨的诱惑，原来获利的跟风盘也由于利润的的快速增值而产生虚妄的放大的心理状态，而产生惜售的心理。主力往往抓住战机，以巨量的买单，将股价封至涨停，从而使多头买气达到高潮，此时后进的跟风买单纷至沓来。股价已牢牢地封住涨停，由于国内的交易规则采取的时间优先和价格优先的原则成交。那么在涨停价格的挂单是一致的，无法比出高低。而时间上却仍有先后之分。首先时间上处在前列的是主力的巨量买单，排在后面的是中小散户的跟风盘。这样，主力采用明修栈道，暗渡陈仓的方法悄悄撤出挂在前列买单，然后再将这些买单后继在跟风盘的后面。如此看来，涨停板上的巨量买单数量并无变化，甚至还有增多。主力可以以小批量的卖单，逐步将手中的筹码过渡给排列在第一时间段内的散户投资者。这种涨停出货的手段既能卖上一个好的价格，又不会引起一般投资者的警觉，可谓一箭双雕。    3.推土机式拉升出货：主力在卖一、卖二和卖三上不断地输出均匀而较小的卖单，在买一、买二和买三上挂出虚张声势的大买单，通过对敲引诱不明真相的散户投资者，买进上方自己的小批量卖单，当卖一上的卖单被散户投资者蚕食掉后，主力适时地再填上买单。这样周而复始，股价在盘面上表现为保持一定角度缓步上涨，从分时K线来看，红红的阳线一个连着一个，股价每时每刻都在上涨，犹如一串诱人的冰糖葫芦。可是这串诱人的冰糖葫芦只宜欣赏，不能品尝的哦！因为这种看似如行云流水般的拉升方法，实际上是主力一种被动的出货方法。    4.钓鱼杆式拉升出货： 此拉升方法和推土机式拉升方法在挂单上恰恰相反。钓鱼竿式拉升方法则是在卖一、卖二、和卖三上连挂三张较大的卖单，下档则不挂或者少挂买单。自己又通过不断的对敲买进上档之卖单的方法，促进股价上涨，引诱多头买进，由于下档买单极小，所以想卖出的空头又无从下手。从成交来看，外盘和内盘成交悬殊极大，外盘有时甚至大出内盘几倍甚至十几倍！但股价上涨的幅度却与此不成比例。这也是一种边拉边出的方式。虽然从一定意义上来说，钓鱼竿式拉升和推土机式拉升有着主力相同的出货意图，但比较起来，钓鱼竿式拉升比推土机式拉升主力显得更为心虚一点。但是两者又同时出现在行情的末期。钓鱼竿式拉升，可能出现的更晚一些……两者的区别，除了挂单方法不一样外，推土机式拉升包含的对敲成份要比钓鱼竿式拉升包含的对敲成份要少得多，或者是说也有可能不包含对敲成份。另外推土机式拉升，如果是出现在低位，也有可能是实力较弱的主力在虚张声势的造高股价，并不完全是出货，而出现在高位则另当别论……    须要指出的是，这几种拉升出货的方法虽然都能卖出较高的价位，效果也很好，但必须要把握好市场背景和市场人气。如果把握不好市场背景和人气的话，画虎不成反犬类，很容易弄巧成拙。    五.边拉边出：这种主力，一般心理压力颇大，他们的主要目的并不是在二级市场上博取太大的利润。可是由于目前股价距离主力成本太近，自己持仓又比较重，或者配股承销被套，或者增发承销，或上市承销被套，造成心理压力极大，多数主力无心恋战，在持筹极重的情况下，又想全身而退，才不得不采取一边做高股价吸引跟风盘一边出货的策略。这种主力被套的筹码其需要向上拉升的空间也要越大。并且伴随着很多媒体大张旗鼓地宣传。从盘面上观察，股价在上涨的过程中，一直存在做空的动能，但做多的动能要更胜一筹。股价在上涨的过程中，时常出现这种情况：在股价拉升途中，往往出现下跌时成交量抛单比较集中而且持续。从盘面成交量来看，下跌时成交量能相对逐步放大。随后突然出现买单，买单更加集中，也很持续，股价迅速走高，成交量能更大。总体给人的感觉好象是两个旗鼓相当，实力不分伯仲的多空主力在进行对抗赛。其实际操作机理是主力在跟风盘旺盛的时候抛出一批筹码，再趁上档抛压较轻的时候抓紧战机做高股价，以稳定长期投资者的持股信心，继续吸引后继跟风盘。周而复始，循环拉升，在股价拉升到剩余筹码足够的派发空间时，做多动能突然消失，荡然无存，股价进入横盘或下跌阶段，成交量也开始萎缩得很小，使很多投资者误以为主力仍在套中，不能出局，从而产生麻痹大意的心理。主力所余筹码此时所剩无几，慢慢震荡派发足亦。    比如2000年底，600186莲花味精股份有限公司实施了增发7000万股的增发方案，增发价格9047元左右，直逼当时的市场价格，造成投资者纷纷放弃认购增发新股，增发的7000万股中至少有近80%的筹码流入投资基金手中，主力被迫坐成庄。2001年3——6月份600186莲花味精主力只好采取边拉边出的战略实现了胜利大逃亡的局面。   六.边打边撤：这是一种出现在下降途中的出货方案。在这一过程中，散户投资者贪婪的心理，被控盘主力所充分了解。而后主力使用各种形态的心理诱导，促使中小散户投资者不能摆脱对后市发展趋势的盲目幻想，沉迷在对后市反弹企稳甚至反转的单相思般的恶性循环心理状态。当这种情况持续一段时间，股价缓慢下跌一定的幅度后，主力为了使已有股票的散户坚定信心，没有的人加入进来，庄家往往会转换多空角色采用示形的方式，施展心理诱导的战术，反手做多，在整个战略做空的基础上战术性做多，重新套牢一批后继的跟风盘。   2.主动攻击式边打边撤：从盘面上来看，主力在推高股价后，成交量迟迟不能快速萎缩，盘中常常出现频繁的向上对敲买单，且股价中心逐步下移。虽然每天股价跌幅并不深，但从长时间的日K线来看，股价运行在阴跌的趋势或通道里，且内盘很小，外盘极大，股价涨幅和内外盘成交不成比例，均证明主力在盘中通过对敲吸引跟风盘买进，从而达到自己出货的目的。这种股票开始跌幅较小，往往不引起散户投资者的重视，在主力出货末期，常常伴随着大幅跳水，等散户明白的时候，往往已深在套中，悔之晚亦！  3.被动式边打边撤：还有一种边打边撤的方法是，股价存在一定跌幅后，远离散户投资者已套牢的区域，主力在相对低位于早盘大幅低开后，在买一、买二和买三上挂上极大的虚张声势的买单，然后在卖一上不断输出较小的卖单，使抢反弹的散户能够从容不迫地在低位上买到看似廉价的筹码。但无论多少买盘，总也买不完卖一上看似不大的卖单。但由于下档极大的买单，同时又维护了其他看空投资者的信心，使其误以为有人在低位吸货。由于股价总跌不破买一、买二和买三的价位，给人以铁底的感觉，无疑进一步增加了持股者的信心，同时又引诱了新的跟风盘，只到尾市几分钟的时候，主力才快速做高股价，这时逢高想出的散户已经没有时间操作。已买进的散户，会为这建立在空中楼阁上的短暂利润而做上一夜的黄梁美梦。第二天早盘开盘，主力以更低的价格大幅低开，套牢前一天的跟风盘，然后故技重演。	股票
主力坐庄的新要求　　前面几章，我们主要讲了坐庄的几个基本步骤。但随着国家政策的改变，市场环境的变化，和机构自身实力的壮大，以及机构本身的性质和构造。已有的坐庄方案已经不能够满足主力坐庄的要求！由于现有的机构大多数是我国证券市场政策下的产物，在政策条件的种种制约和规范下，这些机构都带有浓郁的政策规范下的适应新的市场环境的特色。所谓的适者生存的道理吧。  比如：券商、投资基金、民营企业，私募基金等。这些机构，由于体制和机构内部组织结构的原因，必将影响到其操作的理念上来。一、 我们先从投资基金说起：  1、投资基金的投资市场比较单一，只有一个A股市场，没有跟A股市场相组合的其他市场。投资基金也只有股票和国债这两种投资产品，投资基金只能靠投资这两种产品取得1.5%的管理费用。  2、从投资基金的本质来考虑，其所持有的资产和赚取的利润所有权是投资者的，基金只有使用权和支配权，没有所有权。  3、从投资基金的组织结构上看，投资基金不象券商的组织架构一样，设有投资银行。  4、除此之外，基金还有严格的监管机制和信息披露制度：基金管理公司管理的基金从周报、旬报、月报、季报、半年报到年报全部都有。不仅是信息披露制度严格，在监管机制上还要按照证监会的要求，安排了一个很特别的职位，叫督察员。督察员的权力很大，直接对董事长、对证监会负责；可以列席公司的任何一种会议；可以进入到公司任何一个领域；他对于发现的异常情况、或者一些需要预警的情况、或者已经违规的情况需要即时报告。这些督察员还领导监察部，监察部是投资基金控制风险的一个机构，监察部对公司每一个月的监察报告，除了交报表，还要交监察报告，还有一些专题报告，监管机制非常严格。  以上几点，从根本上制约投资基金不可能象券商一样去投资ST类股票，也不可能去投资资产重组类股票。因为投资基金没有设投资银行，也就不可能去为ST类公司做方案，进行资产重组，如果ST类公司没有资产重组支持就只是一堆资产垃圾；由于这类股票价格的上涨的主要原因是因为收购和壳资源引起的，只具备投机价值；而投资基金又不具备进行资产重组的能力。除此之外，还有另外一个原因，就是投资基金所持有的资产和利润没有所有权，只有支配和使用权，投资资产重组类股票和ST类股票对于投资基金和券商来说，投资基金已经失去优势！况且资产的所有权不是基金自己的，即使侥幸赚钱，也无人喝彩；万一亏损，投资人肯定会有意见！第三个原因是，投资基金在严格的监管机制下；不可能将资产和利润移植给ST类公司进行重组，因为这些资产和利润都是投资人的，基金只有使用权没有支配权，这样基金就无法和ST类公司打交道。第四个原因就是投资基金作为市场上的新的投资主体，还要顾及到市场上的良好形象，靠自己良好的商业信誉，吸引新的投资人加盟！如果没有这些投资人的投资，投资基金将没有资产可管理。最终走向失败！  这一切，都将影响和决定投资基金在市场上投资品种的选择！进一步体现到其运做手法上来。除了ST类个股和资产重组板块以外，其余的投资品种都是投资基金挖掘对象。从近年来投资基金的投资组合来看，投资基金挖掘的投资品种多侧重于成长类股票。采用的也是以中长线为主，短线为辅的手法。除了盈利的目的外，还担负着引导市场创新投资理念，发掘新兴产业的价值，发动大规模的概念性炒做！比如1999年的5.19行情的科技股，和2000年初的网络股炒做。  另外投资基金在国家政策的精心呵护下，得到了证监会在新股配售时候的支持。券商和其他的机构都需要大量的资金去认购，投资基金作为战略配售人，大量的进入了新股发行的领域，能够得到很大的一块实惠。从而大幅度的赚取利润。如果抛出这一点，和不能够投资ST类和重组板块外，投资基金也不过是资金规模较大的投资者而已！  但是投资基金除了资金规模较大外，还有一项优势，就是这些投资基金很多都是国有股份制，大股东都是国有企业，这些大股东往往把股价做高以后，利用投资基金信息披露制度的广泛影响力，让投资基金在高位接手。蒙骗其他投资机构和中小投资者。这也上很多投资机构明明看着投资基金在某个价位介入，自己也赶紧跟进后，却遭受套牢的主要原因。一般来说，这些投资基金在帮助大股东出货后，会得到大股东一定的回报，比如，每卖出去一股，收去0.5到2元的好处费等等。这要看双方谈判的情况而定。  另外，最新推出的开放式基金由于体制上的改变，也会影响到主力的运做思路，最终改变投资理念。开放式基金，由于面临投资人赎回的压力，基金单位的进出，就要更侧重于一种流动性，可能会影响到基金的投资品种的选择上，比如：选股时考虑的因素除了基本业绩、成长性以及市场影响力或未来市场形象外。关于成长性的因素，一般只考虑３年以内的企业发展状况。还有就是个股的股性活跃程度。  二、关于券商：  券商和投资基金比较起来，机制上比较灵活。从券商设计的产品结构来看，也比较广泛，比如，上市推荐，新股承销，配股包销，增发新股等融资行为；这就大大的缩短了券商与上市公司之间的距离，为券商和上市公司亲密的接触创造了客观的条件。另外，由于券商还开展有代客理财的项目，和自营盘，以及管理着投资者的保证金；客观上为券商坐庄的资金打开了方便之门。除此之外，他们的组织结构上还设有投资银行，可以利用和上市公司比较亲密的业务关系通过投资银行去为上市公司做项目挖掘新的利润增长点，创造丰富的题材和符合市场热点的炒做概念。为ST类上市公司做方案；进行资产重组。由于券商的资产和利润是自己的，所以券商可以将资产和利润移植给ST类上市公司，进行资产重组。从而在二级市场上获得更高的收益！  1998年以前，中国的证券市场可以说是基本上由券商独步天下，从1998年以来，证券市场上的这种结构有了巨大的改变。因为证券市场上来了超级恐龙----投资基金。投资基金的规模使一向做为证券市场龙头老大的券商机构的地位发生了动摇。如果龙虎相争必有一伤，在这种新的市场形势下，券商和投资基金各采取了扬长避短的措施。从近几年的行情来看，投资基金主导的新兴产业价值发现，和投资理念的挖掘往往是行情的先驱者，而券商主导的资产重组行情往往起到过渡和掩护的作用。再加上民营企业和私募基金主导的行情，在证券市场上形成三足鼎立的格局。  由于券商体制上的原因，比如考虑到成交量创收、代客理财资金、挪用客户的保证金等等因素，所以反映到主力运做的手法上，此类主力往往是以放大量拉升股价为标志，中短线波段操作为手段，以求速战速决。其运做特点为平时不动，一动就不惜成本，连续涨升，一次到位，途中伴随较大的成交量。  三、三类企业、民营企业和私募基金：  随着证券市场政策的逐步宽松，和允许三类企业进入二级市场；大力培育机构投资者；以及证券公司股票质押贷款管理办法的出台等等一系列利好政策的公布。造就了股票市场资金面上空前繁荣的市场形象。  首先我们简单的剖析一下私募基金产生的市场背景：自1999年9月份以来，国家有关部门允许国有企业、国有资产控股企业和上市公司进入二级市场。并且银行部门同时开展了股票抵押融资业务，加上近几年期货市场极度低迷，大量的资金涌入股票市场。使得股市上资金面异常宽松。这些资金的来源异常广泛，逐渐打破了由投资基金和券商一统天下的格局。充斥在股票市场上。于是，这些国有企业，国有资产控股企业，上市公司，以及本来就不受国家政策制约的民营企业和一些由个人大户联合起来的投资群体纷纷注册和设立投资公司、投资顾问公司、投资咨询公司、投资管理公司、财务管理公司和财务顾问公司等等，以这些公司作为通往股票市场的桥梁。一些没有股票操作经验或者没有条件设立投资公司的上市公司也纷纷采取代客理财的方式借鸡下蛋。这些来自五湖四海的资金为了一个共同的目标，逐渐回笼起来，形成了一个庞大的群体。被市场人士称之为私募基金。这些私募基金处于半地下状态，不象投资基金和券商一样具备有效的监管体制和风险防范措施。即使规模较大的私募基金，也往往是根据保护自身的需要制订一些比较粗略相互监督体系和风险防范措施。缺乏防御系统风险的抵抗能力。  小规模的私募基金往往采取跟庄或者做短线庄的操作理念。很有点游击队的味道，常常是打赢便打，打不赢便跑。  规模较大的私募基金也有一套自己奉行的投资理念。他们经常根据自己的经济实力设计一套完整的项目方案，展现出自己独特的运做思路。完全屏弃公募投资基金和券商的传统的只在二级市场上投资的投资理念！  比如，他们选择一些符合自己标准的上市公司，利用自己的实力，从一级市场上收购国家股和法人股，直接参与到上市公司中来。成为上市公司的大股东。用自己的大股东的身份影响和控制上市公司，甚至收购上市公司。这需要资本的投入，也需要管理和技术上的投入。收购和控制上市公司后，不断的积极的利用各种关系寻找一些好的项目去提高上市公司的质量，甚至促使上市公司进行产业转型，改变上市公司的命运。  这是利用我国股本结构的先天缺陷以及特殊的杠杆作用控制和影响股价。由于在我国未流通的“国家股”和“法人股”，在上市公司的股份中占有相当大的比例，同时价格远远低于流通股，这使得控股股东，尤其是那些处于相对控股地位的控股股东，可以通过很低的价格受让国家股或法人股，实现对一家上市公司的控制。中国上市公司流通股(A股)占总股本的比例平均为30%。国外的上市公司就没有不流通的国家股和法人股，全部是在二级市场上流通的股份，所以国外的企业收购上市公司的时候耗费的投资成本要比国内的大的多。从这一点上来说，这些控股的股东们已经占了很大的实惠。  另外根据我们的详细统计，在对上市公司国家股和法人股的收购实例中，“每股净资产”是个非常重要的定价依据，转让价格的平均水平为每股净资产溢价30%(对于国家股，管理部门设定了下限，其转让价格，不能低于每股净资产)。在2000年国家股及法人股转让实例中，法人股的平均转让价格，为同期流通股平均价格的13%。由此来看，那些处于相对控股地位的控制性股东可以用很小的代价，通过法人股场外协议转让，实现对一家上市公司的控制。   这些控股股东，通过关联交易，在二级市场上埋下重兵，在低位收集相当多的流通筹码，在一级市场不断的利好配合下，逐渐做高股价。伴随着业绩增长，股价也成倍提升。给中小投资者以塌实的感觉。利用杠杆的原理，在一级市场上用较小的投入，在二级市场上取得丰厚的回报！  有的私募基金，参与一级市场只是简单的为了寻找一个炒做题材。一旦利好出尽，主力在二级市场上取得丰厚的利润后，就开始逐步派发。  但有的私募基金实力很大，也很有气魄！他们认为，与其不断的来回换股操作，不断的吸货，拉升，派发，增加运做成本，既费力，又不讨好！何况对于坐庄来说，难度最大的就是派发，因为在目前的情况下，中小投资者远不是过去年代里那种搏傻行为了。股价有一定的涨幅后，都时刻警惕主力出货，常常弄的主力很被动。在出完货后，帐面赢利和实际赢利悬殊都比较大。甚至遇到市道不好的时候还有可能出现微利或者亏损！不如做一个长线投资。于是，他们就把在二级市场上取得的丰厚利润中的一部分再通过项目注入到上市公司中，在保证业绩的同时，又支持二级市场股价。除此之外，还可以借助中国的资本市场的力量进行融资，在一级市场上做项目，做方案，改善上市公司的质量，扩大企业规模。随着上市公司质量进一步的改善和企业规模的扩大，股价就更上一层楼......股价越是上涨，中小投资者越是不敢跟进！越是不敢跟进，股价越是上涨！只到涨到令人不可思议的程度。但是随着上市公司基本面的不断改善，这种高成长性逐步被市场和投资者所认同！无论价格涨到多高，总有一些投资者会抱着不同的理念和思路或者说是处于某种动机买进，其中不乏一些大户、机构投资者和上市公司，甚至一些投资基金！由于主力在低位收集了绝大部分的流通筹码，使二级市场上只保留一定意义的流通股份，股价经历成年累月的上涨，涨幅已经巨大，在上涨的过程中，主力只要派发极少一部分筹码筹码就可以收回原来的投资成本！使手中的绝大部分筹码等于白赚，这样在派发的时候就要容易的多，无论什么价位卖出，都是净赚！——这就是一种全新的零成本作战方案！  比如：某股，流通盘3000万，市价15元，总共流通市值4.5亿。主力在此价位吸纳流通股的80%，需要资金4.5亿乘以80%计3.6亿。然后主力把股价做到30元（或者更高）经过10送10的送配方案后，股价除权后还是15元，但是流通盘增加到6000万，市值也增长1倍，即9个亿。然后由于上市公司质量的改善和企业规模的扩大，支持股价进一步走高，并填满权，股价再次达到30元区域。这时的流通盘不变，但流通市值已经达到6000万乘以30元既18亿，由于有良好的基本面支持，股价再次进行高送配，实行10送10的方案除权，股价再次回到每股15元。流通盘增至1.2亿，然后，公司增长性依旧，股价继续填权，重新站稳30元（或者更高）此时，流通市值已经达到1.2亿乘以30元合计36个亿。其中主力持仓量80%，市值36亿乘以80%合计28.8亿。也就是说，主力由当初投入的3.6亿资金经历过两次除权和填权后市值已经增加到28.8亿。在这个基础上，主力只要抛出手中流通筹码的13%即可收回当初投入的本金。（当然也可以把股价做的更高，反复除权的次数更多，赚的越多，收回成本就越发容易！）那么主力当初投入市场的投资成本仅占到拉升后流通市值的（3.6亿除以36亿等于10%）10%，10%是什么概念？也就是说，在主力收回投资成本的过程中，减少10%的流通筹码不要说中小投资者，就是一些专家和其他机构的看盘高手也未必看的出来。剩下白赚了（1.2亿流通筹码，乘以70%等于8400万股股票）8400万股股票。也就是说可以随便的兑现成现金，也可以算是白捡了一个上市公司的！何乐而不为呢？  当然，以上仅是举例，是从纯理论的角度去阐述一种坐庄方案，在实际的运做过程中，肯定要考虑一些其他的因素，比如对市场背景的运用，政策面的把握，和为了改善上市公司的质量而投入的资金成本等等。都是至关重要的。  以后一些实力强大的私募基金和三类企业、民营企业，都会借助资本市场的力量，不断的控股和收购上市公司来壮大和发展自己。评判坐庄的成功与失败的标准已经不在是单一的赚取现金利润的多少了。因为赚取的控股股份同样象征着主力的实力和雄厚的财产资源。	股票
天道　　我们以前谈到的都是机构主力操盘手关于股票操作的方法，这些方法都是用来于阶段时间内取得胜利的途径，这只是一种手段。好比习武之人经常使用的一招一式，当他们掌握这些招式之后并非真的就能够战无不胜！然而我们的股市也是如此。我们熟悉这些方法后，并不能够完全的支配天下大势，股市有其内在的运行规律——久涨必跌，久跌必涨。这就如一年四季分有春夏秋冬，人有生、老、病、死一般的客观自然规律；犹如春去夏至，秋归冬来，此非人力所能企及，须按照自然之规律，由盛衰分表里。我们的股市也是如此，必须按照规律使用不同的操作方法！很多主力遭遇滑铁卢就于此有关。好比一个武功高强的习武之人，当他的武功练到炉火纯青的地步，便会产生一种自我膨胀，幻想以一人之力就能回天有术，改变客观规律，而不知道掌握进退之道！须知“善泳者溺于水，善攀者坠于崖”的道理！    这种自我膨胀的结果，必将引火自焚！发生在2001年度的亿安科技事件、中科创业事件、银广厦事件，从这些个股的崩盘表面上来看，是由于资金链断裂或者是遭遇违规查处，或者是业绩造假，是一种偶然。然而在偶然的背后，这种事态发展的结果却存在着必然的因果关系！我们知道，股价的上涨与下跌，取决于供需双方的力量变化，而需方的买力则取决于投资者的信心！那么是什么决定投资者的信心呢？就是上市公司的内在价值。当一只个股，他的市场价值远远脱离实际价值的时候，这只股票在高位又能存在多久呢？我们承认在现实中，由于这样或者那样的原因，事物的发展往往存在着一些背离现象，但是这种背离现象是短暂的，是经不起考验的。就象自然界中，有时也会出现“倒春寒”的现象，就是说，有时候春季里的气温和冬季气温相似，甚至比冬季里还要冷，可那是短暂的，偶然的。这种短暂的背离是不会长期存在的！    那么，作为主力也是这样，不管你资金多么地雄厚，实力多么地强大，即使你把控制的个股所有的流通筹码全部买完，那么高屹的股价会支撑多久呢？是的，只要你不派发，股价自然会维持在高位，那么我们要说，做为主力是做什么的？难道你真的要以十倍，甚至数十倍的代价去购买一个昂贵的并没有价值的公司么？这和做为主力的初衷是一致的么？如果你是准备购买一个公司，然后进行实业投资，这种方法是不是有点得不偿失了呢？如果不是，你的初衷是在市场上套利，那么你不但违背了客观自然规律，而且也违背了自己的初衷！股票的价格和实际价值严重背离，投资者不会去跟风买进的，你拥有的也只是帐面上的虚增利润，不能够兑现，稍微一派发，股价就会如雪崩一样。0557银广厦十五个跌停板，0008亿安科技从120元跌到40元经过大幅下跌后又连续六个跌停板，一直到13元还喘息不定；0048中科创业从84元经过大幅下跌后仍然一连跌了九个跌停板一直到8元还魂飞魄散！这些都是活生生的例子！都是主力自我膨胀的结果！    审时度势，掌握进退之道，能曲能伸，方为职业主力操盘手的成功要决！好比习武之人，纵有盖世之武功，然一人难抵万夫之勇，充其量不过能成为一个平凡的将领罢了。要想成为天下名将，除了武功高强之外，非精通兵法不可！股市之职业操盘手也是如此，无论操作方法如何精妙了得，要想功成名就，取得非凡的事业也须懂得股票操作的学问。然而股票的操作学问和各国的政治、经济、军事等息息相关。股票的操作方法和股票的操作学问有着本质的区别。首先，股票的操作学是研究在某种政治、经济、军事等的环境下产生的因果关系，并因此而申时度势，先为自己创造不被别人战胜的条件，以等待别人可以被自己战胜的时机，使自己“立于不败之地”。怎样创造不被别人战胜的条件呢？这就要借助“时”和“势”。“时”是什么呢？“时”就是别人可以被我战胜的有利时机！“势”呢就是一种不可以阻挡的力量！犹如高山上滚下来的石头，也似奔腾咆哮的潮水！股票操作学研究的就是这种“时”和“势”！    股票操作法则是研究符合这种“时”与“势”的环境下如何操作的方法。股票操作学研究的是一种学问，股票的操作方法则是一种在这种“时”与“势”中求得赢利和取胜的手段！这好比带兵打仗，李自成虽然攻下了大明宫，但仍为一介草莽而已。他虽骁勇善战，但却并不懂得治国之道，终未形成气候！股市上也是如此，研究股票操作方法，乃职业操盘手在阶段时间内取得赢利和获取胜利的手段，股票操作学则是职业操盘手在研究操盘手法之前的基本精神！只有在股票操作学的前提下研究股票之操作方法，掌握进退之道——进可攻之，退可守之，顺应形势。这就是适者生存的客观自然规律！    我们虽然生活在股市的这个小的圈子里，但是和外界的一切变化仍然息息相关。比如国家。假如一个国家的政策朝令夕改，你将无所适从。再比如制度。如果制度混乱，贪污腐化，为政者昏庸无能，国将不国，这样皮之不存，毛将焉附呢？大到一个国家，分久必合，合久必分，由盛而及衰，由衰而及盛，小到股市，久涨必跌，跌久必涨。如果国家繁荣昌盛，股市就没有不涨的道理，假如国家陷入危机，股市焉能一枝独秀？这就是天道，是人力不可企及的天道！即使你掌握了股市很好的操作方法，取得了非凡的成就，如果不去研究股市操作的学问，不懂得掌握国际、国内经济形势、政策的变化，一道政令下来，你所有的财富就会化为乌有！这就是人们常说的政策市的威力——政策的宏观调控又是着眼于一个国家的政治、经济和发展方向的需要而不断变化的！    比如2001年六月股市的暴跌。管理层首先查处银行和上市公司进入股市的违轨资金，从后方切断市场做多主力的援军，然后开始上市新股、减持国有股，扩大市场需求。再就是上市公司增发、配股进行再融资，采用釜底抽薪的办法削弱市场上现存资金的做多能量，最后加大监管力度，对市场上存量资金进行监管。这就好比一场打仗时攻城掠地的计策——首先把要攻击的城市团团围住，接着派出兵力切断对方的粮草、援军和退路，然后一面加紧攻击步伐，一面指示间谍进入城内散布谣言，进行策反，削弱敌方的战斗力量，最后派出侦察飞机监视敌方的行动！在这种一边是海水，一边是火焰的情况下股票的操作方法根本就没有用武之地！    但这种情况不是没有办法可以防范的，防范的办法就是要学会股票操作学！首先我们来分析，中国要在2001年11月份加入世界贸易组织，其经济领域必须和世界经济接轨，当然也包括股市。然而中国的股市又存在着自己的特点----国有股和法人股不能够流通，股本结构存在着流通股份和非流通股份，造成了同股、同权不同价格的特殊现象。这是历史的原因，遗留问题。和世界股市接轨，就要废除这种不合理的现象。然而，要改变这种特殊的股本结构，大致有两种方法。一种方法是通过国有股减持让不能流通的国有股参与流通市场，这里面牵扯到个国有股减持的价格问题。另一种办法是通过国有股和其他资产进行转换来改善特殊的股本结构。在中国加入世界贸易组织这个大的前提下，政府出台国有股减持这项政策就不难理解了。其次，中国股市经过两年的上涨，已经积累了很多的获利筹码，明摆着久涨必跌的道理，由盛而及衰，由衰而及盛的客观规律。再就是在政府开始动手彻查银行和上市公司违规进入股市的资金的同时，就应该明白，原来的大力培养机构投资者的政策已经发生改变了！大量的上市新股，大量的增发和配股，都是国家为了抑制股市上涨而采取的扩大供给的手段！    从股票操作学上我们了解到上述情况以后，就不能够再采用股票操作方法来试图扭转乾坤了！股票操作方法是讲究的顺势而为，当然也可以逆小势而顺大势！好比一个国家，可以通过战争、武力而求得胜利，而战争和武力并不能够使国家和人民富强起来！这也就是人们常说的武力可以平天下。但谁又见过以武力治天下呢？    天有正道，如暑去寒来，春归夏至。人有生、老、病、死，这是自然律令。经济有周期，股市有涨有跌，有峰有谷，这也是客观规律。机构主力操盘手只有在掌握股市操作学的基础上，才能够做到高瞻远瞩，进退自如，游刃有余！正确使用股市操作法，在谷底建仓，峰顶出局，一定要明白以退为进和以进为退的道理。春秋战国时代的越国宰相范蠡在数年中帮助越王勾践卧薪尝胆，终败吴国以后，发现勾践乃只能共受苦，不能共享乐的昏庸君王，便隐瞒过去的身份，更名为“邸夷子皮”只身隐退于齐国海滨，从事农业发展，最终成为富甲一方的巨富。相反，越国的另一宰相文种，贪图眼前的荣华富贵，最终遭到越王勾践的杀害！股市上职业操盘手最忌讳逐鹿者不见山——明知道股市已至峰顶，而仍然犹豫不决。贪图眼前小利，不懂得掌握进退之道，最终导致全军覆没！    研究股票操作学，主要研究一个国家的政治和经济。如果政治腐败，则经济发展肯定难以繁荣，经济上的衰退继而会影响到企业和上市公司的质量。股票操作法虽然能短时间内支撑和改变股价的走势，但最终会走向价值回归之路。假如政治混乱，政策法令朝令夕改、频繁变化，则股市同样找不到发展的方向！这就是股市上的天道！是主力操盘手只能适应而不能逆转的天道！	股票
10、瓶子里放进2个球，水面升高4格。如果放进4个球，水面升高（ ）格；放进5个球，升高（ ）格。	奥数
人 鸡 猫 米过河鸡吃米猫鸡打架怎么过河	奥数
鱼缸里有7条金鱼 死了2条现在鱼缸里面有几条鱼？	奥数
一公斤棉花和一公斤铁哪个重	奥数
三角形剪去一个角 还剩几个角	奥数
哥哥今年10岁 弟弟今年六岁五年后哥哥比弟弟大几岁	奥数
小蜗牛从井底向上爬。白天往上爬2米。晚上往下滑落1米。井深5米，问小蜗牛几天才能爬出这口井	奥数
小玲把一根木头锯成5段，锯一次需要1分钟，她一共需要几分钟？	奥数
在8名排成一排的男同学队伍中，每2个男同学之间插进1名女同学。想一想，可以插进多少名女同学？	奥数
一块橡皮2元，妈妈给你9元，能买几块橡皮，还剩多少钱？	奥数
乐乐喜欢看课外书，她从第10页看到20页，她一共看了多少页的书？	奥数
萌萌代表市里参加国际象棋比赛，比赛前她和18位参赛选手每一位都握了一次手，那么一共有多少人参加国际象棋比赛？	奥数
天津的小林在上海有一个做医生的哥哥，而这个上海哥哥在天津却没有弟弟，这是为什么？	奥数
环形跑道上小红正在自行车比赛他发现在它前面有三辆车后面也有三辆车请问一共有多少辆自行车	奥数
翘翘板比较一步化小怪兽第二部换号第三部串糖葫芦	奥数
17个小朋友在玩捉迷藏的游戏已经坐不住了其中的八个人请问还藏着的有几个人	奥数
20个小朋友站成一对，从前往后数小红排在第九个，从后往前数小明排在第六个，小红和小明之间有几个小朋友	奥数
六个小朋友一起吃饭 每一个人一个饭碗 每两个人一个菜碗 每三个人一个汤碗，请问他们一共用了多少个碗	奥数
<blockquote>“等量代换”是解数学题时常用的一种思考方法，即两个相等的量，可以互相代换。当年曹冲称象时，就是运用了这种方法。因为只有当大象的重量与一船石头重量相等时，两次船下沉后被水面所淹没的深度才一样，所以大象的体重只要称出一船石头的重量就可以了。　　在有些问题中，存在着两个相等的量，我们可以根据已知条件与未知数量之间的关系，用一个未知数量代替另一个未知数量，从而找出解题的方法。这就是等量代换的基本方法。　　练习一：　　1、如果1个梨的重量等于2个苹果的重量，1个苹果的重量等于3个桃的重量。问一个梨的重量等于几个桃的重量？　　2、如果1个菠萝的重量等于6个苹果的重量，同时又等2根香蕉的重量。问一根香蕉的重量等于几个苹果的重量？　　3、如果1个足球相当于2个排球的重量，一个排球相当于20个乒乓球的重量，假设一个乒乓球重8克，那么一个足球重多少克？　　4、1只猴子等于2只兔子的重量，1只兔子的重量等于3只小鸡的重量。已知每只小鸡重200克。1只猴子重多少克？</blockquote>	奥数
<blockquote>练习二：　　1、1只兔子的重量＋1只猴子的重量＝8只鸡的重量　　3只兔子的重量＝9只鸡的重量　　1只猴子的重量＝（）只鸡的重量　　2、1只松鼠的重量＋1只兔子的重量＝5只鸭的重量　　2只松鼠的重量＝6只鸭的重量　　1只兔子的重量＝（）只鸭的重量　　3、用3个鹅蛋可换9个鸡蛋，2个鸡蛋可换4个鸽子蛋，用5个鹅蛋能换多少个鸽子蛋？　　4、20只桃子可换2只香瓜，9只香瓜可换3只西瓜，8只西瓜可换多少只桃子？　　5、2头小猪可换4只羊，3只羊可换6只兔子，3头猪可换几只兔子？</blockquote>	奥数
<img src="http://www.wp.rabbit-e.com/wp/wp-content/uploads/2017/04/1-228x300.png" alt="" class="alignnone size-medium wp-image-353" /><img src="http://www.wp.rabbit-e.com/wp/wp-content/uploads/2017/04/2-223x300.png" alt="" class="alignnone size-medium wp-image-354" /><img src="http://www.wp.rabbit-e.com/wp/wp-content/uploads/2017/04/3-218x300.png" alt="" class="alignnone size-medium wp-image-355" /><img src="http://www.wp.rabbit-e.com/wp/wp-content/uploads/2017/04/4-220x300.png" alt="" class="alignnone size-medium wp-image-356" /><img src="http://www.wp.rabbit-e.com/wp/wp-content/uploads/2017/04/5-225x300.png" alt="" class="alignnone size-medium wp-image-357" /><img src="http://www.wp.rabbit-e.com/wp/wp-content/uploads/2017/04/6-219x300.png" alt="" class="alignnone size-medium wp-image-358" />	奥数
ID3算法思想描述：（个人总结 仅供参考）a.对当前例子集合，计算属性的信息增益；b.选择信息增益最大的属性Ai(关于信息增益后面会有详细叙述)c.把在Ai处取值相同的例子归于同于子集，Ai取几个值就得几个子集d.对依次对每种取值情况下的子集,递归调用建树算法，即返回a，e.若子集只含有单个属性，则分支为叶子节点，判断其属性值并标上相应的符号，然后返回调用处。	建模
由于ID3算法在实际应用中存在一些问题，于是Quilan提出了C4.5算法，严格上说C4.5只能是ID3的一个改进算法。<p align="left">C4.5算法继承了ID3算法的优点，并在以下几方面对ID3算法进行了改进：</p><div><ul> <li>用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；有关信息增益率的定义可以参考栾丽华和吉根林的论文《决策树分类技术研究》1.2节。</li> <li> 在树构造过程中进行剪枝；</li> <li> 能够完成对连续属性的离散化处理；</li> <li> 能够对不完整数据进行处理。</li></ul></div><p align="left">C4.5算法有如下优点：产生的分类规则易于理解，准确率较高。其缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。</p><p align="left">2)主要步骤：</p>a. 读取文件信息，统计数目b. 建立决策树<ul> <li><ul> <li>如果样本集为空，则生成一个信息数目都为0的树节点返回</li> <li>如果样本均为同一类别，则生成一个叶子节点返回</li> <li>计算节点正负样本的数目</li> <li>如果属性值只有那个类别的属性，则生成一个叶子节点，并赋值类型索引</li> <li>如果以上都不是，则选择一个增益率最大的属性（连续属性要用增益率离散化），按那个属性的取值情况从新定义样本集和属性集，建造相关子树</li></ul></li></ul>c. 事后剪枝（采用悲观错误率估算）d. 输出决策树e. 移除决策时主要重点有：信息增益率的计算、事后剪枝使用悲观错误率衡量、树的建造（分治思想）	建模
3.另一种决策树算法C4.5这里仅作简单介绍1）概览：由于ID3算法在实际应用中存在一些问题，于是Quilan提出了C4.5算法，严格上说C4.5只能是ID3的一个改进算法。C4.5算法继承了ID3算法的优点，并在以下几方面对ID3算法进行了改进：用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；有关信息增益率的定义可以参考栾丽华和吉根林的论文《决策树分类技术研究》1.2节。 在树构造过程中进行剪枝； 能够完成对连续属性的离散化处理； 能够对不完整数据进行处理。C4.5算法有如下优点：产生的分类规则易于理解，准确率较高。其缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。2)主要步骤：a. 读取文件信息，统计数目b. 建立决策树如果样本集为空，则生成一个信息数目都为0的树节点返回如果样本均为同一类别，则生成一个叶子节点返回计算节点正负样本的数目如果属性值只有那个类别的属性，则生成一个叶子节点，并赋值类型索引如果以上都不是，则选择一个增益率最大的属性（连续属性要用增益率离散化），按那个属性的取值情况从新定义样本集和属性集，建造相关子树c. 事后剪枝（采用悲观错误率估算）d. 输出决策树e. 移除决策时主要重点有：信息增益率的计算、事后剪枝使用悲观错误率衡量、树的建造（分治思想）	建模
<title></title>K-近邻算法的思想如下：首先，计算新样本与训练样本之间的距离，找到距离最近的K个邻居；然后，根据这些邻居所属的类别来判定新样本的类别，如果它们都属于同一个类别，那么新样本也属于这个类；否则，对每个后选类别进行评分，按照某种规则确定新样本的类别。	建模
<title></title>搜索k个近邻的算法：kNN(A[n],k)输入：A[n]为N个训练样本在空间中的坐标，k为近邻数输出：x所属的类别取A[1]~A[k]作为x的初始近邻，计算与测试样本x间的欧式距离d（x,A[i]）,i=1,2,.....,k；按d（x，A[i]）升序排序，计算最远样本与x间的距离D<-----max{d(x,a[j]) | j=1,2,.....,k};  for(i=k+1;i<=n;i++)       计算a[i]与x间的距离d(x,A[i]);       if(d(x,A[i]))<D              then 用A[i]代替最远样本     按照d(x,A[i])升序排序，计算最远样本与x间的距离D<---max{d(x,A[j]) | j=1,...,i };计算前k个样本A[i]),i=1,2,...,k所属类别的概率，具有最大概率的类别即为样本x的类	建模
<title></title>谓LR分类器(Logistic Regression Classifier)，并没有什么神秘的。在分类的情形下，经过学习之后的LR分类器其实就是一组权值w0,w1,...,wm. 当测试样本集中的测试数据来到时，这一组权值按照与测试数据线性加和的方式，求出一个z值：z = w0+w1*x1+w2*x2+...+wm*xm。 ① （其中x1,x2,...,xm是某样本数据的各个特征，维度为m）之后按照sigmoid函数的形式求出：σ(z) = 1 / (1+exp(z)) 。②由于sigmoid函数的定义域是(-INF, +INF),而值域为(0, 1)。因此最基本的LR分类器适合于对两类目标进行分类。那么LR分类器的这一组权值w0,w1,...,wm是如何求得的呢？这就需要涉及到极大似然估计MLE和优化算法的概念了。我们将sigmoid函数看成样本数据的概率密度函数，每一个样本点，都可以通过上述的公式①和②计算出其概率密度	建模
<title></title>支持向量机(Support Vector Machine，SVM)是Corinna Cortes和Vapnik等于1995年首先提出的，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。在机器学习中，支持向量机（SVM，还支持矢量网络）是与相关的学习算法有关的监督学习模型，可以分析数据，识别模式，用于分类和回归分析。	建模
<title></title>// 1. 选择数据源// 数据源格式为UserID,MovieID,Ratings// 使用文件型数据接口DataModel model = new FileDataModel(new File("/Users/matrix/Documents/plan/test/ratings.txt"));// 2. 实现相似度算法// 使用PearsonCorrelationSimilarity实现UserSimilarity接口, 计算用户的相似度// 其中PearsonCorrelationSimilarity是基于皮尔逊相关系数计算相似度的实现类// 其它的还包括// EuclideanDistanceSimilarity：基于欧几里德距离计算相似度// TanimotoCoefficientSimilarity：基于 Tanimoto 系数计算相似度// UncerteredCosineSimilarity：计算 Cosine 相似度UserSimilarity similarity = new PearsonCorrelationSimilarity(model);// 可选项similarity.setPreferenceInferrer(new AveragingPreferenceInferrer(model));// 3. 选择邻居用户// 使用NearestNUserNeighborhood实现UserNeighborhood接口, 选择最相似的三个用户// 选择邻居用户可以基于'对每个用户取固定数量N个最近邻居'和'对每个用户基于一定的限制，取落在相似度限制以内的所有用户为邻居'// 其中NearestNUserNeighborhood即基于固定数量求最近邻居的实现类// 基于相似度限制的实现是ThresholdUserNeighborhoodUserNeighborhood neighborhood = new NearestNUserNeighborhood(3, similarity, model);// 4. 实现推荐引擎// 使用GenericUserBasedRecommender实现Recommender接口, 基于用户相似度进行推荐Recommender recommender = new GenericUserBasedRecommender(model, neighborhood, similarity);Recommender cachingRecommender = new CachingRecommender(recommender);List<RecommendedItem> recommendations = cachingRecommender.recommend(1234, 10);// 输出推荐结果for (RecommendedItem item : recommendations) {    System.out.println(item.getItemID() + "\t" + item.getValue());}	建模
在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。似然函数在统计推断中有重大作用，如在最大似然估计和费雪信息之中的应用等等。“似然性”与“或然性”或“概率”意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”和“或然性”或“概率”又有明确的区分。概率 用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而似然性 则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。	建模
如果你在开发一个音乐分类的应用，需要对k种类型的音乐进行识别，那么是选择使用 softmax 分类器呢，还是使用 logistic 回归算法建立 k 个独立的二元分类器呢？      这一选择取决于你的类别之间是否互斥，例如，如果你有四个类别的音乐，分别为：古典音乐、乡村音乐、摇滚乐和爵士乐，那么你可以假设每个训练样本只会被打上一个标签（即：一首歌只能属于这四种音乐类型的其中一种），此时你应该使用类别数 k = 4 的softmax回归。      如果你的四个类别如下：人声音乐、舞曲、影视原声、流行歌曲，那么这些类别之间并不是互斥的。例如：一首歌曲可以来源于影视原声，同时也包含人声 。这种情况下，使用4个二分类的 logistic 回归分类器更为合适。这样，对于每个新的音乐作品 ，我们的算法可以分别判断它是否属于各个类别。      现在我们来看一个计算视觉领域的例子，你的任务是将图像分到三个不同类别中。(i) 假设这三个类别分别是：室内场景、户外城区场景、户外荒野场景。你会使用sofmax回归还是 3个logistic 回归分类器呢？ (ii) 现在假设这三个类别分别是室内场景、黑白图片、包含人物的图片，你又会选择 softmax 回归还是多个 logistic 回归分类器呢？      在第一个例子中，三个类别是互斥的，因此更适于选择softmax回归分类器 。而在第二个例子中，建立三个独立的 logistic回归分类器更加合适。	建模
<strong>线性关系  </strong><code>两个变量之间存在一次方函数关系，就称它们之间存在线性关系。正比例关系是线性关系中的特例，反比例关系不是线性关系。更通俗一点讲，如果把这两个变量分别作为点的横坐标与纵坐标，其图象是平面上的一条直线，则这两个变量之间的关系就是线性关系。即如果可以用一个二元一次方程来表达两个变量之间关系的话，这两个变量之间的关系称为线性关系，因而，二元一次方程也称为线性方程。推而广之，含有n个变量的一次方程，也称为n元线性方程，不过这已经与直线没有什么关系了。</code>	建模
logistic和softmax的关系　　1）logistic具体针对的是二分类问题，而softmax解决的是多分类问题，因此从这个角度也可以理解logistic函数是softmax函数的一个特例。 　　这与logistic回归是完全一致的。　　2）从概率角度来看logistic和softmax函数的区别。　　softmax建模使用的分布是多项式分布，而logistic则基于伯努利分布，这方面具体的解释可以参考Andrew Ng的讲义去理解。　　3）softmax回归和多个logistic回归的关系。　　有了解的同学可能知道多个logistic回归通过叠加也同样可以实现多分类的效果，那么多个logistic回归和softmax一样不一样呢？　　softmax回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类；　　多个logistic回归进行多分类，输出的类别并不是互斥的，即"苹果"这个词语既属于"水果"类也属于"3C"类别。	建模
<blockquote>学习和预测现在我们已经获得一些数据，我们想要从中学习和预测一个新的数据。在scikit-learn中，我们通过创建一个估计器(estimator)从已经存在的数据学习，并且调用它的fit(X,Y)方法。In [14]: from sklearn import svmIn [15]: clf = svm.LinearSVC()In [16]: clf.fit(iris.data, iris.target) # learn from the data Out[16]: LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,     intercept_scaling=1, loss='l2', multi_class='ovr', penalty='l2',     tol=0.0001, verbose=0)一旦我们已经从数据学习，我们可以使用我们的模型来预测未观测数据最可能的结果。In [17]: clf.predict([[ 5.0,  3.6,  1.3,  0.25]])Out[17]: array([0], dtype=int32)注意：我们可以通过它以下划线结束的属性存取模型的参数：In [18]: clf.coef_  Out[18]: array([[ 0.18424352,  0.45122644, -0.8079467 , -0.45071302],       [ 0.05190619, -0.89423619,  0.40519245, -0.93781587],       [-0.85087844, -0.98667529,  1.38088883,  1.86538111]])</blockquote>	建模
<code>class LinearSVC(BaseEstimator, LinearClassifierMixin,                _LearntSelectorMixin, SparseCoefMixin):    """Linear Support Vector Classification.    Similar to SVC with parameter kernel='linear', but implemented in terms of    liblinear rather than libsvm, so it has more flexibility in the choice of    penalties and loss functions and should scale better to large numbers of    samples.    This class supports both dense and sparse input and the multiclass support    is handled according to a one-vs-the-rest scheme.    Read more in the :ref:`User Guide <svm_classification>`.    Parameters    ----------    C : float, optional (default=1.0)        Penalty parameter C of the error term.    loss : string, 'hinge' or 'squared_hinge' (default='squared_hinge')        Specifies the loss function. 'hinge' is the standard SVM loss        (used e.g. by the SVC class) while 'squared_hinge' is the        square of the hinge loss.    penalty : string, 'l1' or 'l2' (default='l2')        Specifies the norm used in the penalization. The 'l2'        penalty is the standard used in SVC. The 'l1' leads to ``coef_``        vectors that are sparse.    dual : bool, (default=True)        Select the algorithm to either solve the dual or primal        optimization problem. Prefer dual=False when n_samples > n_features.    tol : float, optional (default=1e-4)        Tolerance for stopping criteria.    multi_class: string, 'ovr' or 'crammer_singer' (default='ovr')        Determines the multi-class strategy if `y` contains more than        two classes.        ``"ovr"`` trains n_classes one-vs-rest classifiers, while ``"crammer_singer"``        optimizes a joint objective over all classes.        While `crammer_singer` is interesting from a theoretical perspective        as it is consistent, it is seldom used in practice as it rarely leads        to better accuracy and is more expensive to compute.        If ``"crammer_singer"`` is chosen, the options loss, penalty and dual will        be ignored.    fit_intercept : boolean, optional (default=True)        Whether to calculate the intercept for this model. If set        to false, no intercept will be used in calculations        (i.e. data is expected to be already centered).    intercept_scaling : float, optional (default=1)        When self.fit_intercept is True, instance vector x becomes        ``[x, self.intercept_scaling]``,        i.e. a "synthetic" feature with constant value equals to        intercept_scaling is appended to the instance vector.        The intercept becomes intercept_scaling * synthetic feature weight        Note! the synthetic feature weight is subject to l1/l2 regularization        as all other features.        To lessen the effect of regularization on synthetic feature weight        (and therefore on the intercept) intercept_scaling has to be increased.    class_weight : {dict, 'balanced'}, optional        Set the parameter C of class i to ``class_weight[i]*C`` for        SVC. If not given, all classes are supposed to have        weight one.        The "balanced" mode uses the values of y to automatically adjust        weights inversely proportional to class frequencies in the input data        as ``n_samples / (n_classes * np.bincount(y))``    verbose : int, (default=0)        Enable verbose output. Note that this setting takes advantage of a        per-process runtime setting in liblinear that, if enabled, may not work        properly in a multithreaded context.    random_state : int seed, RandomState instance, or None (default=None)        The seed of the pseudo random number generator to use when        shuffling the data.    max_iter : int, (default=1000)        The maximum number of iterations to be run.    Attributes    ----------    coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]        Weights assigned to the features (coefficients in the primal        problem). This is only available in the case of a linear kernel.        ``coef_`` is a readonly property derived from ``raw_coef_`` that        follows the internal memory layout of liblinear.    intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]        Constants in decision function.    Notes    -----    The underlying C implementation uses a random number generator to    select features when fitting the model. It is thus not uncommon    to have slightly different results for the same input data. If    that happens, try with a smaller ``tol`` parameter.    The underlying implementation, liblinear, uses a sparse internal    representation for the data that will incur a memory copy.    Predict output may not match that of standalone liblinear in certain    cases. See :ref:`differences from liblinear <liblinear_differences>`    in the narrative documentation.    References    ----------    `LIBLINEAR: A Library for Large Linear Classification    <http://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__    See also    --------    SVC        Implementation of Support Vector Machine classifier using libsvm:        the kernel can be non-linear but its SMO algorithm does not        scale to large number of samples as LinearSVC does.        Furthermore SVC multi-class mode is implemented using one        vs one scheme while LinearSVC uses one vs the rest. It is        possible to implement one vs the rest with SVC by using the        :class:`sklearn.multiclass.OneVsRestClassifier` wrapper.        Finally SVC can fit dense data without memory copy if the input        is C-contiguous. Sparse data will still incur memory copy though.    sklearn.linear_model.SGDClassifier        SGDClassifier can optimize the same cost function as LinearSVC        by adjusting the penalty and loss parameters. In addition it requires        less memory, allows incremental (online) learning, and implements        various loss functions and regularization regimes.    """</code>	建模
<blockquote>Iris Plants Database====================Notes-----Data Set Characteristics:    :Number of Instances: 150 (50 in each of three classes)    :Number of Attributes: 4 numeric, predictive attributes and the class    :Attribute Information:        - sepal length in cm        - sepal width in cm        - petal length in cm        - petal width in cm        - class:                - Iris-Setosa                - Iris-Versicolour                - Iris-Virginica    :Summary Statistics:    ============== ==== ==== ======= ===== ====================                    Min  Max   Mean    SD   Class Correlation    ============== ==== ==== ======= ===== ====================    sepal length:   4.3  7.9   5.84   0.83    0.7826    sepal width:    2.0  4.4   3.05   0.43   -0.4194    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)    ============== ==== ==== ======= ===== ====================    :Missing Attribute Values: None    :Class Distribution: 33.3% for each of 3 classes.    :Creator: R.A. Fisher    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)    :Date: July, 1988This is a copy of UCI ML iris datasets.http://archive.ics.uci.edu/ml/datasets/IrisThe famous Iris database, first used by Sir R.A FisherThis is perhaps the best known database to be found in thepattern recognition literature.  Fisher's paper is a classic in the field andis referenced frequently to this day.  (See Duda & Hart, for example.)  Thedata set contains 3 classes of 50 instances each, where each class refers to atype of iris plant.  One class is linearly separable from the other 2; thelatter are NOT linearly separable from each other.References----------   - Fisher,R.A. "The use of multiple measurements in taxonomic problems"     Annual Eugenics, 7, Part II, 179-188 (1936); also in "Contributions to     Mathematical Statistics" (John Wiley, NY, 1950).   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.   - Dasarathy, B.V. (1980) "Nosing Around the Neighborhood: A New System     Structure and Classification Rule for Recognition in Partially Exposed     Environments".  IEEE Transactions on Pattern Analysis and Machine     Intelligence, Vol. PAMI-2, No. 1, 67-71.   - Gates, G.W. (1972) "The Reduced Nearest Neighbor Rule".  IEEE Transactions     on Information Theory, May 1972, 431-433.   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al"s AUTOCLASS II     conceptual clustering system finds 3 classes in the data.   - Many, many more ...</blockquote>	建模
<code>from sklearn import datasetsfrom sklearn import svmimport numpy as npiris = datasets.load_iris()clf = svm.LinearSVC()clf.fit(iris.data, iris.target)X = [[ 5.0,  3.6,  1.3,  0.25],[ 2.0,  3.6,  1.3,  1.25]];print "\nX=\n", Xprint "\nclf.coef_=\n",clf.coef_print "\nnp.dot(X,clf.coef_.T)=\n",np.dot(X,clf.coef_.T)print "\nclf.intercept_=\n",clf.intercept_print "\nnp.dot(X,clf.coef_.T)+clf.intercept_=\n",np.dot(X,clf.coef_.T)+clf.intercept_print "\nclf.decision_function(X)=\n",clf.decision_function(X)print clf.predict(X)</code><blockquote>X=[[5.0, 3.6, 1.3, 0.25], [2.0, 3.6, 1.3, 1.25]]clf.coef_=[[ 0.1842342   0.4512267  -0.80794745 -0.45071037] [ 0.05217835 -0.89257794  0.40478014 -0.93829541] [-0.85075387 -0.98671947  1.38099741  1.86547753]]np.dot(X,clf.coef_.T)=[[ 1.38257785 -2.66074849 -5.54429344] [ 0.37916487 -3.75557897 -1.1265543 ]]clf.intercept_=[ 0.10956035  1.6669923  -1.70960085]np.dot(X,clf.coef_.T)+clf.intercept_=[[ 1.4921382  -0.99375619 -7.25389429] [ 0.48872522 -2.08858667 -2.83615515]]clf.decision_function(X)=[[ 1.4921382  -0.99375619 -7.25389429] [ 0.48872522 -2.08858667 -2.83615515]][0 0]</blockquote>	建模
<code>"""Examples illustrating the use of plt.subplots().This function creates a figure and a grid of subplots with a single call, whileproviding reasonable control over how the individual plots are created.  Forvery refined tuning of subplot creation, you can still use add_subplot()directly on a new figure."""import matplotlib.pyplot as pltimport numpy as np# Simple data to display in various formsx = np.linspace(0, 2 * np.pi, 400)y = np.sin(x ** 2)plt.close('all')# Just a figure and one subplotf, ax = plt.subplots()ax.plot(x, y)ax.set_title('Simple plot')# Two subplots, the axes array is 1-df, axarr = plt.subplots(2, sharex=True)axarr[0].plot(x, y)axarr[0].set_title('Sharing X axis')axarr[1].scatter(x, y)# Two subplots, unpack the axes array immediatelyf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)ax1.plot(x, y)ax1.set_title('Sharing Y axis')ax2.scatter(x, y)# Three subplots sharing both x/y axesf, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)ax1.plot(x, y)ax1.set_title('Sharing both axes')ax2.scatter(x, y)ax3.scatter(x, 2 * y ** 2 - 1, color='r')# Fine-tune figure; make subplots close to each other and hide x ticks for# all but bottom plot.f.subplots_adjust(hspace=0)plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)# row and column sharingf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row')ax1.plot(x, y)ax1.set_title('Sharing x per column, y per row')ax2.scatter(x, y)ax3.scatter(x, 2 * y ** 2 - 1, color='r')ax4.plot(x, 2 * y ** 2 - 1, color='r')# Four axes, returned as a 2-d arrayf, axarr = plt.subplots(2, 2)axarr[0, 0].plot(x, y)axarr[0, 0].set_title('Axis [0,0]')axarr[0, 1].scatter(x, y)axarr[0, 1].set_title('Axis [0,1]')axarr[1, 0].plot(x, y ** 2)axarr[1, 0].set_title('Axis [1,0]')axarr[1, 1].scatter(x, y ** 2)axarr[1, 1].set_title('Axis [1,1]')# Fine-tune figure; hide x ticks for top plots and y ticks for right plotsplt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)# Four polar axesf, axarr = plt.subplots(2, 2, subplot_kw=dict(projection='polar'))axarr[0, 0].plot(x, y)axarr[0, 0].set_title('Axis [0,0]')axarr[0, 1].scatter(x, y)axarr[0, 1].set_title('Axis [0,1]')axarr[1, 0].plot(x, y ** 2)axarr[1, 0].set_title('Axis [1,0]')axarr[1, 1].scatter(x, y ** 2)axarr[1, 1].set_title('Axis [1,1]')# Fine-tune figure; make subplots farther from each other.f.subplots_adjust(hspace=0.3)plt.show()</code><a href='http://matplotlib.org/examples/pylab_examples/subplots_demo.html'>link</a>	建模
<blockquote>SVM模型类型枚举enum { C_SVC, NU_SVC, ONE_CLASS, EPSILON_SVR, NU_SVR }; C_SVC：C表示惩罚因子，C越大表示对错误分类的惩罚越大 NU_SVC：和C_SVC相同。 ONE_CLASS:不需要类标号,用于支持向量的密度估计和聚类. EPSILON_SVR:-不敏感损失函数，对样本点来说，存在着一个不为目标函数提供任何损失值的区域，即-带。 NU_SVR：由于EPSILON_SVR需要事先确定参数，然而在某些情况下选择合适的参数却不是一件容易的事情。而NU_SVR能够自动计算参数。注意：C_SVC与NU_SVC其实采用的模型相同，但是它们的参数C的范围不同,C_SVC采用的是0到正无穷，NU_SVC是[0,1]。核函数类型枚举enum { LINEAR, POLY, RBF, SIGMOID, PRECOMPUTED };  LINEAR：线性核函数（linear kernel） POLY:多项式核函数（ploynomial kernel） RBF:径向机核函数(radical basis function) SIGMOID: 神经元的非线性作用函数核函数(Sigmoid tanh) PRECOMPUTED：用户自定义核函数 只有四个常用核函数，但我们必须决定哪一个是首选。然后是惩罚因子C和核参数的选择。在支持向量机中使用的核函数主要有四类：线性核函数： 多项式核函数： RBF核函数： Sigmoid核函数： 其中， 和 均为核参数。</blockquote>	建模
发现 spark 2.0 的特点，也可以用第七篇里总结的 dataframe 的特点来说明，那就是：<ul> <li><em>write less : 写更少的代码</em></li> <li><em>do more : 做更多的事情</em></li> <li><em>faster : 以更快的速度</em></li></ul>	大数据
<title></title>Mahout的K-means       mahout实现了标准K-Means Clustering，思想与前面相同，一共使用了2个map操作、1个combine操作和1个reduce操作，每次迭代都用1个map、1个combine和一个reduce操作得到并保存全局Cluster集合，迭代结束后，用一个map进行聚类操作。1.数据结构模型      Mahout聚类算法将对象以Vector的方式表示，它同时支持dense vector和sparse vector，一共有三种表示方式（它们拥有共同的基类AbstractVector，里面实现了有关Vector的很多操作）：      (1)、DenseVector      它实现的时候用一个double数组表示Vector（private double[] values）， 对于dense data可以使用它；      (2)、RandomAccessSparseVector     它用来表示一个可以随机访问的sparse vector，只存储非零元素，数据的存储采用hash映射：OpenIntDoubleHashMap;      关于OpenIntDoubleHashMap，其key为int类型，value为double类型，解决冲突的方法是double hashing，      (3)、SequentialAccessSparseVector      它用来表示一个顺序访问的sparse vector，同样只存储非零元素，数据的存储采用顺序映射：OrderedIntDoubleMapping;      关于OrderedIntDoubleMapping，其key为int类型，value为double类型，存储的方式让我想起了Libsvm数据表示的形式：非零元素索引:非零元素的值，这里用一个int数组存储indices，用double数组存储非零元素，要想读写某个元素，需要在indices中查找offset，由于indices应该是有序的，所以查找操作用的是二分法。	大数据
<title></title>Mesos与YARN比较Mesos与YARN主要在以下几方面有明显不同：（1）框架担任的角色在Mesos中，各种计算框架是完全融入Mesos中的，也就是说，如果你想在Mesos中添加一个新的计算框架，首先需要在Mesos中部署一套该框架；而在YARN中，各种框架作为client端的library使用，仅仅是你编写的程序的一个库，不需要事先部署一套该框架。从这点上说，YARN运行和使用起来更加方便。（2）调度机制Mesos采用了双层调度策略，第一层是Mesos master将空闲资源分配给某个框架，而第二层是计算框架自带的调度器对分配到的空闲资源进行分配，也就是说，Mesos将大部分调度任务授权给了计算框架；而YARN是一个单层调度架构，各种框架的任务一视同仁，全由Resource Manager进行统一调度。总结来说，Mesos master首先完成粗粒度的资源分配，即：将资源分配给框架，然后由框架进行细粒度的资源分配；而Resource manager直接进行细粒度的分配，即:直接将资源分配给某个任务（Task）。其他各个特性对比如下表：5. Mesos与YARN发展情况个人认为Mesos和YARN均不成熟，很多承诺的功能还未实现或者实现得不全，但总体看，它们发展很快，尤其是YARN，在去年年末推出Hadoop-0.23.0后，近期又推出Hadoop-0.23.1。随着各种计算框架（如Spark，S4，Storm等）的日趋成熟，一个统一的资源管理和调度平台将不可或缺。另一个与Mesos和YARN类似的系统是Facebook开源的Hadoop Coroca，具体可参考：“Hadoop Corona介绍”。	大数据
<title></title>Taste简介Taste 是 Apache Mahout提供的一个协同过滤算法的高效实现，它是一个基于 Java 实现的可扩展的，高效的推荐引擎。Taste 既实现了最基本的基于用户的和基于内容的推荐算法，同时也提供了扩展接口，使用户可以方便的定义和实现自己的推荐算法。同时，Taste 不仅仅只适用于 Java 应用程序，它可以作为内部服务器的一个组件以 HTTP 和 Web Service 的形式向外界提供推荐的逻辑。Taste 的设计使它能满足企业对推荐引擎在性能、灵活性和可扩展性等方面的要求。Taste原理系统架构image0接口设计DataModelDataModel是用户喜好信息的抽象接口，它的具体实现可能来自任意类型的数据源以抽取用户喜好信息。Taste提供了MySQLDataModel，方便用户通过JDBC和MySQL访问数据, 此外还通过FileDataModel提供了对文件数据源的支持。UserSimilarity 和 ItemSimilarityUserSimilarity用于定义两个用户间的相似度，它是基于协同过滤的推荐引擎的核心部分，可以用来计算用户的“邻居”，这里我们将与当前用户口味相似的用户称为他的邻居。ItemSimilarity 类似的，定义内容之间的相似度。UserNeighborhood用于基于用户相似度的推荐方法中，推荐的内容是基于找到与当前用户喜好相似的“邻居用户”的方式产生的。UserNeighborhood 定义了确定邻居用户的方法，具体实现一般是基于 UserSimilarity 计算得到的。RecommenderRecommender 是推荐引擎的抽象接口，Taste中的核心组件。程序中，为它提供一个DataModel，它可以计算出对不同用户的推荐内容。实际应用中，主要使用它的实现类 GenericUserBasedRecommender 或者 GenericItemBasedRecommender，分别实现基于用户相似度的推荐引擎或者基于内容的推荐引擎。	大数据
<title></title>The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.The project includes these modules:Hadoop Common: The common utilities that support the other Hadoop modules.Hadoop Distributed File System (HDFS™): A distributed file system that provides high-throughput access to application data.Hadoop YARN: A framework for job scheduling and cluster resource management.Hadoop MapReduce: A YARN-based system for parallel processing of large data sets.Other Hadoop-related projects at Apache include:Ambari™: A web-based tool for provisioning, managing, and monitoring Apache Hadoop clusters which includes support for Hadoop HDFS, Hadoop MapReduce, Hive, HCatalog, HBase, ZooKeeper, Oozie, Pig and Sqoop. Ambari also provides a dashboard for viewing cluster health such as heatmaps and ability to view MapReduce, Pig and Hive applications visually alongwith features to diagnose their performance characteristics in a user-friendly manner.Avro™: A data serialization system.Cassandra™: A scalable multi-master database with no single points of failure.Chukwa™: A data collection system for managing large distributed systems.HBase™: A scalable, distributed database that supports structured data storage for large tables.Hive™: A data warehouse infrastructure that provides data summarization and ad hoc querying.Mahout™: A Scalable machine learning and data mining library.Pig™: A high-level data-flow language and execution framework for parallel computation.Spark™: A fast and general compute engine for Hadoop data. Spark provides a simple and expressive programming model that supports a wide range of applications, including ETL, machine learning, stream processing, and graph computation.Tez™: A generalized data-flow programming framework, built on Hadoop YARN, which provides a powerful and flexible engine to execute an arbitrary DAG of tasks to process data for both batch and interactive use-cases. Tez is being adopted by Hive™, Pig™ and other frameworks in the Hadoop ecosystem, and also by other commercial software (e.g. ETL tools), to replace Hadoop™ MapReduce as the underlying execution engine.ZooKeeper™: A high-performance coordination service for distributed applications.	大数据
<title></title>chukwa 是一个开源的用于监控大型分布式系统的数据收集系统。这是构建在 hadoop 的 hdfs 和 map/reduce 框架之上的，继承了 hadoop 的可伸缩性和鲁棒性。Chukwa 还包含了一个强大和灵活的工具集，可用于展示、监控和分析已收集的数据。在一些网站上，甚至声称 chukwa 是一个“日志处理/分析的full stack solution”。说了这么多，你心动了吗？我们先来看看 chukwa 是什么样子的： chukwa 不是什么1. chukwa 不是一个单机系统. 在单个节点部署一个 chukwa 系统,基本没有什么用处. chukwa 是一个构建在 hadoop 基础上的分布式日志处理系统.换言之,在搭建 chukwa 环境之前,你需要先构建一个 hadoop 环境,然后在 hadoop 的基础上构建 chukwa 环境,这个关系也可以从稍后的 chukwa 架构图上看出来.这也是因为 chukwa 的假设是要处理的数据量是在 T 级别的.2. chukwa 不是一个实时错误监控系统.在解决这个问题方面, ganglia,nagios 等等系统已经做得很好了,这些系统对数据的敏感性都可以达到秒级. chukwa 分析的是数据是分钟级别的,它认为像集群的整体 cpu 使用率这样的数据,延迟几分钟拿到,不是什么问题.3. chukwa 不是一个封闭的系统.虽然 chukwa 自带了许多针对 hadoop 集群的分析项,但是这并不是说它只能监控和分析 hadoop.chukwa 提供了一个对大数据量日志类数据采集、存储、分析和展示的全套解决方案和框架,在这类数据生命周期的各个阶段, chukwa 都提供了近乎完美的解决方案,这一点也可以从它的架构中看出来.chukwa 是什么上一节说了很多 chukwa 不是什么,下面来看下 chukwa 具体是干什么的一个系统呢?具体而言, chukwa 致力于以下几个方面的工作:1. 总体而言, chukwa 可以用于监控大规模(2000+ 以上的节点, 每天产生数据量在T级别) hadoop 集群的整体运行情况并对它们的日志进行分析2. 对于集群的用户而言: chukwa 展示他们的作业已经运行了多久,占用了多少资源,还有多少资源可用,一个作业是为什么失败了,一个读写操作在哪个节点出了问题.3. 对于集群的运维工程师而言: chukwa 展示了集群中的硬件错误,集群的性能变化,集群的资源瓶颈在哪里.4. 对于集群的管理者而言: chukwa 展示了集群的资源消耗情况,集群的整体作业执行情况,可以用以辅助预算和集群资源协调.5. 对于集群的开发者而言: chukwa 展示了集群中主要的性能瓶颈,经常出现的错误,从而可以着力重点解决重要问题.	大数据
<title></title>hadoop@ubuntu:~$ pig -x local--加载数据(注意“=”左右两边要空格)grunt>> A = load '/home/hadoop/ziliao/student.txt' using PigStorage(':') as (sno:chararray, sname:chararray, ssex:chararray, sage:int, sdept:chararray);--从A中选出Student相应的字段(注意“=”左右两边要空格)grunt>> B = foreach A generate sname, sage;--将B中的内容输出到屏幕上grunt>> dump B;--将B的内容输出到本地文件中grunt>> store B into '/home/hadoop/ziliao/result.txt';--查看本地文件内容,没有''grunt>> cat /home/hadoop/ziliao/result.txt;b、脚本文件脚本文件实质上是pig命令的批处理文件。我们给出的script.pig文件包含以下内容：A = load '/home/hadoop/ziliao/student.txt' using PigStorage(':') as (sno:chararray, sname:chararray, ssex:chararray, sage:int, sdept:chararray);B = foreach A generate sname, sage;dump B;store B into '/home/hadoop/ziliao/result.txt';然后通过执行pig -x local script.pig即可。想看执行结果，可执行如下命令查看：	大数据
给出了C++和Shell编写的Wordcount实例，供大家参考。1. C++版WordCount（1）Mapper实现（mapper.cpp）#include <iostream>#include <string>using namespace std;int main() {string key;while(cin &gt;&gt; key) {cout &lt;&lt; key &lt;&lt; "\t" &lt;&lt; "1" &lt;&lt; endl;// Define counter named counter_no in group counter_groupcerr &lt;&lt; "reporter:counter:counter_group,counter_no,1\n";// dispaly statuscerr &lt;&lt; "reporter:status:processing......\n";// Print logs for testingcerr &lt;&lt; "This is log, will be printed in stdout file\n";}return 0;}（2）Reducer实现（reducer.cpp）12345678910111213141516171819202122#include <iostream>#include <string></string></iostream></string></iostream>using namespace std;int main() { //reducer将会被封装成一个独立进程，因而需要有main函数string cur_key, last_key, value;cin &gt;&gt; cur_key &gt;&gt; value;last_key = cur_key;int n = 1;while(cin &gt;&gt; cur_key) { //读取map task输出结果cin &gt;&gt; value;if(last_key != cur_key) { //识别下一个keycout &lt;&lt; last_key &lt;&lt; "\t" &lt;&lt; n &lt;&lt; endl;last_key = cur_key;n = 1;} else { //获取key相同的所有value数目n++; //key值相同的，累计value值}}cout &lt;&lt; last_key &lt;&lt; "\t" &lt;&lt; n &lt;&lt; endl;return 0;}（3）编译运行编译以上两个程序：g++ -o mapper mapper.cppg++ -o reducer reducer.cpp测试一下：echo “dong xicheng is here now, talk to dong xicheng now” | ./mapper | sort | ./reducer	大数据
<title></title>MapReduce重要配置参数1.  资源相关参数(1) mapreduce.map.memory.mb: 一个Map Task可使用的资源上限（单位:MB），默认为1024。如果Map Task实际使用的资源量超过该值，则会被强制杀死。(2) mapreduce.reduce.memory.mb: 一个Reduce Task可使用的资源上限（单位:MB），默认为1024。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。(3) mapreduce.map.java.opts: Map Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g.“-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc” （@taskid@会被Hadoop框架自动换为相应的taskid）, 默认值: “”(4) mapreduce.reduce.java.opts: Reduce Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g.“-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc”, 默认值: “”(5) mapreduce.map.cpu.vcores: 每个Map task可使用的最多cpu core数目, 默认值: 1(6) mapreduce.map.cpu.vcores: 每个Reduce task可使用的最多cpu core数目, 默认值: 12.  容错相关参数(1) mapreduce.map.maxattempts: 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。(2) mapreduce.reduce.maxattempts: 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。(3) mapreduce.map.failures.maxpercent: 当失败的Map Task失败比例超过该值为，整个作业则失败，默认值为0. 如果你的应用程序允许丢弃部分输入数据，则该该值设为一个大于0的值，比如5，表示如果有低于5%的Map Task失败（如果一个Map Task重试次数超过mapreduce.map.maxattempts，则认为这个Map Task失败，其对应的输入数据将不会产生任何结果），整个作业扔认为成功。(4) mapreduce.reduce.failures.maxpercent: 当失败的Reduce Task失败比例超过该值为，整个作业则失败，默认值为0.(5) mapreduce.task.timeout: Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该task处于block状态，可能是卡住了，也许永远会卡主，为了防止因为用户程序永远block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是300000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。3.  本地运行mapreduce 作业设置以下几个参数:mapreduce.framework.name=localmapreduce.jobtracker.address=localfs.defaultFS=local4.  效率和稳定性相关参数(1) mapreduce.map.speculative: 是否为Map Task打开推测执行机制，默认为false(2) mapreduce.reduce.speculative: 是否为Reduce Task打开推测执行机制，默认为false(3) mapreduce.job.user.classpath.first & mapreduce.task.classpath.user.precedence：当同一个class同时出现在用户jar包和hadoop jar中时，优先使用哪个jar包中的class，默认为false，表示优先使用hadoop jar中的class。(4) mapreduce.input.fileinputformat.split.minsize: 每个Map Task处理的数据量（仅针对基于文件的Inputformat有效，比如TextInputFormat，SequenceFileInputFormat），默认为一个block大小，即 134217728。HBase 相关配置参数(1) hbase.rpc.timeout：rpc的超时时间，默认60s，不建议修改，避免影响正常的业务，在线上环境刚开始配置的是3秒，运行半天后发现了大量的timeout error，原因是有一个region出现了如下问题阻塞了写操作：“Blocking updates … memstore size 434.3m is >= than blocking 256.0m size”可见不能太低。(2) ipc.socket.timeout：socket建立链接的超时时间，应该小于或者等于rpc的超时时间，默认为20s(3) hbase.client.retries.number：重试次数，默认为14，可配置为3(4) hbase.client.pause：重试的休眠时间，默认为1s，可减少，比如100ms(5) hbase.regionserver.lease.period：scan查询时每次与server交互的超时时间，默认为60s，可不调整。Spark 相关配置参数1.  效率及稳定性相关参数建议打开map（注意，在spark引擎中，也只有map和reduce两种task，spark叫ShuffleMapTask和ResultTask）中间结果合并及推测执行功能：spark.shuffle.consolidateFiles=truespark.speculation=trure2.  容错相关参数建议将这些值调大，比如：spark.task.maxFailures=8spark.akka.timeout=300spark.network.timeout=300spark.yarn.max.executor.failures=100	大数据
<title></title>　Spark SQL主要目的是使得用户可以在Spark上使用SQL，其数据源既可以是RDD，也可以是外部的数据源（比如Parquet、Hive、Json等）。Spark SQL的其中一个分支就是Spark on Hive，也就是使用Hive中HQL的解析、逻辑执行计划翻译、执行计划优化等逻辑，可以近似认为仅将物理执行计划从MR作业替换成了Spark作业。本文就是来介绍如何通过Spark SQL来读取现有Hive中的数据。　　不过，预先编译好的Spark assembly包是不支持Hive的，如果你需要在Spark中使用Hive，必须重新编译，加上-Phive选项既可，具体如下：[iteblog@www.iteblog.com spark]$ ./make-distribution.sh --tgz -Phadoop-2.2 -Pyarn -DskipTests -Dhadoop.version=2.2.0  -Phive　　编译完成之后，会在SPARK_HOME的lib目录下多产生三个jar包，分别是datanucleus-api-jdo-3.2.6.jar、datanucleus-core-3.2.10.jar、datanucleus-rdbms-3.2.9.jar，这些包都是Hive所需要的。下面就开始介绍步骤。一、环境准备　　为了让Spark能够连接到Hive的原有数据仓库，我们需要将Hive中的hive-site.xml文件拷贝到Spark的conf目录下，这样就可以通过这个配置文件找到Hive的元数据以及数据存放。　　如果Hive的元数据存放在Mysql中，我们还需要准备好Mysql相关驱动，比如：mysql-connector-java-5.1.22-bin.jar。二、启动spark-shell　　环境准备好之后，为了方便起见，我们使用spark-shell来进行说明如何通过Spark SQL读取Hive中的数据。我们可以通过下面的命令来启动spark-shell：[iteblog@www.iteblog.com spark]$  bin/spark-shell --master yarn-client  --jars lib/mysql-connector-java-5.1.22-bin.jar ....15/08/27 18:21:25 INFO repl.SparkILoop: Created spark context..Spark context available as sc.....15/08/27 18:21:30 INFO repl.SparkILoop: Created sql context (with Hive support)..SQL context available as sqlContext.　　启动spark-shell的时候会先向ResourceManager申请资源，而且还会初始化SparkContext和SQLContext实例。sqlContext对象其实是HiveContext的实例，sqlContext是进入Spark SQL的切入点。接下来我们来读取Hive中的数据。scala> sqlContext.sql("CREATE EXTERNAL  TABLE IF NOT EXISTS ewaplog (key STRING, value STRING) STORED AS INPUTFORMAT 'com.hadoop.mapred.DeprecatedLzoTextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION '/user/iteblog/ewaplog' ")res0: org.apache.spark.sql.DataFrame = [result: string]scala> sqlContext.sql("LOAD DATA LOCAL INPATH '/data/test.lzo' INTO TABLE ewaplog")res1: org.apache.spark.sql.DataFrame = [result: string]scala> sqlContext.sql("FROM ewaplog SELECT key, value").collect().foreach(println)[12,wyp][23,ry][12,wyp][23,ry]　　我们先创建了ewaplog表，然后导入数据，最后查询。我们可以看出所有的SQL和在Hive中是一样的，只是在Spark上运行而已。在执行SQL的时候，默认是调用hiveql解析器来解析SQL的。当然，你完全可以调用Spark SQL内置的SQL解析器sql，可以通过spark.sql.dialect参数来设置。但是建议还是使用hivesql解析器，因为它支持的语法更多，而且还支持Hive的UDF函数，在多数情况下推荐使用hivesql解析器。	大数据
<title></title>spark sql中将数据保存成parquet,json格式val df = sqlContext.load("/opt/modules/spark1.3.1/examples/src/main/resources/people.json","json")df.select("name","age").save("/opt/test/namesAndAges.json","json")//或者 df.select("name","age").save("/opt/test/namesAndAges.parquet","parquet")	大数据
将文件转换为Parquet格式cd /usr/local/hadoop./sbin/start-dfs.sh./sbin/start-yarn.sh# 上传行为日志文件至HDFShdfs dfs -mkdir /user/hadoop/testhdfs dfs -put /home/hadoop/test/user_behavior.txt /user/hadoop/test/# 启动Sparkcd /usr/local/spark./bin/spark-shell// 读取HDFS上的JSON文件val df = sqlContext.read.json("hdfs://localhost:9000/user/hadoop/test/user_behavior.txt")// 保存为parquet文件<blockquote>df.saveAsParquetFile("hdfs://localhost:9000/user/hadoop/test/userBehavior.parquet")</blockquote>// 读取parquet文件<blockquote>val parquetData = sqlContext.parquetFile("hdfs://localhost:9000/user/hadoop/test/userBehavior.parquet")parquetData.show()parquetData.printSchema()</blockquote>// 执行计算df.groupBy("Behavior").count().show()parquetData.groupBy("Behavior").count().show()	大数据
hadoop 2.6.0版本中，YARN引入了一种新的调度策略：基于标签的调度机制。该机制的主要引入动机是更好地让YARN运行在异构集群中，进而更好地管理和调度混合类型的应用程序。注意，截止这篇文章发布时，只有apache hadoop 2.6.0和hdp 2.2两个发行版具有该特性（CDH5.3尚不支持，CDH5.4会支持），在hadoop自带的调度器重，只有 Capacity Scheduler支持该特性，FIFO Scheduler和Fair Scheduler尚不支持。什么是Label based scheduling？故名思议，Label based scheduling是一种调度策略，就像priority-based scheduling一样，是调度器调度众多调度策略中的一种，可以跟其他调度策略混合使用，实际上，hadoop也是这样做的。但是，相比于其他调度策略，基于标签的调度策略则复杂的多，这个feature的代码量非常大，基本上需要修改YARN的各个模块，包括API， ResourceManager，Scheduler等。该策略的基本思想是：用户可以为每个nodemanager标注几个标签，比如highmem，highdisk等，以表明该nodemanager的特性；同时，用户可以为调度器中每个队列标注几个标签，这样，提交到某个队列中的作业，只会使用标注有对应标签的节点上的资源。举个例子：比如最初你们的hadoop集群共有20个节点，硬件资源是32GB内存，4TB磁盘；后来，随着spark地流行，公司希望引入spark计算框架，而为了更好地运行spark程序，公司特地买了10个大内存节点，比如内存是64GB，为了让spark程序与mapreduce等其他程序更加和谐地运行在一个集群中，你们希望spark程序只运行在后来的10个大内存节点上，而之前的mapreduce程序既可以运行在之前的20个节点上，也可以运行在后来的10个大内存节点上，怎么办？有了label-based scheduling后，这是一件非常easy的事情，你需要按一以下步骤操作：步骤1：为旧的20个节点打上normal标签，为新的10个节点打上highmem标签；步骤2：在capacity scheduler中，创建两个队列，分别是hadoop和spark，其中hadoop队列可使用的标签是nornal和highmem，而spark则是highmem，并配置两个队列的capacity和maxcapacity。如何配置使用Label based scheduling？首先，要选择apache hadoop 2.6或hdp2.2（可使用ambari部署）发行版。之后按照以下步骤操作：步骤1：添加系统级别的label（相当于所有label的全集），注意，各个节点上的label必须都在系统级别的label中。yarn rmadmin -addToClusterNodeLabels normal,highmem步骤2：为各个节点分别添加label（可动态修改）yarn rmadmin -replaceLabelsOnNode “nodeId,label1,label2,…,labeln”注意，nodeId是nodemanager的唯一标示，注意，一个节点上可以有多个nodemanager，每个nodemanager的nodeid可以在ResourceManager界面上看到，通常有host和PRC port拼接而成，默认情况下，各个nodemanager的RPC port是随机选取的，你可以将所有的nodemanager配置成一样的，便于管理：<property><name>yarn.nodemanager.address</name><value>0.0.0.0:45454</value></property>步骤3：配置label重启恢复功能。这样，label信息会保存到hdfs上（默认是保存在内存中的），之后yarn重新启动，可以自动恢复所有label信息：<property><name>yarn.node-labels.manager-class</name><value>org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager</value></property>步骤4：修改capacity scheduler相关配置，设置每个队列对应的label，以及每中label的资源上下限。具体配置，可参考相关资源。总结基于标签的调度策略是hadoop yarn新引入的feature，它能让YARN更好地运行在异构集群中，进而更好地管理和调度混合类型的应用程序。	大数据
目前Apache Spark支持三种分布式部署方式，分别是standalone、spark on mesos和 spark on YARN，其中，第一种类似于MapReduce 1.0所采用的模式，内部实现了容错性和资源管理，后两种则是未来发展的趋势，部分容错性和资源管理交由统一的资源管理系统完成：让Spark运行在一个通用的资源管理系统之上，这样可以与其他计算框架，比如MapReduce，公用一个集群资源，最大的好处是降低运维成本和提高资源利用率（资源按需分配）。本文将介绍这三种部署方式，并比较其优缺点。standalone模式，即独立模式，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统。从一定程度上说，该模式是其他两种的基础。借鉴Spark开发模式，我们可以得到一种开发新型计算框架的一般思路：先设计出它的standalone模式，为了快速开发，起初不需要考虑服务（比如master/slave）的容错性，之后再开发相应的wrapper，将stanlone模式下的服务原封不动的部署到资源管理系统yarn或者mesos上，由资源管理系统负责服务本身的容错。目前Spark在standalone模式下是没有任何单点故障问题的，这是借助zookeeper实现的，思想类似于Hbase master单点故障解决方案。将Spark standalone与MapReduce比较，会发现它们两个在架构上是完全一致的：1)  都是由master/slaves服务组成的，且起初master均存在单点故障，后来均通过zookeeper解决（Apache MRv1的JobTracker仍存在单点问题，但CDH版本得到了解决）；2) 各个节点上的资源被抽象成粗粒度的slot，有多少slot就能同时运行多少task。不同的是，MapReduce将slot分为map slot和reduce slot，它们分别只能供Map Task和Reduce Task使用，而不能共享，这是MapReduce资源利率低效的原因之一，而Spark则更优化一些，它不区分slot类型，只有一种slot，可以供各种类型的Task使用，这种方式可以提高资源利用率，但是不够灵活，不能为不同类型的Task定制slot资源。总之，这两种方式各有优缺点。Spark On Mesos模式。这是很多公司采用的模式，官方推荐这种模式（当然，原因之一是血缘关系）。正是由于Spark开发之初就考虑到支持Mesos，因此，目前而言，Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。目前在Spark On Mesos环境中，用户可选择两种调度模式之一运行自己的应用程序（可参考Andrew Xia的“Mesos Scheduling Mode on Spark”）：1)   粗粒度模式（Coarse-grained Mode）：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中，每个Executor占用若干资源，内部可运行多个Task（对应多少个“slot”）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。举个例子，比如你提交应用程序时，指定使用5个executor运行你的应用程序，每个executor占用5GB内存和5个CPU，每个executor内部设置了5个slot，则Mesos需要先为executor分配资源并启动它们，之后开始调度任务。另外，在程序运行过程中，mesos的master和slave并不知道executor内部各个task的运行情况，executor直接将任务状态通过内部的通信机制汇报给Driver，从一定程度上可以认为，每个应用程序利用mesos搭建了一个虚拟集群自己使用。2)   细粒度模式（Fine-grained Mode）：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。与粗粒度模式一样，应用程序启动时，先会启动executor，但每个executor占用资源仅仅是自己运行所需的资源，不需要考虑将来要运行的任务，之后，mesos会为每个executor动态分配资源，每分配一些，便可以运行一个新任务，单个Task运行完之后可以马上释放对应的资源。每个Task会汇报状态给Mesos slave和Mesos Master，便于更加细粒度管理和容错，这种调度模式类似于MapReduce调度模式，每个Task完全独立，优点是便于资源控制和隔离，但缺点也很明显，短作业运行延迟大。Spark On YARN模式。这是一种最有前景的部署模式。但限于YARN自身的发展，目前仅支持粗粒度模式（Coarse-grained Mode）。这是由于YARN上的Container资源是不可以动态伸缩的，一旦Container启动之后，可使用的资源不能再发生变化，不过这个已经在YARN计划（具体参考：https://issues.apache.org/jira/browse/YARN-1197）中了。总之，这三种分布式部署方式各有利弊，通常需要根据公司情况决定采用哪种方案。进行方案选择时，往往要考虑公司的技术路线（采用Hadoop生态系统还是其他生态系统）、服务器资源（资源有限的话就不要考虑standalone模式了）、相关技术人才储备等。	大数据
Aparapi一个Java并行化数据处理的API，通过OpenCL的方式实现了Java代码在GPU上直接执行。 什么是Aparapi？Aparapi能够让Java开发者充分利用GPU和APU设备的计算能力，用以执行数据的并行化处理代码，而非局限在CPU上执行。它的执行过程是将Java字节码在运行时转化为OpenCL执行码然后运行在GPU上，如果遇到Aparapi不能在GPU上执行的情况，它会转为在Java线程池中执行（CPU）。我们认为，通过适当的工作负载（调度），能将Java语言“Write Once Run Anywhere”的特性拓展到GPU设备上。	大数据
hive在建表是，可以通过‘STORED AS FILE_FORMAT’ 指定存储文件格式<code>> CREATE EXTERNAL TABLE MYTEST(num INT, name STRING)  > ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'  > STORED AS TEXTFILE  > LOCATION '/data/test';  </code>ive文件存储格式包括以下几类：TEXTFILESEQUENCEFILERCFILE自定义格式TEXTFIEL默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用（系统自动检查，执行查询时自动解压），但使用这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。SEQUENCEFILE:SequenceFile是Hadoop API提供的一种二进制文件支持，其具有使用方便、可分割、可压缩的特点。SequenceFile支持三种压缩选择：NONE, RECORD, BLOCK。 Record压缩率低，一般建议使用BLOCK压缩。> create table test2(str STRING)  > STORED AS SEQUENCEFILE;  OK  Time taken: 5.526 seconds  hive> SET hive.exec.compress.output=true;  hive> SET io.seqfile.compression.type=BLOCK;  hive> INSERT OVERWRITE TABLE test2 SELECT * FROM test1;  RCFILERCFILE是一种行列存储相结合的存储方式。首先，其将数据按行分块，保证同一个record在一个块上，避免读一个记录需要读取多个block。其次，块数据列式存储，有利于数据压缩和快速的列存取。> create table test3(str STRING)  > STORED AS RCFILE;  OK  Time taken: 0.184 seconds  >  INSERT OVERWRITE TABLE test3 SELECT * FROM test1;  自定义格式当用户的数据文件格式不能被当前 Hive 所识别的时候，可以自定义文件格式。用户可以通过实现inputformat和outputformat来自定义输入输出格式> create table test4(str STRING)  > stored as  > inputformat 'org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat'  > outputformat 'org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat';	大数据
<strong>数据仓库的特点</strong>：一次写入、多次读取，因此，整体来看，ORCFile相比其他格式具有较明显的优势。TextFile 默认格式，加载速度最快，可以采用Gzip、bzip2等进行压缩，压缩后的文件无法split，即并行处理SequenceFile 压缩率最低，查询速度一般，三种压缩格式NONE，RECORD，BLOCKRCfile 压缩率最高，查询速度最快，数据加载最慢。	大数据
Hive数据表的默认格式，存储方式：行存储。可使用Gzip,Bzip2等压缩算法压缩,压缩后的文件不支持split但在反序列化过程中，必须逐个字符判断是不是分隔符和行结束符，因此反序列化开销会比SequenceFile高几十倍。<code>--创建数据表：create table if not exists textfile_table(site string,url  string,pv   bigint,label string)row format delimited fields terminated by '\t'stored as textfile;--插入数据：set hive.exec.compress.output=true; --启用压缩格式 set mapred.output.compress=true;    set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;  --指定输出的压缩格式为Gzip  set io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec;      insert overwrite table textfile_table select * from T_Name;</code>	大数据
SequenceFileHadoop API提供的一种二进制文件，以<key,value>的形式序列化到文件中。存储方式：行存储。支持三种压缩选择：NONE，RECORD，BLOCK。Record压缩率低，一般建议使用BLOCK压缩。优势是文件和hadoop api中的MapFile是相互兼容的<blockquote>create table if not exists seqfile_table(site string,url  string,pv   bigint,label string)row format delimited fields terminated by '\t'stored as sequencefile;--插入数据操作：set hive.exec.compress.output=true;  --启用输出压缩格式set mapred.output.compress=true;  set mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;  --指定输出压缩格式为Gzipset io.compression.codecs=org.apache.hadoop.io.compress.GzipCodec;  SET mapred.output.compression.type=BLOCK; --指定为Blockinsert overwrite table seqfile_table select * from T_Name;</blockquote>	大数据
压缩格式Hadoop 对于压缩格式的是自动识别。如果我们压缩的文件有相应压缩格式的扩展名（比如 lzo，gz，bzip2 等）。Hadoop 会根据压缩格式的扩展名自动选择相对应的解码器来解压数据，此过程完全是 Hadoop 自动处理，我们只需要确保输入的压缩文件有扩展名。Hadoop 对每个压缩格式的支持, 详细见下表：<blockquote>表 1. 压缩格式压缩格式工具算法扩展名多文件可分割性DEFLATE无DEFLATE.deflate不不GZIPgzipDEFLATE.gzp不不ZIPzipDEFLATE.zip是是，在文件范围内BZIP2bzip2BZIP2.bz2不是LZOlzopLZO.lzo不是</blockquote>如果压缩的文件没有扩展名，则需要在执行 MapReduce 任务的时候指定输入格式。<code>hadoop jar /usr/home/hadoop/hadoop-0.20.2/contrib/streaming/  hadoop-streaming-0.20.2-CD H3B4.jar -file /usr/home/hadoop/hello/mapper.py -mapper /  usr/home/hadoop/hello/mapper.py -file /usr/home/hadoop/hello/  reducer.py -reducer /usr/home/hadoop/hello/reducer.py -input lzotest -output result4 -  jobconf mapred.reduce.tasks=1*-inputformatorg.apache.hadoop.mapred.LzoTextInputFormat*</code>1) Bzip2 压缩效果明显是最好的，但是 bzip2 压缩速度慢，可分割。2) Gzip 压缩效果不如 Bzip2，但是压缩解压速度快，不支持分割。3) LZO 压缩效果不如 Bzip2 和 Gzip，但是压缩解压速度最快！并且支持分割！这里提一下，文件的可分割性在 Hadoop 中是很非常重要的，它会影响到在执行作业时 Map 启动的个数，从而会影响到作业的执行效率！所有的压缩算法都显示出一种时间空间的权衡，更快的压缩和解压速度通常会耗费更多的空间。在选择使用哪种压缩格式时，我们应该根据自身的业务需求来选择。MapReduce 可以在三个阶段中使用压缩。1. 输入压缩文件。如果输入的文件是压缩过的，那么在被 MapReduce 读取时，它们会被自动解压。2.MapReduce 作业中，对 Map 输出的中间结果集压缩。实现方式如下：1）可以在 core-site.xml 文件中配置，代码如下图 2. core-site.xml 代码示例mapred.conpress.map.output2）使用 Java 代码指定 conf.setCompressMapOut(true); conf.setMapOutputCompressorClass(GzipCode.class);最后一行代码指定 Map 输出结果的编码器。3.MapReduce 作业中，对 Reduce 输出的最终结果集压。实现方式如下：1）可以在 core-site.xml 文件中配置，代码如下mapred.output.compress2）使用 Java 代码指定 conf.setBoolean(“mapred.output.compress”,true); conf.setClass(“mapred.output.compression.codec”,GzipCode.class,CompressionCodec.class);最后一行同样指定 Reduce 输出结果的编码器。	大数据
<ins>- 1+1等于几 ？- 50- 傻x，多了- 1+2等于几？- 20- 傻x，多了- 3+4等于几- 7- 傻x，对了- 6+9等于几- 13- 傻x，少了很多很多次以后……- 2+2等于几- 4- 4+5等于几- 9这就是机器学习，准确来说是最常见的一种，监督学习。最开始的几步是对于模型的训练，“多了”或“少了”可以理解为训练时的误差，模型根据误差调整自身参数，这就是机器学习里常用的反向传播(Backpropagation)的简单的解释。梯度下降涉及到计算，真没想到该怎么通俗解释。。</ins>	大数据
梯度下降法是一个一阶最优化算法，通常也称为最速下降法。我之前也没有关注过这类算法。最近，听斯坦福大学的机器学习课程时，碰到了用梯度下降算法求解线性回归问题，于是看了看这类算法的思想。今天只写了一些入门级的知识。<code>$e=0.00001;//定义迭代精度  $alpha=0.1;//定义迭代步长  $x=0;//初始化x  $y0=$x*$x-3*$x+2;//与初始化x对应的y值  $y1=0;//定义变量，用于保存当前值 $ci=1; while (true)  {  $xielv=2.0*$x-3.0;$x=$x-$alpha*(2.0*$x-3.0);  $y1=$x*$x-3*$x+2;  if (abs($y1-$y0)<$e)//如果2次迭代的结果变化很小，结束迭代  {  break;  }echo "No. $ci  $y0,$y1,$xielv\n";$ci++;$y0=$y1;//更新迭代的结果  }  echo "Min(f(x))=",$y0,"\n";  echo "minx=",$x;  </code>	大数据
支持向量机将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面。分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。一个极好的指南是C.J.C Burges的《模式识别支持向量机指南》。van der Walt 和 Barnard 将支持向量机和其他分类器进行了比较。 有很多个分类器(超平面）可以把数据分开，但是只有一个能够达到最大分割。我们通常希望分类的过程是一个机器学习的过程。这些数据点并不需要是中的点，而可以是任意(统计学符号)中或者 (计算机科学符号) 的点。我们希望能够把这些点通过一个n-1维的超平面分开，通常这个被称为线性分类器。有很多分类器都符合这个要求，但是我们还希望找到分类最佳的平面，即使得属于两个不同类的数据点间隔最大的那个面，该面亦称为最大间隔超平面。如果我们能够找到这个面，那么这个分类器就称为最大间隔分类器。	大数据
1. C4.5C4.5算法是机器学习算法中的一种分类决策树算法,其核心算法是ID3算法.  C4.5算法继承了ID3算法的优点，并在以下几方面对ID3算法进行了改进：1) 用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；    2) 在树构造过程中进行剪枝；    3) 能够完成对连续属性的离散化处理；    4) 能够对不完整数据进行处理。C4.5算法有如下优点：产生的分类规则易于理解，准确率较高。其缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。2. The k-means algorithm 即K-Means算法k-means algorithm算法是一个聚类算法，把n的对象根据他们的属性分为k个分割，k < n。它与处理混合正态分布的最大期望算法很相似，因为他们都试图找到数据中自然聚类的中心。它假设对象属性来自于空间向量，并且目标是使各个群组内部的均方误差总和最小。3. Support vector machines支持向量机，英文为Support Vector Machine，简称SV机（论文中一般简称SVM）。它是一种監督式學習的方法，它广泛的应用于统计分类以及回归分析中。支持向量机将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面。分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。一个极好的指南是C.J.C Burges的《模式识别支持向量机指南》。van der Walt 和 Barnard 将支持向量机和其他分类器进行了比较。4. The Apriori algorithmApriori算法是一种最有影响的挖掘布尔关联规则频繁项集的算法。其核心是基于两阶段频集思想的递推算法。该关联规则在分类上属于单维、单层、布尔关联规则。在这里，所有支持度大于最小支持度的项集称为频繁项集，简称频集。 5. 最大期望(EM)算法在统计计算中，最大期望（EM，Expectation–Maximization）算法是在概率（probabilistic）模型中寻找参数最大似然估计的算法，其中概率模型依赖于无法观测的隐藏变量（Latent Variabl）。最大期望经常用在机器学习和计算机视觉的数据集聚（Data Clustering）领域。6. PageRankPageRank是Google算法的重要内容。2001年9月被授予美国专利，专利人是Google创始人之一拉里·佩奇（Larry Page）。因此，PageRank里的page不是指网页，而是指佩奇，即这个等级方法是以佩奇来命名的。 PageRank根据网站的外部链接和内部链接的数量和质量俩衡量网站的价值。PageRank背后的概念是，每个到页面的链接都是对该页面的一次投票，被链接的越多，就意味着被其他网站投票越多。这个就是所谓的“链接流行度”——衡量多少人愿意将他们的网站和你的网站挂钩。PageRank这个概念引自学术中一篇论文的被引述的频度——即被别人引述的次数越多，一般判断这篇论文的权威性就越高。7. AdaBoostAdaboost是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器(弱分类器)，然后把这些弱分类器集合起来，构成一个更强的最终分类器 (强分类器)。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。8. kNN: k-nearest neighbor classificationK最近邻(k-Nearest Neighbor，KNN)分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。 9. Naive Bayes在众多的分类模型中，应用最为广泛的两种分类模型是决策树模型(Decision Tree Model)和朴素贝叶斯模型（Naive Bayesian Model，NBC）。 朴素贝叶斯模型发源于古典数学理论，有着坚实的数学基础，以及稳定的分类效率。同时，NBC模型所需估计的参数很少，对缺失数据不太敏感，算法也比较简单。理论上，NBC模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为NBC模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，这给NBC模型的正确分类带来了一定影响。在属性个数比较多或者属性之间相关性较大时，NBC模型的分类效率比不上决策树模型。而在属性相关性较小时，NBC模型的性能最为良好。10. CART: 分类与回归树CART, Classification and Regression Trees。 在分类树下面有两个关键的思想。第一个是关于递归地划分自变量空间的想法；第二个想法是用验证数据进行剪枝。	大数据
scikit-learn提供了各种机器学习算法的接口，允许用户可以很方便地使用。每个算法的调用就像一个黑箱，对于用户来说，我们只需要根据自己的需求，设置相应的参数。<a href='http://scikit-learn.org/stable/index.html'>link</a>	大数据
随机森林的构造过程：<blockquote>　　1. 假如有N个样本，则有放回的随机选择N个样本(每次随机选择一个样本，然后返回继续选择)。这选择好了的N个样本用来训练一个决策树，作为决策树根节点处的样本。　　2. 当每个样本有M个属性时，在决策树的每个节点需要分裂时，随机从这M个属性中选取出m个属性，满足条件m << M。然后从这m个属性中采用某种策略（比如说信息增益）来选择1个属性作为该节点的分裂属性。　　3. 决策树形成过程中每个节点都要按照步骤2来分裂（很容易理解，如果下一次该节点选出来的那一个属性是刚刚其父节点分裂时用过的属性，则该节点已经达到了叶子节点，无须继续分裂了）。一直到不能够再分裂为止。注意整个决策树形成过程中没有进行剪枝。　　4. 按照步骤1~3建立大量的决策树，这样就构成了随机森林了。　　从上面的步骤可以看出，随机森林的随机性体现在每颗数的训练样本是随机的，树中每个节点的分类属性也是随机选择的。有了这2个随机的保证，随机森林就不会产生过拟合的现象了。 　　随机森林有2个参数需要人为控制，一个是森林中树的数量，一般建议取很大。另一个是m的大小，推荐m的值为M的均方根。</blockquote>	大数据
根据下列算法而建造每棵树：<code>      1. 用 N 来表示训练例子的个数，M表示变量的数目。      2. 我们会被告知一个数 m ，被用来决定当在一个节点上做决定时，会使用到多少个变量。m应小于M      3. 从N个训练案例中以可重复取样的方式，取样N次，形成一组训练集（即bootstrap取样。）。并使用这棵树来对剩余预测其类别，并评估其误差。      4. 对于每一个节点，随机选择m个基于此点上的变量。根据这 m 个变量，计算其最佳的分割方式。      5. 每棵树都会完整成长而不会剪枝（Pruning）（这有可能在建完一棵正常树状分类器后会被采用）。</code>优点 随机森林的优点有：<code>      1. 对于很多种资料，它可以产生高准确度的分类器。      2. 它可以处理大量的输入变量。      3. 它可以在决定类别时，评估变量的重要性。      4. 在建造森林时，它可以在内部对于一般化后的误差产生不偏差的估计。      5. 它包含一个好方法可以估计遗失的资料，并且，如果有很大一部分的资料遗失，仍可以维持准确度。      6. 它提供一个实验方法，可以去侦测 variable interactions 。      7. 对于不平衡的分类资料集来说，它可以平衡误差。      8. 它计算各例中的亲近度，对于数据挖掘、侦测偏离者（outlier）和将资料视觉化非常有用。      9. 使用上述。它可被延伸应用在未标记的资料上，这类资料通常是使用非监督式聚类。也可侦测偏离者和观看资料。      10. 学习过程是很快速的。</code>缺点<code>      1. 随机森林已经被证明在某些噪音较大的分类或回归问题上会过拟      2. 对于有不同级别的属性的数据，级别划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。</code>	大数据
<code>from sklearn.datasets import load_irisfrom sklearn.ensemble import RandomForestClassifierimport pandas as pdimport numpy as npiris = load_iris()df = pd.DataFrame(iris.data, columns=iris.feature_names)df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75#df['species'] = pd.Categorical(iris.target, iris.target_names)df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)df.head()train, test = df[df['is_train']==True], df[df['is_train']==False]features = df.columns[:4]clf = RandomForestClassifier(n_jobs=2)y, _ = pd.factorize(train['species'])clf.fit(train[features], y)preds = iris.target_names[clf.predict(test[features])]pd.crosstab(test['species'], preds, rownames=['actual'], colnames=['preds'])</code>	大数据
<code>import matplotlib.pyplot as plt# Import datasets, classifiers and performance metricsfrom sklearn import datasets, svm, metrics# The digits datasetdigits = datasets.load_digits()# The data that we are interested in is made of 8x8 images of digits, let's# have a look at the first 3 images, stored in the `images` attribute of the# dataset.  If we were working from image files, we could load them using# pylab.imread.  Note that each image must have the same size. For these# images, we know which digit they represent: it is given in the 'target' of# the dataset.images_and_labels = list(zip(digits.images, digits.target))for index, (image, label) in enumerate(images_and_labels[:4]):    plt.subplot(2, 4, index + 1)    plt.axis('off')    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')    plt.title('Training: %i' % label)# To apply a classifier on this data, we need to flatten the image, to# turn the data in a (samples, feature) matrix:n_samples = len(digits.images)data = digits.images.reshape((n_samples, -1))# Create a classifier: a support vector classifierclassifier = svm.SVC(gamma=0.001)# We learn the digits on the first half of the digitsclassifier.fit(data[:n_samples / 2], digits.target[:n_samples / 2])# Now predict the value of the digit on the second half:expected = digits.target[n_samples / 2:]predicted = classifier.predict(data[n_samples / 2:])print("Classification report for classifier %s:\n%s\n"      % (classifier, metrics.classification_report(expected, predicted)))print("Confusion matrix:\n%s" % metrics.confusion_matrix(expected, predicted))images_and_predictions = list(zip(digits.images[n_samples / 2:], predicted))for index, (image, prediction) in enumerate(images_and_predictions[:4]):    plt.subplot(2, 4, index + 5)    plt.axis('off')    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')    plt.title('Prediction: %i' % prediction)plt.show()</code>	大数据
iris以鸢尾花的特征作为数据来源，数据集包含150个数据集，分为3类，每类50个数据，每个数据包含4个属性，是在数据挖掘、数据分类中非常常用的测试集、训练集 三类分别为:setosa, versicolor, virginica 数据包含4个独立的属性,这些属性变量测量植物的花朵,比如萼片和花瓣的长度等.	大数据
大数定律 <blockquote>概率论历 史上第一个极限定理属于伯努利，后人称之为“大数定律”。概率论中讨论随机变量序列的算术平均值向随机变量各数学期望的算术平均值收敛的定律。大数定律(law of large numbers)，是一种描述当试验次数很大时所呈现的概率性质的定律。但是注意到，大数定律并不是经验规律，而是在一些附加条件上经严格证明了的定理，它是一种自然规律因而通常不叫定理而是大数“定律”。而我们说的大数定理通常是经数学家证明并以数学家名字命名的大数定理，如伯努利大数定理。在随机事件的大量重复出现中，往往呈现几乎必然的规律，这个规律就是大数定律。通俗地说，这个定理就是，在试验不变的条件下，重复试验多次，随机事件的频率近似于它的概率。偶然中包含着某种必然。[1] 大数定律分为弱大数定律和强大数定律。</blockquote>	大数据
<code>最长公共子序列。题目：如果字符串一的所有字符按其在字符串中的顺序出现在另外一个字符串二中，则字符串一称之为字符串二的子串。注意，并不要求子串（字符串一）的字符必须连续出现在字符串二中。请编写一个函数，输入两个字符串，求它们的最长公共子串，并打印出最长公共子串。例如：输入两个字符串BDCABA和ABCBDAB，字符串BCBA和BDAB都是是它们的最长公共子串，则输出它们的长度4，并打印任意一个子串。 分析：求最长公共子串（Longest Common Subsequence, LCS）是一道非常经典的动态规划题，因此一些重视算法的公司像MicroStrategy都把它当作面试题。</code>	大数据
<blockquote>ndarray：n-dimensional array object，即多维数组对象，是python自带的array对象的扩展，array对象和list对象的区别是array对象的每一个元素都是数值，而list保存的是每个元素对象的指针，而作为array对象的扩展，ndarray在科学计算中就非常适合并且功能强大。创建ndarray1. 使用列表对象创建ndarrayimport bumpy as npa = np.array([1,2,3,4])以上为一种创建ndarray的方式，即用一个列表对象来创建，注意，ndarray中的所有元素类型都必须一致（atomic vector），上例中，如果其中一个元素为字符类型，则所有的元素都会自动转换为字符类型，通过dtype属性来获取ndarray中每个元素的类型，同时numpy中还提供了numpy.typeDict来存放各种类型的简称-类型字典；由于ndarray是多维数组对象，可以通过shape属性来获取数组的各维情况，并且可以通过reshape方法改变当前的形状；NewImage 2. 使用numpy中特有函数创建ndarray除了使用python中的列表对象来创建ndarray之外，numpy中还提供了一系列特定的函数用来创建ndarraynumpy.arange(0,1,0.1)：arange和range类似，通过指定起始值、终止值、步长来创建数组，是一个前闭后开的区间，如该例创建的数组为：array([0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])；numpy.linspace(0,1,10)：linspace是通过指定起始值、终止值、元素个数来创建一个等差数组，默认为前闭后闭，通过设置endpoint=False可以对终止值设置是否包含；numpy.logspace(0,1,12,base=2,endpoint=False)：logspace是和linspace的区别是创建的是等比数组，默认以10为底，通过设置base改变底值；numpy.zeros()、numpy.ones()、numpy.empty()等函数创建特定形状的数组，和matlab中类似；numpy.fromstring()：通过字符串来创建数组，字符串的每一个字符占用一个字节的空间，也就是8个字节，通过指定dtype，创建不同的数组；当dtype=int8时，每个字符对应一个元素，当dtype=16时，两个字符对应一个元素，并且以16进制保存（高位的字符需要乘以16）；numpy.fromfunction(func, (10,))：该函数通过指定一个创建函数和一个shape来创建数组，当shape只有一个元素（一维数组）时，func函数应该也只有一个参数，并且该参数自动从0开始递增带入得到一系列返回值，从而创建ndarray；NewImageNewImage存取ndarray1. 存取一维ndarrayndarray的元素访问支持python中的列表的访问方式（下标以及切片）访问ndarray元素，通过下标和切片访问得到的新数组是原始数组的一个视图（view），和原始数组共享存储；除了列表的访问方式，ndarray还支持整数列表、整数数组、布尔数组等访问方式，通过这些访问方式得到的新数组是原始数组的拷贝，并不共享存储；整数列表的访问会按照给定列表的元素作为下标依次访问原始数组中的每个元素：a = np.arange(10)b = a[[3,3,-3,8]]整数数组的访问在列表的基础上，访问得到的新数组会保留数组的形状；布尔数组的访问，指使用一个布尔数组，只访问到布尔数组中True对应下标的元素；注意，只对布尔数组有效，如果是列表，则会讲True和False当做1和0作为下标处理；布尔数组的访问通常由ufunc产生；NewImage 2. 存取多维数组访问多维数组需要指定多维的下标，在python中实现多维下标的方式是通过元组（tuple），由于 1,2 和 (1,2)表示的语义相同，所以用元组表示下标时，也变得非常自然；ndarray的多维数组中，可以通过元组中分别指定切片来访问多维数组（得到的仍然是视图）；同时也可以通过数组和切片的组合、数组和数组的组合来访问多维数组，记住将元组作为下标，元组中的每个元素分别对应某一维上的下标，则一切迎刃而解；需要注意的是，使用切片是指定某一行或者某一列等（保留形状），但是用整数数组访问时，访问的是某一个下标（不会保留形状，只是简单地将值取出）；如果一个多维数组访问时，指定的下标小于维数，则剩余的维数下标默认全部访问（相当于切片 : 代替）；如果用数组和数组的结合来访问多维数组，则只有当两个数组的形状相同时，得到的数组和下标数组的形状相同；numpy中可以使用一个列数组和一个行数组相加得到一个二维数组；NewImageNewImage结构数组通过使用结构数组，可以实现类似C语言中struct的功能；结构数组使用numpy.dtype()实例，实例化时，使用一个字典指定结构的名称和对应的类型（类型可以使用numpy.typeDict中的简称，字符串通过S32等来表示，32指定保存时的字符串长度，用以内存对齐）；结构数组除了数组用下标的访问方式之外，还可以通过字段访问；通过numpy定义的结构数组可以通过tofile方法直接保存到文件中，并且该文件可以被c语言获取结构；NewImagea.tofile(’test.bin')通过c语言读取：#include <stdio.h>struct person{    char name[32];    int age;    float weight;} struct person p[3];void main(){    FILE *fp;    int i;    fp = fopen("test.bin","rb");    fread(p, sizeof(struct person), 2, fp);    fclose(fp);    for(i=0; i<2; i++){       printf("%s %d %f\n",p[i].name, p[i].age, p[i].weight);    }} 内存结构adarray的信息使用一个数组来描述，该数组引用两个对象：dtype和data，其中data表示一个连续的数据存储区域，并保存数组的形状以及间隔，从而访问连续存储区域中的指定位置存放的数据，实现对数组的访问；NewImage如上图，dtype指向元素类型的对象；dim count表示维数，dimensions是一个长度为dim count的数组，指定数组每一维的长度；strides是一个长度为dim count的数组，每个元素是一个整数值，用来指定各维元素的间隔，如上图中表示第一维元素每增加1，则地址增加12，第二维元素每增加1，则地址增加4；numpy中默认的数据存储排序方式是c语言中的排序方式，即第一维是最上位的，第一维的值增加1时，地址增加的范围最大，而在fortran语言中，第一维是最下位的，第一维增加1时，只增加一个元素的字节长度，如上例中，strides如果按照fortran语言中的排序方式，则应该为：(4,12)，通过在定义数组时，指定order=“F”，来使用fortran语言的排序方式；了解内存结构可以解释以下问题：为什么切片访问数组时，得到的数组是视图，而通过数组访问数组时，得到的数组是一份拷贝？由于切片访问数组时，只需要改变strides的值，就可以对数组进行另外一种方式的访问，如如果存在数组a按照上图的方式保存，则b = [::2,::2]得到一个新的数组时，只需要将strides改为(24,8)就可以，而data则可以引用相同的存储区域；而当使用数组进行访问时，由于并不能保证是等间隔分布的，无法通过改变其他属性的方式保持数据不变，所以只能新建一份数据的拷贝来实现；通过数组的flags属性可以查看数据存储区域的属性；</blockquote>	语言
<code>class LinearSVC(BaseEstimator, LinearClassifierMixin,                _LearntSelectorMixin, SparseCoefMixin):    """Linear Support Vector Classification.    Similar to SVC with parameter kernel='linear', but implemented in terms of    liblinear rather than libsvm, so it has more flexibility in the choice of    penalties and loss functions and should scale better to large numbers of    samples.    This class supports both dense and sparse input and the multiclass support    is handled according to a one-vs-the-rest scheme.    Read more in the :ref:`User Guide <svm_classification>`.    Parameters    ----------    C : float, optional (default=1.0)        Penalty parameter C of the error term.    loss : string, 'hinge' or 'squared_hinge' (default='squared_hinge')        Specifies the loss function. 'hinge' is the standard SVM loss        (used e.g. by the SVC class) while 'squared_hinge' is the        square of the hinge loss.    penalty : string, 'l1' or 'l2' (default='l2')        Specifies the norm used in the penalization. The 'l2'        penalty is the standard used in SVC. The 'l1' leads to ``coef_``        vectors that are sparse.    dual : bool, (default=True)        Select the algorithm to either solve the dual or primal        optimization problem. Prefer dual=False when n_samples > n_features.    tol : float, optional (default=1e-4)        Tolerance for stopping criteria.    multi_class: string, 'ovr' or 'crammer_singer' (default='ovr')        Determines the multi-class strategy if `y` contains more than        two classes.        ``"ovr"`` trains n_classes one-vs-rest classifiers, while ``"crammer_singer"``        optimizes a joint objective over all classes.        While `crammer_singer` is interesting from a theoretical perspective        as it is consistent, it is seldom used in practice as it rarely leads        to better accuracy and is more expensive to compute.        If ``"crammer_singer"`` is chosen, the options loss, penalty and dual will        be ignored.    fit_intercept : boolean, optional (default=True)        Whether to calculate the intercept for this model. If set        to false, no intercept will be used in calculations        (i.e. data is expected to be already centered).    intercept_scaling : float, optional (default=1)        When self.fit_intercept is True, instance vector x becomes        ``[x, self.intercept_scaling]``,        i.e. a "synthetic" feature with constant value equals to        intercept_scaling is appended to the instance vector.        The intercept becomes intercept_scaling * synthetic feature weight        Note! the synthetic feature weight is subject to l1/l2 regularization        as all other features.        To lessen the effect of regularization on synthetic feature weight        (and therefore on the intercept) intercept_scaling has to be increased.    class_weight : {dict, 'balanced'}, optional        Set the parameter C of class i to ``class_weight[i]*C`` for        SVC. If not given, all classes are supposed to have        weight one.        The "balanced" mode uses the values of y to automatically adjust        weights inversely proportional to class frequencies in the input data        as ``n_samples / (n_classes * np.bincount(y))``    verbose : int, (default=0)        Enable verbose output. Note that this setting takes advantage of a        per-process runtime setting in liblinear that, if enabled, may not work        properly in a multithreaded context.    random_state : int seed, RandomState instance, or None (default=None)        The seed of the pseudo random number generator to use when        shuffling the data.    max_iter : int, (default=1000)        The maximum number of iterations to be run.    Attributes    ----------    coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features]        Weights assigned to the features (coefficients in the primal        problem). This is only available in the case of a linear kernel.        ``coef_`` is a readonly property derived from ``raw_coef_`` that        follows the internal memory layout of liblinear.    intercept_ : array, shape = [1] if n_classes == 2 else [n_classes]        Constants in decision function.    Notes    -----    The underlying C implementation uses a random number generator to    select features when fitting the model. It is thus not uncommon    to have slightly different results for the same input data. If    that happens, try with a smaller ``tol`` parameter.    The underlying implementation, liblinear, uses a sparse internal    representation for the data that will incur a memory copy.    Predict output may not match that of standalone liblinear in certain    cases. See :ref:`differences from liblinear <liblinear_differences>`    in the narrative documentation.    References    ----------    `LIBLINEAR: A Library for Large Linear Classification    <http://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__    See also    --------    SVC        Implementation of Support Vector Machine classifier using libsvm:        the kernel can be non-linear but its SMO algorithm does not        scale to large number of samples as LinearSVC does.        Furthermore SVC multi-class mode is implemented using one        vs one scheme while LinearSVC uses one vs the rest. It is        possible to implement one vs the rest with SVC by using the        :class:`sklearn.multiclass.OneVsRestClassifier` wrapper.        Finally SVC can fit dense data without memory copy if the input        is C-contiguous. Sparse data will still incur memory copy though.    sklearn.linear_model.SGDClassifier        SGDClassifier can optimize the same cost function as LinearSVC        by adjusting the penalty and loss parameters. In addition it requires        less memory, allows incremental (online) learning, and implements        various loss functions and regularization regimes.    """</code>	语言
下载 numpy-1.12.0+mkl-cp27-cp27m-win_amd64.whl ， scipy-0.18.1-cp27-cp27m-win_amd64.whl 和 matplotlib-2.0.0-cp27-cp27m-win_amd64.whl 。然后进入文件所在目录，使用pip安装即可。pip install numpy-1.12.0+mkl-cp27-cp27m-win_amd64.whlpip install scipy-0.18.1-cp27-cp27m-win_amd64.whlpip install matplotlib-2.0.0-cp27-cp27m-win_amd64.whl	语言
Download <a href='https://bootstrap.pypa.io/get-pip.py'>get-pip.py</a>, being careful to save it as a .py file rather than .txt. Then, run it from the command prompt:python get-pip.py	语言
pip install --user numpy scipy matplotlib ipython jupyter pandas sympy nosewindows http://www.lfd.uci.edu/~gohlke/pythonlibs/#scipyWindows packages<a class="headerlink" href="#windows-packages" title="Permalink to this headline">¶</a></h2><p>Windows does not have any package manager analogous to that in Linux, so installingone of the scientific Python distributions mentioned above is preferred. However, ifthat is not an option, Christoph Gohlke provides <a class="reference external" href="http://www.lfd.uci.edu/~gohlke/pythonlibs/">pre-built Windows installers</a>for many Python packages, including all of the core SciPy stack, which work extremely well.</p>	语言
<span class="kwd">from</span><span class="pln"> numpy</span><span class="pun">.</span><span class="pln">_distributor_init </span><span class="kwd">import</span><span class="pln"> NUMPY_MKL  </span><span class="com"># requires numpy+mkl</span></code></pre><p>This line comment states the dependency as <code>numpy+mkl</code> (<code>numpy</code> with <a href="http://www.intel.com/software/products/mkl/"><strong>Intel Math Kernel Library</strong></a>). This means that you've installed the <code>numpy</code> by <code>pip</code>, but the <code>scipy</code> was installed by precompiled archive, which expects <code>numpy+mkl</code>.</p><p>This problem can be easy solved by installation for <code>numpy+mkl</code> from whl file from <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy">here</a>.</p>	语言
<span class="kwd">from</span><span class="pln"> numpy</span><span class="pun">.</span><span class="pln">_distributor_init </span><span class="kwd">import</span><span class="pln"> NUMPY_MKL  </span><span class="com"># requires numpy+mkl</span></code></pre><p>This line comment states the dependency as <code>numpy+mkl</code> (<code>numpy</code> with <a href="http://www.intel.com/software/products/mkl/"><strong>Intel Math Kernel Library</strong></a>). This means that you've installed the <code>numpy</code> by <code>pip</code>, but the <code>scipy</code> was installed by precompiled archive, which expects <code>numpy+mkl</code>.</p><p>This problem can be easy solved by installation for <code>numpy+mkl</code> from whl file from <a href="http://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy">here</a>.</p>	python
<code>from sklearn import datasetsfrom sklearn import svmimport numpy as npiris = datasets.load_iris()clf = svm.LinearSVC()clf.fit(iris.data, iris.target)X = [[ 5.0,  3.6,  1.3,  0.25],[ 2.0,  3.6,  1.3,  1.25]];print "\nX=\n", Xprint "\nclf.coef_=\n",clf.coef_print "\nnp.dot(X,clf.coef_.T)=\n",np.dot(X,clf.coef_.T)print "\nclf.intercept_=\n",clf.intercept_print "\nnp.dot(X,clf.coef_.T)+clf.intercept_=\n",np.dot(X,clf.coef_.T)+clf.intercept_print "\nclf.decision_function(X)=\n",clf.decision_function(X)print clf.predict(X)</code><blockquote>X=[[5.0, 3.6, 1.3, 0.25], [2.0, 3.6, 1.3, 1.25]]clf.coef_=[[ 0.1842342   0.4512267  -0.80794745 -0.45071037] [ 0.05217835 -0.89257794  0.40478014 -0.93829541] [-0.85075387 -0.98671947  1.38099741  1.86547753]]np.dot(X,clf.coef_.T)=[[ 1.38257785 -2.66074849 -5.54429344] [ 0.37916487 -3.75557897 -1.1265543 ]]clf.intercept_=[ 0.10956035  1.6669923  -1.70960085]np.dot(X,clf.coef_.T)+clf.intercept_=[[ 1.4921382  -0.99375619 -7.25389429] [ 0.48872522 -2.08858667 -2.83615515]]clf.decision_function(X)=[[ 1.4921382  -0.99375619 -7.25389429] [ 0.48872522 -2.08858667 -2.83615515]][0 0]</blockquote>	python
<code>"""Examples illustrating the use of plt.subplots().This function creates a figure and a grid of subplots with a single call, whileproviding reasonable control over how the individual plots are created.  Forvery refined tuning of subplot creation, you can still use add_subplot()directly on a new figure."""import matplotlib.pyplot as pltimport numpy as np# Simple data to display in various formsx = np.linspace(0, 2 * np.pi, 400)y = np.sin(x ** 2)plt.close('all')# Just a figure and one subplotf, ax = plt.subplots()ax.plot(x, y)ax.set_title('Simple plot')# Two subplots, the axes array is 1-df, axarr = plt.subplots(2, sharex=True)axarr[0].plot(x, y)axarr[0].set_title('Sharing X axis')axarr[1].scatter(x, y)# Two subplots, unpack the axes array immediatelyf, (ax1, ax2) = plt.subplots(1, 2, sharey=True)ax1.plot(x, y)ax1.set_title('Sharing Y axis')ax2.scatter(x, y)# Three subplots sharing both x/y axesf, (ax1, ax2, ax3) = plt.subplots(3, sharex=True, sharey=True)ax1.plot(x, y)ax1.set_title('Sharing both axes')ax2.scatter(x, y)ax3.scatter(x, 2 * y ** 2 - 1, color='r')# Fine-tune figure; make subplots close to each other and hide x ticks for# all but bottom plot.f.subplots_adjust(hspace=0)plt.setp([a.get_xticklabels() for a in f.axes[:-1]], visible=False)# row and column sharingf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row')ax1.plot(x, y)ax1.set_title('Sharing x per column, y per row')ax2.scatter(x, y)ax3.scatter(x, 2 * y ** 2 - 1, color='r')ax4.plot(x, 2 * y ** 2 - 1, color='r')# Four axes, returned as a 2-d arrayf, axarr = plt.subplots(2, 2)axarr[0, 0].plot(x, y)axarr[0, 0].set_title('Axis [0,0]')axarr[0, 1].scatter(x, y)axarr[0, 1].set_title('Axis [0,1]')axarr[1, 0].plot(x, y ** 2)axarr[1, 0].set_title('Axis [1,0]')axarr[1, 1].scatter(x, y ** 2)axarr[1, 1].set_title('Axis [1,1]')# Fine-tune figure; hide x ticks for top plots and y ticks for right plotsplt.setp([a.get_xticklabels() for a in axarr[0, :]], visible=False)plt.setp([a.get_yticklabels() for a in axarr[:, 1]], visible=False)# Four polar axesf, axarr = plt.subplots(2, 2, subplot_kw=dict(projection='polar'))axarr[0, 0].plot(x, y)axarr[0, 0].set_title('Axis [0,0]')axarr[0, 1].scatter(x, y)axarr[0, 1].set_title('Axis [0,1]')axarr[1, 0].plot(x, y ** 2)axarr[1, 0].set_title('Axis [1,0]')axarr[1, 1].scatter(x, y ** 2)axarr[1, 1].set_title('Axis [1,1]')# Fine-tune figure; make subplots farther from each other.f.subplots_adjust(hspace=0.3)plt.show()</code><a href='http://matplotlib.org/examples/pylab_examples/subplots_demo.html'>link</a>	python
升级python下载python2.7并安装复制代码wget https://www.python.org/ftp/python/2.7.10/Python-2.7.10.tgztar -zxvf Python-2.7.10.tgzcd Python-2.7.10./configure  make all             make installmake clean  make distclean  复制代码检查python版本python --version　　发现还是2.6更改python命令指向mv /usr/bin/python /usr/bin/python2.6.6_bakln -s /usr/local/bin/python2.7 /usr/bin/python再次检查版本# python --versionPython 2.7.10	python
pip install ipython==2.3.0pip instlal "ipython[notebook]"	python
<blockquote>#encoding=utf-8import sysreload(sys) sys.setdefaultencoding( "utf-8" )import jiebaseg_list = jieba.cut("我来到北京清华大学", cut_all=True)print "Full Mode:", "/ ".join(seg_list)  # 全模式seg_list = jieba.cut("我来到北京清华大学", cut_all=False)print "Default Mode:", "/ ".join(seg_list)  # 精确模式seg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式print ", ".join(seg_list)seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式print ", ".join(seg_list)</blockquote>	python
<a href="http://www.wp.rabbit-e.com/wp/wp-content/uploads/2017/04/img_4400.jpg"><img src="http://www.wp.rabbit-e.com/wp/wp-content/uploads/2017/04/img_4400.jpg" alt="" width="800" height="600" class="alignnone size-full wp-image-329"></a>	机器学习
全文检索引擎 1、Sphinx1.1.Sphinx是什么Sphinx是由俄罗斯人Andrew Aksyonoff开发的一个全文检索引擎。意图为其他应用提供高速、低空间占用、高结果 相关度的全文搜索功能。Sphinx可以非常容易的与SQL数据库和脚本语言集成。当前系统内置MySQL和PostgreSQL 数据库数据源的支持，也支持从标准输入读取特定格式 的XML数据。通过修改源代码，用户可以自行增加新的数据源（例如：其他类型的DBMS 的原生支持）Official APIs for PHP, Python, Java, Ruby, pure C are included in Sphinx distribution 1.2.Sphinx的特性高速的建立索引(在当代CPU上，峰值性能可达到10 MB/秒);高性能的搜索(在2 – 4GB 的文本数据上，平均每次检索响应时间小于0.1秒);可处理海量数据(目前已知可以处理超过100 GB的文本数据, 在单一CPU的系统上可 处理100 M 文档);提供了优秀的相关度算法，基于短语相似度和统计（BM25）的复合Ranking方法;支持分布式搜索;支持短语搜索提供文档摘要生成可作为MySQL的存储引擎提供搜索服务;支持布尔、短语、词语相似度等多种检索模式;文档支持多个全文检索字段(最大不超过32个);文档支持多个额外的属性信息(例如：分组信息，时间戳等);支持断词;1.3.Sphinx中文分词中文的全文检索和英文等latin系列不一样，后者是根据空格等特殊字符来断词，而中文是根据语义来分词。目前大多数数据库尚未支持中文全文检索，如Mysql。故，国内出现了一些Mysql的中文全文检索的插件，做的比较好的有hightman的中文分词。Sphinx如果需要对中文进行全文检索，也得需要一些插件来补充。其中我知道的插件有 coreseek 和 sfc 。 2、Xapian Xapian is an Open Source Search Engine Library, released under the GPL. It's written in C++, with bindings to allow use from Perl, Python, PHP, Java, Tcl, C# and Ruby (so far!)Xapian is a highly adaptable toolkit which allows developers to easily add advanced indexing and search facilities to their own applications. It supports the Probabilistic Information Retrieval model and also supports a rich set of boolean query operators. 爬虫1、Scrapy1.1、What is ScrapyScrapy is a fast high-level screen scraping and web crawling framework, used to crawl websites and extract structured data from their pages. It can be used for a wide range of purposes, from data mining to monitoring and automated testing.1.2、Scrapy FeaturesSimple        Scrapy was designed with simplicity in mind, by providing the features you need without getting in your wayProductive        Just write the rules to extract the data from web pages and let Scrapy crawl the entire web site for youFastScrapy is used in production crawlers to completely scrape more than 500 retailer sites daily, all in one serverExtensibleScrapy was designed with extensibility in mind and so it provides several mechanisms to plug new code without having to touch the framework corePortableScrapy runs on Linux, Windows, Mac and BSDOpen Source and 100% PythonScrapy is completely written in Python, which makes it very easy to hackWell-testedScrapy has an extensive test suite with very good code coverage Html处理1、Beautiful Soup Beautiful Soup 是用Python写的一个HTML/XML的解析器，它可以很好的处理不规范标记并生成剖析树(parse tree)。它提供简单又常用的导航（navigating），搜索以及修改剖析树的操作。它可以大大节省你的编程时间。 对于Ruby，使用Rubyful Soup。 与web站点交互1、mechanizeStateful programmatic web browsing in Python, after Andy Lester’s Perl module mechanize.Browser and mechanize.UserAgentBase implement the interface of urllib2.OpenerDirector, so:any URL can be opened, not just http:mechanize.UserAgentBase offers easy dynamic configuration of user-agent features like protocol, cookie, redirection and robots.txt handling, without having to make a new OpenerDirector each time, e.g. by calling build_opener().Easy HTML form filling.Convenient link parsing and following.Browser history (.back() and .reload() methods).The Referer HTTP header is added properly (optional).Automatic observance of robots.txt.Automatic handling of HTTP-Equiv and Refresh.	搜索引擎
For searching packages use apt-cache search.Example in your case following can help you:<blockquote>$ apt-cache search libffilibffi-dev - Foreign Function Interface library (development files)libffi6 - Foreign Function Interface library runtimelibffi6-dbg - Foreign Function Interface library runtime (debug symbols)ffindex-dbg - simple index/database for huge amounts of small files (debug)libbfio-dbg - Library to provide basic input/output abstraction (debug)libffindex0 - library for simple index/database for huge amounts of small fileslibffindex0-dev - library for simple index/database for huge amounts of small files (development)libjffi-java - Java Foreign Function Interfacelibjffi-jni - Java Foreign Function Interface (JNI library)</blockquote>	ubuntu
Ubuntu14.04在virtualenv下安装scrapy报错，Failed building wheel for cffi,lxml,cryptography 等. error: command 'x86_64-linux-gnu-gcc' failed with exit status 1sudo apt-get build-dep python-lxmlsudo pip install lxml --upgradesudo apt-get install build-essential libssl-dev libffi-dev python-devpip install scrapy	ubuntu
